<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[盖茨推荐的一堂30分钟的经济学]]></title>
    <url>%2F2017%2F07%2F29%2FEnglish-economic%2F</url>
    <content type="text"><![CDATA[“This knowledge would help everyone as investors and citizens. Watching is a worthwhile 30 minutes investment.” –Bill Gates 经济到底由什么驱动？政府调控利率是如何影响经济的？央行增发货币又是为哪般？经济周期从何而来？经济衰退和经济萧条是一个物种吗？信贷到底是啥？钱原来大部分是信贷？价格到底怎么算？30分钟，带你了解经济引擎的运行原理，真正的经济学远比你忘得干干净净的教科书要简单有趣且精妙的多。 支出总额是经济的驱动力。经济中，政府由中央政府和中央银行组成，钱由货币和信贷组成。 央行调控利率是如何对经济产生影响的？ 增加利率，借贷成本上升，信贷减少，通货紧缩。降低利率，借贷成本下降，信贷增多，通货膨胀。 政府又是用哪些办法调控经济运行状况的？ 政府的四大法宝：1）财富转移（向富人增税，向穷人发放救济金）2）债务重组3）借贷（发行国债）4）增发钞票 经济周期经济的上下起伏是由债务的波动引起 债务的波动有两大周期短期，持续大约5-8年长期，大约持续75-100年 还记得曾今火爆的妖书《货币战争》，里面的一段话，大意是：大萧条时期，全世界都在亏钱，钱不能凭空消失吧？那么总有个人赚了，于是书里引出了被神化的罗斯柴尔德家族。看完这个视频，突然明白，原来现实生活中所谓的钱实际上由”钱+信贷“组成，且大部分都是信贷，而信贷是可以凭空产生和消失的，萧条中消失的钱，不过是上半个经济周期中大家通过借贷提前消费了。这个时代，各种砖家涌现，如何避免某些专家的忽悠？只有自己去了解这些领域的真正的基层原理，那些很简单却真实有效的原理，以此武装自己的大脑，才能避免被砖家殴打你的智商。 参考资料 HOW THE ECONOMIC MACHINE WORKS– 宏观经济运行的框架]]></content>
      <categories>
        <category>眼界</category>
      </categories>
      <tags>
        <tag>Economic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔记（五）-- 高级特性]]></title>
    <url>%2F2017%2F06%2F27%2FpyhthonNote5%2F</url>
    <content type="text"><![CDATA[切片，全局变量，生成器 切片（slice）切片（slice）用于取一个list或tuple的部分元素,比如，一个list如下：1&gt;&gt;&gt; L = [&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;, &apos;Bob&apos;, &apos;Jack&apos;] L[0:3]表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。如果第一个索引是0，还可以省略：12&gt;&gt;&gt; L[:3][&apos;Michael&apos;, &apos;Sarah&apos;, &apos;Tracy&apos;] 类似的，既然Python支持L[-1]取倒数第一个元素，那么它同样支持倒数切片。1234&gt;&gt;&gt; L[-2:]['Bob', 'Jack']&gt;&gt;&gt; L[-2:-1]['Bob'] 所有数，每5个取一个：123&gt;&gt;&gt; L = list(range(100))&gt;&gt;&gt; L[::5][0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] 甚至什么都不写，只写[:]就可以原样复制一个list(这可以用来解决默认参数重复赋值问题)：12&gt;&gt;&gt; L[:][0, 1, 2, 3, ..., 99] tuple也是一种list，唯一区别是tuple不可变。因此，tuple也可以用切片操作，只是操作的结果仍是tuple：12&gt;&gt;&gt; (0, 1, 2, 3, 4, 5)[:3](0, 1, 2) 字符串&#39;xxx&#39;也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串：1234&gt;&gt;&gt; &apos;ABCDEFG&apos;[:3]&apos;ABC&apos;&gt;&gt;&gt; &apos;ABCDEFG&apos;[::2]&apos;ACEG&apos; 全局变量和局部变量12345678A = 10def fun(): global A A = 20print(A) # 10fun()print(A) # 20 generator要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： 123456&gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。我们可以直接打印出list的每一个元素，但我们怎么打印出generator的每一个元素呢？如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值： 123456789&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1# 执行到最后一个元素的时候&gt;&gt;&gt; next(g)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出StopIteration的错误。 通常，我们创建了一个generator后，基本上永远不会调用next()，而是通过for循环来迭代它，并且不需要关心StopIteration的错误。 1234567&gt;&gt;&gt; g = (x * x for x in range(3))&gt;&gt;&gt; for n in g:... print(n)... 014 yeild 定义generatorFibonacci sequence（斐波那契数列）123456789def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(a) a, b = b, a+b n = n+1 return 'done'fib(10) 仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： 这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator。 这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。12345678print("====================")def fibGenerator(max): n, a, b = 0, 0, 1 while n&lt; max: yield a a, b =b, a+b n += 1 return 'done' 调用该generator时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值： 1234567891011121314151617181920f = fibGenerator(10)print(next(f))print(next(f))print(next(f))print('=====================')for x in f: print(x)# 输出结果如下011=====================2358132134 同样的，把函数改成generator后，我们基本上从来不会用next()来获取下一个返回值，而是直接使用for循环来迭代：1234567print(fibGenerator(10))for x in fibGenerator(10): print(x)print("====================")L = [x for x in fibGenerator(10)]print(L) 用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中： 12345678910111213141516171819while True: try: x = next(f) print('f: ', x) except StopIteration as e: print('Generator return value: ', e.value) break# 输出结果如下f: 0f: 1f: 1f: 2f: 3f: 5f: 8f: 13f: 21f: 34Generator return value: done 练习–杨辉三角杨辉三角定义如下：123456 1 1 1 1 2 1 1 3 3 1 1 4 6 4 11 5 10 10 5 1 把每一行看做一个list，试写一个generator，不断输出下一行的list： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 以下是我写的渣渣版def triangles(max): n = 1 if n == 1: L = [1] # print(L) yield L n = n + 1 L = [1,1] # print(L) yield L while n &lt; max: L = list([*L, 1]) L[0] = 1 L[n-1] = 1 l = L[:] for i in range(1, n): L[i] = l[i-1] + l[i] n = n + 1 yield L # print(L)t = triangles(10)for i in t: print(i)# 输出结果如下[1][1, 1][1, 2, 1][1, 3, 3, 1][1, 4, 6, 4, 1][1, 5, 10, 10, 5, 1][1, 6, 15, 20, 15, 6, 1][1, 7, 21, 35, 35, 21, 7, 1][1, 8, 28, 56, 70, 56, 28, 8, 1][1, 9, 36, 84, 126, 126, 84, 36, 9, 1]# 下面是网友给出的简洁版,再次被秒成渣def trian(max): L=[1] n = 0 while n &lt; max: yield L L = [1] + [ L[x-1] + L[x] for x in range(1,len(L)) ] + [1] n += 1t = trian(10)for i in t: print(i) 参考资料 廖雪峰python教程]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔记（四）-- 函数]]></title>
    <url>%2F2017%2F06%2F26%2FPython%E7%AC%94%E8%AE%B04%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[函数中的各种参数，lambda表达式，递归函数，汉诺塔示例。 空函数有啥卵用？骚年，还记得大明湖畔（java中）的抽象函数么？12def nop(): pass 函数的参数类型检查：if not isinstance(x, (int, float)): raise TypeError(&#39;bad operand type&#39;)1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError(&apos;bad operand type&apos;) if x &gt;= 0: return x else: return -x 参数positional arguments（位置参数）：就是普通的参数了，我猜是因为传参时其位置必须一一对应而得名。key-word arguments（关键字参数）：形式为kwarg=value，（如果出现在函数定义中，这也就是默认参数）或通过词典拆包**dict来传递。arbitrary argument lists(可变参数表): 形式为*anameNOTE：在函数参数列表中的顺序为：位置参数，可变参数表，关键字参数 1234567891011121314151617181920""" Arbitrary Argument Lists """&gt;&gt;&gt; def concat(*args, sep="/"):... return sep.join(args)...&gt;&gt;&gt; concat("earth", "mars", "venus")'earth/mars/venus'&gt;&gt;&gt; concat("earth", "mars", "venus", sep=".")'earth.mars.venus'# 可变参数之后是关键字参数，必须以键值对的形式传入，否则会被认为是可变参数，或者报错SyntaxError: positional argument follows keyword argument&gt;&gt;&gt; concat("earth", "mars", "venus", ".")'earth/mars/venus/.'# Unpacking Argument Lists&gt;&gt;&gt; def parrot(voltage, state='a stiff', action='voom'):... print("-- This parrot wouldn't", action, end=' ')... print("if you put", voltage, "volts through it.", end=' ')... print("E's", state, "!")...&gt;&gt;&gt; d = &#123;"voltage": "four million", "state": "bleedin' demised", "action": "VOOM"&#125;&gt;&gt;&gt; parrot(**d)-- This parrot wouldn't VOOM if you put four million volts through it. E's bleedin' demised ! 关键字参数可以传入0个或者任意个含参数名的参数，这些参数名在函数定义中并没有出现，这些参数在函数内部自动封装成一个字典(dict).123456def portrait(name, **kw): print('name is', name) for k,v in kw.items(): print(k, v) portrait('Mike', age=24, country='China', education='bachelor') 如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下：12def person(name, age, *, city, job): print(name, age, city, job) 调用方式如下：12&gt;&gt;&gt; person(&apos;Jack&apos;, 24, city=&apos;Beijing&apos;, job=&apos;Engineer&apos;)Jack 24 Beijing Engineer 如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了：def person(name, age, *args, city, job): print(name, age, args, city, job)命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错：1234&gt;&gt;&gt; person(&apos;Jack&apos;, 24, &apos;Beijing&apos;, &apos;Engineer&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: person() takes 2 positional arguments but 4 were given 由于调用时缺少参数名city和job，Python解释器把这4个参数均视为位置参数，但person()函数仅接受2个位置参数。 通过可变参数和关键字参数，任何函数都可以用 universal_func(*args, **kw) 表达。 参数小结：默认参数一定要用不可变对象，如果是可变对象，程序运行时会有逻辑错误！要注意定义可变参数和关键字参数的语法： *args是可变参数，args接收的是一个tuple；**kw是关键字参数，kw接收的是一个dict。 以及调用函数时如何传入可变参数和关键字参数的语法： 可变参数既可以直接传入：func(1, 2, 3)，又可以先组装list或tuple，再通过*args传入：func(*(1, 2, 3))；字参数既可以直接传入：func(a=1, b=2)，又可以先组装dict，再通过**kw传入：func(**{&#39;a&#39;: 1, &#39;b&#39;: 2})。 使用*args和**kw是Python的习惯写法，当然也可以用其他参数名，但最好使用习惯用法。命名的关键字参数是为了限制调用者可以传入的参数名，同时可以提供默认值。定义命名的关键字参数在没有可变参数的情况下不要忘了写分隔符，否则定义的将是位置参数。如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符``了。 Lambda Expressions123456789101112def make_incrementor(n): return lambda x: x + n # lambda新建了一个匿名函数，相当于如下代码 # def test(x): # return x + n # return testf = make_incrementor(42) # 将42赋值给n，并将返回的lambda匿名函数赋值给fprint(f)print(f(0))print(f(1)) The above example uses a lambda expression to return a function. Another use is to pass a small function as an argument:1234&gt;&gt;&gt; pairs = [(1, &apos;one&apos;), (2, &apos;two&apos;), (3, &apos;three&apos;), (4, &apos;four&apos;)]&gt;&gt;&gt; pairs.sort(key=lambda pair: pair[0])&gt;&gt;&gt; pairs[(1, &apos;one&apos;), (2, &apos;two&apos;), (3, &apos;three&apos;), (4, &apos;four&apos;)] 递归函数在函数内部，可以调用其他函数。如果一个函数在内部调用自身本身，这个函数就是递归函数。 1234def fact(n): if n==1: return 1 return n * fact(n - 1) 递归函数的优点是定义简单，逻辑清晰。理论上，所有的递归函数都可以写成循环的方式，但循环的逻辑不如递归清晰。 使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出。可以试试fact(1000)： 1234567&gt;&gt;&gt; fact(1000)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 4, in fact ... File "&lt;stdin&gt;", line 4, in factRuntimeError: maximum recursion depth exceeded in comparison 一些语言可以通过尾递归来解决这个问题，但是Python标准的解释器没有针对尾递归做优化，任何递归函数都存在栈溢出的问题。 汉诺塔–递归函数的应用实例请编写move(n, a, b, c)函数，它接收参数n，表示3个柱子A、B、C中第1个柱子A的盘子数量，然后打印出把所有盘子从A借助B移动到C的方法。画图分析一下：从图中的分析可以看出f(n,a,b,c)=f(n-1,a,c,b)+f(1,a,b,c)+f(n-1,b,a,c)，于是使用递归函数，代码如下：12345678910def move(n, a, b, c): if n == 1: print(a + '--&gt;' + c) return move(n-1, a, c, b) move(1, a, b, c) move(n-1, b, a, c)move(7, 'A', 'B', 'C') 参考资料 官方文档– Defining Functions 莫烦Python 廖雪峰的Python教程]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔记（三）-- Important Warning]]></title>
    <url>%2F2017%2F06%2F25%2FPython%E7%AC%94%E8%AE%B03warning%2F</url>
    <content type="text"><![CDATA[本篇整理下python中容易出错的那些坑。在学习Python的过程中，发现其很多与java不同的地方，很多人并没有仔细研读过python，只是大概地看下相关的语法就开始上手写代码去了，最容易被忽视的就是简单的for，if等控制语句，实际上python的控制语句与java等高级语言是不同的，所谓失之毫厘差之千里，请看下面的例子。 for循环操作可变序列12345678910111213a = [-1, 2, -4, -3, -1, -2, 3]print(a)for x in a: if x &lt; 0: a.remove(x)print(a) # [2, -3, -2, 3]print("=========================")b = [-1, 2, -4, -3, -1, -2, 3]print(b)for x in b[:]: if x &lt; 0: b.remove(x)print(b) # [2, 3] Note There is a subtlety when the sequence is being modified by the loop (this can only occur for mutable sequences, i.e. lists). An internal counter is used to keep track of which item is used next, and this is incremented on each iteration. When this counter has reached the length of the sequence the loop terminates. This means that if the suite deletes the current (or a previous) item from the sequence, the next item will be skipped (since it gets the index of the current item which has already been treated). Likewise, if the suite inserts an item in the sequence before the current item, the current item will be treated again the next time through the loop. This can lead to nasty bugs that can be avoided by making a temporary copy using a slice of the whole sequence.简而言之，循环时指针会移动，所以直接删除是不安全的。而解决办法也很精妙（真是完美契合subtlety这个词）,使用b[:]会在内存中临时复制出一个b，在b[:]中循环，在原本的b上remove。 for else我问了身边三个会python的程序猿，他们都震惊地表示：还有这种操作？！然而官方文档写得很清楚，for else，try else都是python的独特的正确的语法。12for_stmt ::= &quot;for&quot; target_list &quot;in&quot; expression_list &quot;:&quot; suite [&quot;else&quot; &quot;:&quot; suite] A break statement executed in the first suite terminates the loop without executing the else clause’s suite. A continue statement executed in the first suite skips the rest of the suite and continues with the next item, or with the else clause if there is no next item. 也就是说，正常情况下else语句会在循环完成（遍历完整个expression_list）后执行。但是如果有break发生时，else语句不执行，也就是说break会同时跳出for和else的整程序组（suite）。而continue会在循环中跳过else语句，然后在循环完毕后执行else语句。 示例：12345678for n in range(2, 10): for x in range(2, n): if n % x == 0: print(n, 'equals', x, '*', n//x) break else: # loop fell through without finding a factor print(n, 'is a prime number') 换个说法再来解释一遍： Loop statements may have an else clause; it is executed when the loop terminates through exhaustion of the list (with for) or when the condition becomes false (with while), but not when the loop is terminated by a break statement. 你能不用for else语句来实现相同的功能么？(if x == n-1:) 默认参数函数 Important warning: The default value is evaluated only once. This makes a difference when the default is a mutable object such as a list, dictionary, or instances of most classes. For example, the following function accumulates the arguments passed to it on subsequent calls: 1234567891011def f(a, L=[]): L.append(a) return Lprint(f(1))print(f(2))print(f(3))# 输出结果为 [1][1, 2][1, 2, 3] 官方的解释不够详细，廖雪峰的解释如下：Python函数在定义的时候，默认参数L的值就被计算出来了，即[]，因为默认参数L也是一个变量，它指向对象[]，每次调用该函数，如果改变了L的内容，则下次调用时，默认参数的内容就变了，不再是函数定义时的[]了。所以，定义默认参数要牢记一点：默认参数必须指向不变对象！要修改上面的例子，我们可以用None这个不变对象来实现。解决办法：1234567def f(a, L=None): if L is None: L = [] L.append(a) return Lprint(f(1), f(2), f(3)) # [1] [2] [3] 参考文章 官方文档–More Control Flow Tools]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔记（二）-- 容器]]></title>
    <url>%2F2017%2F06%2F22%2Fpython%E7%AC%94%E8%AE%B02%2F</url>
    <content type="text"><![CDATA[List 和 Tuplelist 和 tuple均为有序表，即可用索引访问，但是list可变，tuple不可变。 先来几个基础单词：parenthesis/parentheses 圆括号（）square brackets 方括号[]curly braces 大括号/花括号{}heterogeneous [‘hɛtərə’dʒinɪəs] 不同种类的homogeneous [,homə’dʒinɪəs]同种类的 Listlist类似于数组，是一个可变的有序表，元素的数据类型也可以不同,ist元素也可以是另一个list。如classmates = [&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;]，索引从0开始到len(classmates)-1，如果要取最后一个元素，除了计算索引位置外，还可以用-1做索引，直接获取最后一个元素：classmates[-1],以此类推，可以获取倒数第2个、倒数第3个classmates[-2]。方法：list.append(x)Add an item to the end of the list. Equivalent to a[len(a):] = [x].list.extend(iterable)Extend the list by appending all the items from the iterable. Equivalent to a[len(a):] = iterable.list.insert(i, x)Insert an item at a given position. The first argument is the index of the element before which to insert, so a.insert(0, x) inserts at the front of the list, and a.insert(len(a), x) is equivalent to a.append(x).list.remove(x)Remove the first item from the list whose value is x. It is an error if there is no such item.list.pop([i])Remove the item at the given position in the list, and return it. If no index is specified, a.pop() removes and returns the last item in the list. (The square brackets around the i in the method signature denote that the parameter is optional, not that you should type square brackets at that position. You will see this notation frequently in the Python Library Reference.)list.clear()Remove all items from the list. Equivalent to del a[:].list.index(x[, start[, end]])Return zero-based index in the list of the first item whose value is x. Raises a ValueError if there is no such item.The optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.list.count(x)Return the number of times x appears in the list.list.sort(key=None, reverse=False)Sort the items of the list in place (the arguments can be used for sort customization, see sorted() for their explanation).list.reverse()Reverse the elements of the list in place.list.copy()Return a shallow copy of the list. Equivalent to a[:]. del() 删除指定位置元素list(tup) 将不可变的元祖tup转换为可变的list1234567891011121314151617181920212223242526272829303132333435363738394041424344&gt;&gt;&gt; fruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana']&gt;&gt;&gt; fruits.count('apple')2&gt;&gt;&gt; fruits.count('tangerine')0&gt;&gt;&gt; fruits.index('banana')3&gt;&gt;&gt; fruits.index('banana', 4) # Find next banana starting a position 46&gt;&gt;&gt; fruits.reverse()&gt;&gt;&gt; fruits['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']&gt;&gt;&gt; fruits.append('grape')&gt;&gt;&gt; fruits['banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange', 'grape']&gt;&gt;&gt; fruits.sort()&gt;&gt;&gt; fruits['apple', 'apple', 'banana', 'banana', 'grape', 'kiwi', 'orange', 'pear']&gt;&gt;&gt; fruits.pop()'pear'classmates.append('Adam')classmates.insert(1, 'Jack')&gt;&gt;&gt; classmates.pop()'Adam'&gt;&gt;&gt; classmates.pop(1)'Jack'# del语句,通过角标删除元素&gt;&gt;&gt; a = [-1, 1, 66.25, 333, 333, 1234.5]&gt;&gt;&gt; del a[0]&gt;&gt;&gt; a[1, 66.25, 333, 333, 1234.5]# 删除部分&gt;&gt;&gt; del a[2:4]&gt;&gt;&gt; a[1, 66.25, 1234.5]# 清空&gt;&gt;&gt; del a[:]&gt;&gt;&gt; a[]# del还可以直接删除整个list变量（不是清空）&gt;&gt;&gt; del a#tuple转换为listaTuple = (123, 'xyz', 'zara', 'abc');aList = list(aTuple) Nested List ComprehensionsThe initial expression in a list comprehension can be any arbitrary expression, including another list comprehension.Consider the following example of a 3x4 matrix implemented as a list of 3 lists of length 4: 12345matrix = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12],] The following list comprehension will transpose rows and columns:12&gt;&gt;&gt; [[row[i] for row in matrix] for i in range(4)][[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] As we saw in the previous section, the nested listcomp is evaluated in the context of the for that follows it, so this example is equivalent to:123456&gt;&gt;&gt; transposed = []&gt;&gt;&gt; for i in range(4):... transposed.append([row[i] for row in matrix])...&gt;&gt;&gt; transposed[[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] In the real world, you should prefer built-in functions to complex flow statements. The zip() function would do a great job for this use case:12&gt;&gt;&gt; list(zip(*matrix))[(1, 5, 9), (2, 6, 10), (3, 7, 11), (4, 8, 12)] Tupletuple是一个不可变的有序表，tuple和list非常类似，但是tuple一旦初始化就不能修改，比如同样是列出同学的名字：classmates = (&#39;Michael&#39;, &#39;Bob&#39;, &#39;Tracy&#39;)现在，classmates这个tuple不能变了，自然也就没有append()，insert()这样的方法。其他获取元素的方法和list是一样的，你可以正常地使用classmates[0]，classmates[-1]，但不能赋值成另外的元素。Tuples may be constructed in a number of ways: Using a pair of parentheses to denote the empty tuple: () Using a trailing comma for a singleton tuple: a, or (a,) Separating items with commas: a, b, c or (a, b, c) Using the tuple() built-in: tuple() or tuple(iterable) 不可变的tuple有什么意义？因为tuple不可变，所以代码更安全。如果可能，能用tuple代替list就尽量用tuple。 Though tuples may seem similar to lists, they are often used in different situations and for different purposes. Tuples are immutable, and usually contain a heterogeneous sequence of elements that are accessed via unpacking (see later in this section) or indexing (or even by attribute in the case of namedtuples). Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list.The statement t = 12345, 54321, &#39;hello!&#39; is an example of tuple packing: the values 12345, 54321 and &#39;hello!&#39; are packed together in a tuple. The reverse operation is also possible:1&gt;&gt;&gt; x, y, z = t This is called, appropriately enough, sequence unpacking and works for any sequence on the right-hand side. Sequence unpacking requires that there are as many variables on the left side of the equals sign as there are elements in the sequence. Note that multiple assignment is really just a combination of tuple packing and sequence unpacking.注意： 1234# 定义一个空的tuple&gt;&gt;&gt; t = ()# 定义一个只有一个元素的tuple, 需要加逗号&gt;&gt;&gt; t = (4,) # 这里可以的括号可以省略即t = 4, t = (1)定义的不是tuple，是1这个数！这是因为括号()既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，Python规定，这种情况下，按小括号进行计算，计算结果自然是1。所以，只有1个元素的tuple定义时必须加一个逗号,，来消除歧义。 当tuple中嵌套list时，tuple是对list的指向不变，但list可变，。 Dictionary和SetDictdict全称dictionary，类似于java中的map，使用键-值（key-value）存储（key要求唯一性），具有极快的查找速度。示例：123456789101112131415161718&gt;&gt;&gt; tel = &#123;'jack': 4098, 'sape': 4139&#125;&gt;&gt;&gt; tel['guido'] = 4127&gt;&gt;&gt; tel&#123;'sape': 4139, 'guido': 4127, 'jack': 4098&#125;&gt;&gt;&gt; tel['jack']4098&gt;&gt;&gt; del tel['sape']&gt;&gt;&gt; tel['irv'] = 4127&gt;&gt;&gt; tel&#123;'guido': 4127, 'irv': 4127, 'jack': 4098&#125;&gt;&gt;&gt; list(tel.keys())['irv', 'guido', 'jack']&gt;&gt;&gt; sorted(tel.keys())['guido', 'irv', 'jack']&gt;&gt;&gt; 'guido' in telTrue&gt;&gt;&gt; 'jack' not in telFalse 使用dict()构造器、comprehension、直接赋值法来创建dict。123456789# The dict() constructor builds dictionaries directly from sequences of key-value pairs:&gt;&gt;&gt; dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])&#123;'sape': 4139, 'jack': 4098, 'guido': 4127&#125;# dict comprehensions&gt;&gt;&gt; &#123;x: x**2 for x in (2, 4, 6)&#125;&#123;2: 4, 4: 16, 6: 36&#125;# When the keys are simple strings, it is sometimes easier to specify pairs using keyword arguments:&gt;&gt;&gt; dict(sape=4139, guido=4127, jack=4098)&#123;'sape': 4139, 'jack': 4098, 'guido': 4127&#125; 方法：get()pop() 删除指定元素12345678910d = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 85&#125;# 若key不存在，则返回None。# 注意：返回None的时候,Python的交互式命令行不显示结果。d.get('Tom')# 指定默认值。如果key不存在自己指定的value&gt;&gt;&gt; d.get('Tom', -1)-1# pop(key)，删除key-value,返回被删除的value&gt;&gt;&gt; d.pop('Bob')75 When looping through dictionaries, the key and corresponding value can be retrieved at the same time using the items() method. 123456&gt;&gt;&gt; knights = &#123;'gallahad': 'the pure', 'robin': 'the brave'&#125;&gt;&gt;&gt; for k, v in knights.items():... print(k, v)...gallahad the purerobin the brave 请务必注意，dict内部存放的顺序和key放入的顺序是没有关系的。和list比较，dict有以下几个特点： 1. 查找和插入的速度极快，不会随着key的增加而变慢； 2. 需要占用大量的内存，内存浪费多。 而list相反： 1. 查找和插入的时间随着元素的增加而增加； 2. 占用空间小，浪费内存很少。 所以，dict是用空间来换取时间的一种方法。 Dictionary的迭代取出因为dict的存储不是按照list的方式顺序排列，所以，迭代出的结果顺序很可能不一样。 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()。1234567891011121314&gt;&gt;&gt; d = &#123;'a': 1, 'b': 2, 'c': 3&#125;&gt;&gt;&gt; for key in d:... print(key)...acb# 字符串也能迭代&gt;&gt;&gt; for ch in 'ABC':... print(ch)...ABC 当我们使用for循环时，只要作用于一个可迭代对象，for循环就可以正常运行，而我们不太关心该对象究竟是list还是其他数据类型。 那么，如何判断一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断：123456&gt;&gt;&gt; isinstance('abc', Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False Setset是无序不重复的集合(A set is an unordered collection with no duplicate elements.)。可以这样理解：set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 由于官方文档写得实在是太好，再加上想练习英语，所以我打算直接贴英文原文了。Curly braces or the set() function can be used to create sets. Note: to create an empty set you have to use set(), not {}; the latter creates an empty dictionary. Here is a brief demonstration: 123456789101112131415161718192021&gt;&gt;&gt; basket = &#123;'apple', 'orange', 'apple', 'pear', 'orange', 'banana'&#125;&gt;&gt;&gt; print(basket) # show that duplicates have been removed&#123;'orange', 'banana', 'pear', 'apple'&#125;&gt;&gt;&gt; 'orange' in basket # fast membership testingTrue&gt;&gt;&gt; 'crabgrass' in basketFalse# Demonstrate set operations on unique letters from two words&gt;&gt;&gt; a = set('abracadabra')&gt;&gt;&gt; b = set('alacazam')&gt;&gt;&gt; a # unique letters in a&#123;'a', 'r', 'b', 'c', 'd'&#125;&gt;&gt;&gt; a - b # letters in a but not in b&#123;'r', 'd', 'b'&#125;&gt;&gt;&gt; a | b # letters in a or b or both&#123;'a', 'c', 'r', 'd', 'b', 'm', 'z', 'l'&#125;&gt;&gt;&gt; a &amp; b # letters in both a and b&#123;'a', 'c'&#125;&gt;&gt;&gt; a ^ b # letters in a or b but not both&#123;'r', 'd', 'b', 'm', 'z', 'l'&#125; Similarly to list comprehensions, set comprehensions are also supported: 123&gt;&gt;&gt; a = &#123;x for x in 'abracadabra' if x not in 'abc'&#125;&gt;&gt;&gt; a&#123;'r', 'd'&#125; Looping TechniquesWhen looping through dictionaries, the key and corresponding value can be retrieved at the same time using the items() method. 123456&gt;&gt;&gt; knights = &#123;&apos;gallahad&apos;: &apos;the pure&apos;, &apos;robin&apos;: &apos;the brave&apos;&#125;&gt;&gt;&gt; for k, v in knights.items():... print(k, v)...gallahad the purerobin the brave When looping through a sequence, the position index and corresponding value can be retrieved at the same time using the enumerate() function. 123456&gt;&gt;&gt; for i, v in enumerate([&apos;tic&apos;, &apos;tac&apos;, &apos;toe&apos;]):... print(i, v)...0 tic1 tac2 toe To loop over two or more sequences at the same time, the entries can be paired with the zip() function.12345678&gt;&gt;&gt; questions = [&apos;name&apos;, &apos;quest&apos;, &apos;favorite color&apos;]&gt;&gt;&gt; answers = [&apos;lancelot&apos;, &apos;the holy grail&apos;, &apos;blue&apos;]&gt;&gt;&gt; for q, a in zip(questions, answers):... print(&apos;What is your &#123;0&#125;? It is &#123;1&#125;.&apos;.format(q, a))...What is your name? It is lancelot.What is your quest? It is the holy grail.What is your favorite color? It is blue. To loop over a sequence in reverse, first specify the sequence in a forward direction and then call the reversed() function.12345678&gt;&gt;&gt; for i in reversed(range(1, 10, 2)):... print(i)...97531 To loop over a sequence in sorted order, use the sorted() function which returns a new sorted list while leaving the source unaltered.123456&gt;&gt;&gt; basket = [&apos;apple&apos;, &apos;orange&apos;, &apos;apple&apos;, &apos;pear&apos;, &apos;orange&apos;, &apos;banana&apos;]&gt;&gt;&gt; a = sorted(set(basket))&gt;&gt;&gt; print(a)[&apos;apple&apos;, &apos;banana&apos;, &apos;orange&apos;, &apos;pear&apos;] # 注意sort还有去重的效果&gt;&gt;&gt; print(basket)[&apos;apple&apos;, &apos;orange&apos;, &apos;apple&apos;, &apos;pear&apos;, &apos;orange&apos;, &apos;banana&apos;] It is sometimes tempting to change a list while you are looping over it; however, it is often simpler and safer to create a new list instead.123456789&gt;&gt;&gt; import math&gt;&gt;&gt; raw_data = [56.2, float(&apos;NaN&apos;), 51.7, 55.3, 52.5, float(&apos;NaN&apos;), 47.8]&gt;&gt;&gt; filtered_data = []&gt;&gt;&gt; for value in raw_data:... if not math.isnan(value):... filtered_data.append(value)...&gt;&gt;&gt; filtered_data[56.2, 51.7, 55.3, 52.5, 47.8] 或者参见Python笔记（三）– for else还有这种操作中的临时复制法。 扩展阅读Comparing Sequences and Other Types Sequence objects may be compared to other objects with the same sequence type. The comparison uses lexicographical ordering: first the first two items are compared, and if they differ this determines the outcome of the comparison; if they are equal, the next two items are compared, and so on, until either sequence is exhausted. If two items to be compared are themselves sequences of the same type, the lexicographical comparison is carried out recursively. If all items of two sequences compare equal, the sequences are considered equal. If one sequence is an initial sub-sequence of the other, the shorter sequence is the smaller (lesser) one. Lexicographical ordering for strings uses the Unicode code point number to order individual characters. Some examples of comparisons between sequences of the same type: 1234567(1, 2, 3) &lt; (1, 2, 4)[1, 2, 3] &lt; [1, 2, 4]&apos;ABC&apos; &lt; &apos;C&apos; &lt; &apos;Pascal&apos; &lt; &apos;Python&apos;(1, 2, 3, 4) &lt; (1, 2, 4)(1, 2) &lt; (1, 2, -1)(1, 2, 3) == (1.0, 2.0, 3.0)(1, 2, (&apos;aa&apos;, &apos;ab&apos;)) &lt; (1, 2, (&apos;abc&apos;, &apos;a&apos;), 4) Note that comparing objects of different types with &lt; or &gt; is legal provided that the objects have appropriate comparison methods. For example, mixed numeric types are compared according to their numeric value, so 0 equals 0.0, etc. Otherwise, rather than providing an arbitrary ordering, the interpreter will raise a TypeError exception. 参考资料 官方文档–Data Structures 莫烦Python 廖雪峰的Python教程]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔记（一）--数据类型]]></title>
    <url>%2F2017%2F06%2F22%2Fpython%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[看完了尚学堂的java300集（除了手写服务器和设计模式部分），由于不是按顺序看的，最后复习了多线程，昨晚写完多线程的博客，java学习算是告一段落了，现在向Python发起进攻。有的人用“Python还用学么？”来形容Python，看来Python实在是一门简单易学的语言，特别是在你已经掌握一门高级语言之后。但是千万不能因此就真的“不学”，完整的理论知识是你之后“敲键盘如有神”的基本条件。这里罗列一些我没见过的小知识点和它与java等语言的不同之处。仅作为个人笔记，不供参考。 浮点数浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，比如，1.23x109和12.3x108是相等的。浮点数可以用数学写法，如1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x109就是1.23e9，或者12.3e8，0.000012可以写成1.2e-5，等等。 整数和浮点数在计算机内部存储的方式是不同的，整数运算永远是精确的（除法难道也是精确的？是的！），而浮点数运算则可能会有四舍五入的误差。顺便一提，与java不同，9/4输出结果为2.5，9//4输出结果为2。 字符串–转义与换行如果字符串里面有很多字符都需要转义，就需要加很多\，为了简化，Python还允许用r’’表示’’内部的字符串默认不转义。12&gt;&gt;&gt; print(r'换行/n还是不换行，这是一个问题')换行/n还是不换行，这是一个问题 如果字符串内部有很多换行，用\n写在一行里不好阅读，为了简化，Python允许用’’’…’’’的格式表示多行内容.在终端repl里这样用，IDE里就用不着了。 Boolean0和0.0表示False其余为True布尔值不外乎True和False，但Python中整数和浮点数也能进行 Boolean 数据操作, 具体规则，如果该值等于 0 或者 0.0 将会返回 False，其余的返回 True。1234con = 10while con: print(con) con -= 1 逻辑运算符and or not布尔值可以用and、or和not运算,而非 &amp; | ! 。如果你测试发现它们也能用的话，那只是巧合，因为&amp; | !是位运算符，比如1 &gt; 2 &amp; 1事实上是1 &gt; (2 &amp; 1)结果为false, 它是按照位运算之后的结果。如果是逻辑运算1 &gt; 2 and 1，结果应该是true。 Python中还支持x &lt; y &lt; z（PS:还有这种操作！），等同于x &lt; y and y &lt; z，虽然有人认为第一种操作可能会让其他语言的程序猿看不懂，但是PyCharm却会在第二种格式中提示你将其转为第一种格式，因为更简洁，一贯的JetBrains智能处女座风格。 空值空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 变量变量在程序中就是用一个变量名表示了，可以是任意数据类型,变量名必须是大小写英文、数字和_的组合，且不能用数字开头， 常量所谓常量就是不能变的变量，比如常用的数学常数π就是一个常量。在Python中，通常用全部大写的变量名表示常量,如PI = 3.14159265359但事实上PI仍然是一个变量，Python根本没有任何机制保证PI不会被改变，所以，用全部大写的变量名表示常量只是一个习惯上的用法，如果你一定要改变变量PI的值，也没人能拦住你。 格式化在Python中，采用的格式化方式和C语言是一致的，用%实现，如&#39;Hello, %s&#39; % &#39;world&#39;。常见的占位符有：%d 整数%f 浮点数%s 字符串%x 十六进制整数其中，格式化整数和浮点数还可以指定是否补0和整数与小数的位数1234&gt;&gt;&gt; '%2d-%02d' % (3,1)' 3-01'&gt;&gt;&gt; '%d-%.2f' % (3,1.5647)'3-1.56' 参考文章 莫烦Python 廖雪峰的Python教程]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挣脱你的轨道--一则公益小短片《cogs》]]></title>
    <url>%2F2017%2F06%2F11%2FEnglish-cogs%2F</url>
    <content type="text"><![CDATA[var dplayer3 = new DPlayer({"element":document.getElementById("dplayer3"),"autoplay":false,"theme":"#FADFA3","loop":true,"video":{"url":"http://ugcydzd.qq.com/flv/92/216/g0512cgb51w.p712.1.mp4?sdtfrom=v1010&guid=72ffbc53bc13455246dcec4efd2c2b02&vkey=0CE5FC72FB6FA6D97FD1077E0449F0AB0ADDF71FFE82014D6FA31F80237EA5C3A40C048E207507FA283E0EB1C1C3E10188B0D7CAD66E072FF8AB6BBC2D2E8E9E34631C122081535D7168D0D2723548E25E94E04EC20FD1A10848CDB66FBE45E35E6F7D1D0C3C0520AFA5331498386C8D","pic":"/media/14972328758364.jpg"}}); Here is a related report: AIME crafts beautiful animated film for global launchThe Australian charity has taken its ambitions to help underprivileged children to the world with a new film by M&amp;C Saatchi Sydney and Zeilt Productions. With over a thousand shares on Facebook within a few hours of its launch, a new animated film for AIME appears to have already struck a chord. The overwhelmingly positive reviews—“So beautiful and inspiring”, “ profound and beautifully created”—will be a boon for the charity’s founder, 31-year-old Australian Jack Manning Bancroft, as he prepares to take his education charity international for the first time. Creative partners M&amp;C Saatchi Sydney and director Laurent Witz (winner of a 2013 Academy Award for the animated short film Mr Hublot) and his team at Zeilt Productions will also be celebrating the release of a project that has been over a year in development. ‘Cogs’ tells the story of two boys who find themselves, quite literally, on separate, pre-determined tracks in their lives, and the drama hinges on their effort to break free from those imposed limitations. The tagline, “If we want to change the world, we need to change the way it works”, reflects the goal Manning Bancroft has aimed for since establishing AIME 12 years ago to end education inequality in Australia. On average, 61.5 percent of indigenous children in Australia finish school, with only 42 percent going onto further education. AIME’s programme pairs university student volunteers with underprivileged school children, and the numbers of those completing school and going on to university leap to 87.9 percent and 74 percent as a result. With the launch of Cogs, says Manning Bancroft, AIME’s battle against “the broken system” that breeds inequality is heading overseas. “We are calling out to find 10 young people around the world who want to take our model of mentoring to their country to fight inequality and create a fairer world.” AIME has already had enquiries from Nigeria, France, the USA, Canada, New Zealand, Uganda and more, he says: 33 young people have started their applications in the last 24 hours since the film’s release. “AIME is addictively positive,” says Andy Flemming, group creative director, M&amp;C Saatchi, Sydney, who describes his work on the film as a “dream job”. “They’re run by an incredible dreamer / thinker / storyteller who has an unwavering mission to change the world for the better. It’s impossible not to be swept away by their boundless enthusiasm, which is backed up by results that show that their system works. We live in a particularly shitty world, so you just can’t pass up the chance to maybe, just maybe help them do something about it.” The specific theme for the film came from the need, says Flemming, to find “a wonderfully simple idea to introduce AIME to the world.” “The team was playing with the idea of a system that’s fractured and broken. That evolved into people not realising that their lives are on set tracks, and this is precisely how society works—it needs that separation. We then imagined a machine-like city that’s incredibly old and the whole thing just clicked together. It was emotional.” Once the agency engaged with Zeilt Productions, Witz had between 20 and 30 people working on the film for many months, with a continuous team engaged from the agency side too. “All of us had a singular vision and every month the work just got better and better,” says Flemming. “Obviously the time helped. We crafted the hell out of this, and it’s not often we get to pour so much love into advertising projects these days.” As the message in Cogs takes flight and online shares start to gather speed, Manning Bancroft says he hopes the film will mark the incremental rise of mentors around the world, with all the subsequent benefits this could have for improving lives. “Success will be when we see every university student in the world being mentors for their most disadvantaged high school kids,” he says. vocabularyanimated /ˈænɪˌmeɪtɪd/ ADJ Someone who is animated or who is having an animated conversation is lively and is showing their feelings. 热烈的 ADJ An animated film is one in which puppets or drawings appear to move. 动画的 underprivileged /ˌʌndəˈprɪvɪlɪdʒd/ ADJ Underprivileged people have less money and fewer possessions and opportunities than other people in their society. 贫穷的; 缺少机遇的 strike a chord 打动(某人的)心弦，在(某人心中)引起共鸣. 引起共鸣，触动心弦 overwhelmingly adv. 压倒性地；不可抵抗地 profound /prəˈfaʊnd/ ADJ You use profound to emphasize that something is very great or intense. 深刻的; 极大的例：…discoveries which had a profound effect on many areas of medicine.…对医学的许多领域都有深刻影响的一些发现。例：…profound disagreement.…极大的分歧。 ADJ A profound idea, work, or person shows great intellectual depth and understanding. 高深的 ADV 深刻地; 极大地 例：This has profoundly affected my life. 这已极大地影响了我的生活。 boon /buːn/ N-COUNT You can describe something as a boon when it makes life better or easier for someone. 福音 例：It is for this reason that television proves such a boon to so many people. 正是这个原因电视机成为这么多人的一大福音。 ADJ close, special, or intimate (in the phrase boon companion) 亲密的; 特别的 animated short film 动画短片 cog /kɒɡ/ N-COUNT A cog is a wheel with square or triangular teeth around the edge, which is used in a machine to turn another wheel or part. 齿轮 PHRASE If you describe someone as a cog in a machine or wheel, you mean that they are a small part of a large organization or group. (大型机构或组织中的)小人物 例：Mr. Lake was an important cog in the republican campaign machine. 雷克先生是共和党竞选机器中一个小人物，但却发挥了重要作用。 N-COUNT a tenon that projects from the end of a timber beam for fitting into a mortise (木工)凸榫 literally /ˈlɪtərəlɪ/ ADV You can use literally to emphasize an exaggeration. Some careful speakers of English think that this use is incorrect. 真地 例：We’ve got to get the economy under control or it will literally eat us up. 我们必须控制住经济，否则它真地就会把我们困住。 ADV You use literally to emphasize that what you are saying is true, even though it seems exaggerated or surprising. 确实地 例：Putting on an opera is a tremendous enterprise involving literally hundreds of people. 上演一台话剧是一项巨大的事业，它确实要几百个人参与。 ADV If a word or expression is translated literally, its most simple or basic meaning is translated. 字面上地 例：The word “volk” translates literally as “folk.” “”这个词照字面意思翻译为“”。 predetermined /ˌpriːdɪˈtɜːmɪnd/ ADJ If you say that something is predetermined, you mean that its form or nature was decided by previous events or by people rather than by chance. 预先确定的 例：The prince’s destiny was predetermined from the moment of his birth. 该王子的命运从出生那一刻起就已经被决定了。 例：The capsules can be made to release the pesticides at a predetermined time. 可使这些胶囊在预定时间释放出杀虫剂. track [铁路] 轨道（track的复数）；[计] 磁道；轮胎 impose /ɪmˈpəʊz/ V-T If you impose something on people, you use your authority to force them to accept it. 强制实行 N-UNCOUNT 强制实行 例：…the imposition of sanctions against Pakistan. …对巴基斯坦制裁的强制实行。 V-T If you impose your opinions or beliefs on other people, you try and make people accept them as a rule or as a model to copy. 把 (观点、信仰等) 强加于 例：Parents should beware of imposing their own tastes on their children. 父母应该提防把自己的兴趣强加给孩子。 V-T If something imposes strain, pressure, or suffering on someone, it causes them to experience it. 使承受 (令人不快之事物) 例：The filming imposed an additional strain on her. 影片拍摄使她承受了额外的压力。 V-I If someone imposes on you, they unreasonably expect you to do something for them which you do not want to do. 不合理地要求例：I was afraid you’d feel we were imposing on you. 我担心你会觉得我们在不合理地要求你N-COUNT 不合理的要求 例：I know this is an imposition. But please hear me out. 我知道这是个不合理的要求，但请听我把话说完。 V-T If someone imposes themselves on you, they force you to accept their company although you may not want to. 使强迫接受 tagline n. 标语；品牌口号 indigenous [ɪn’dɪdʒənəs] adj. 本土的；土著的；国产的；固有的 breed /briːd/ N-COUNT A breed of a pet animal or farm animal is a particular type of it. For example, terriers are a breed of dog. (动物的) 品种 例：…rare breeds of cattle. …稀有牛种。 养殖 breed animals or plant 繁殖 When animals breed, they have babies. V-T If you say that something breeds bad feeling or bad behaviour, you mean that it causes bad feeling or bad behaviour to develop. 酿成 (不良情绪或不良行为)例：If they are unemployed it’s bound to breed resentment.如果他们失业了,一定会酿成怨恨。 storyteller /ˈstɔːrɪˌtɛlə/ n. 说故事的人；故事作者；短篇小说作家 unwavering /ʌnˈweɪvərɪŋ/ ADJ If you describe a feeling or attitude as unwavering, you mean that it is strong and firm and does not weaken. (情感、态度)强烈的; 坚定的 例：She has been encouraged by the unwavering support of her family. 她家人坚定地支持鼓舞着她。 boundless /ˈbaʊndlɪs/ ADJ If you describe something as boundless, you mean that there seems to be no end or limit to it. 无限的 例：His reforming zeal was boundless. 他的改革热情是无穷尽的。 shitty /ˈʃɪtɪ/ ADJ If someone describes something as shitty, they do not like it or they think that it is of poor quality. adj. 较差的；劣等的,狗屎的 fractured [‘fræktʃəd]adj. 断裂的；挫伤的；折裂的 v. 断裂（fracture的过去式） evolve /ɪˈvɒlv/ V-I When animals or plants evolve, they gradually change and develop into different forms. 进化 例：Birds are widely believed to have evolved from dinosaurs. 鸟类普遍被认为是从恐龙进化而来的。 V-T/V-I If something evolves or you evolve it, it gradually develops over a period of time into something different and usually more advanced. 使…逐步发展; 逐步发展 例：…a tiny airline which eventually evolved into Pakistan International Airlines. …最终发展成为巴基斯坦国际航空公司的一家小型航空公司。 incremental /ˌɪnkrɪˈmɛntəl/ ADJ Incremental is used to describe something that increases in value or worth, often by a regular amount. 递增的 原文链接AIME crafts beautiful animated film for global launch]]></content>
      <categories>
        <category>English Learning</category>
      </categories>
      <tags>
        <tag>English</tag>
        <tag>cogs</tag>
        <tag>animated short film</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些你常用但是却不知道如何用英文表达的句子]]></title>
    <url>%2F2017%2F06%2F06%2FEnglish-%E5%B8%B8%E7%94%A8%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[掌握另外一种语言（外语），真正的难点在于掌握它与已习得语言（母语）的不重合之处。直观的一一对应，其实没什么难度：“那是一本书”对应着“That’s a book”。而那些你常用到的句子或短语，翻译成英文，却一听就知道是中式英文。“若果我没记错的话…” 用英语怎么说？是 If I didn’t remembered wrong…吗？是英语里就没有这样的表达，还是我们不知道它的地道表达方式呢？ 而所谓的“不地道”，其实不过用母语的表达习惯去说外语。比如： • 如果我没记错的话…… • 我从未想过…… • 你竟然跟我这么说话！ • 就知道你有这本事！ • (这)听着耳熟吧？ • 我想不起来那名字了……这些句子，基本上都是我们日常生活中必然用到的句子（或片段），可是，如果“直译”的话，就很别扭： • If I didn’t remembered wrong… • I never thought of/that… • How dare you talking to me like this! • I know you have such a capability! • Does it sound familiar? • I cannot remember that name…同样的语境里，“地道”的说法是这样的： • If my memory serves, … • It never occurred to me that… • Are we really having this conversation? • Always knew you had it in you! • Does it ring a bell to you? • That name escapes me….看完这几个地道的翻译后，是不是有种一拍大腿，醍醐灌顶之快感。 无意中看到李笑来的 《人人都能用英语》，方法论类的书不需要多读，这些书在我看来在我看来其实挺不靠谱，要学英语就去学英语啊，看英语的学习资料啊，但是，闲来无事时选一两本还是就可以一读的，或许会有耳目一新的论见颠覆你的一些固有思维，或者称之为常识的东西，不一定让你的学习事半功倍，但至少开拓了你的思维，也舒缓下学习带来的的紧绷状态。 参考资料： 知笔墨 –《人人都能用英语 – 李笑来》]]></content>
      <categories>
        <category>English Learning</category>
      </categories>
      <tags>
        <tag>English</tag>
        <tag>英语学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Aaron Ralston’s Story]]></title>
    <url>%2F2017%2F06%2F05%2FEnglish-Aron's%20story%2F</url>
    <content type="text"><![CDATA[Aaron Ralston, a 27-year-old mountain sports fanatic from Colorado in the United States, found himself in dire straits alone in a canyon in the desert when a 500kg rock came crashing down the canyon to smash his right hand and trap it against the canyon wall. A terrible accident, but the situation was made all the more serious because on this occasion Aaron had failed to tell anyone where he was going. At the last minute the plans for a trip with his climbing partners had fallen through, and on the spur of the moment he decided to head out on his own to cycle up a long mountain trail, leave his bike and then walk down the Blue John canyon. No one had the slightest idea where he was. After three days of not seeing or hearing any sign of life Aaron realised he would die there if he didn’t do something drastic. The course of action was horrific, but there was no other way. He would have to amputate his right hand. Fortunately he had a small multitool knife with him and he had some straps that he could use to make a tourniquet to stop himself bleeding to death when he cut the arteries. The knife had two blades. When he tried with the larger blade he found that it was too blunt to cut the skin. The following day he found the courage to try the shorter blade, and with that he managed to cut through the skin. Only when he had made a large hole in his arm did he realise that it was going to be impossible to use any of the little tools on his knife to cut through the bones. After another 24 hours of pain and despair the idea and the strength came to him in a flash on the sixth day. With a final burst of energy he broke both bones in his arm and freed himself. The ordeal was not over, though. He was still a long way from help. He had to carefully strap up his right arm and then find a way of lowering himself down a 20m drop in the canyon with a rope and only his left arm, and then walk the 10 km back to his car. Despite his ingenuity* and all his efforts he would have bled to death if it hadn’t been for a very happy coincidence: the moment he got out of the canyon into the open desert the rescue helicopter just happened to be flying overhead. One of the doctors at the hospital recalls being impressed to see Ralston walk into the hospital on his own, in spite of his injuries and the gruelling experience of being in the desert for six days with almost nothing to eat and only a couple of litres of water. He describes the amputation as remarkable. “It’s a perfect example of someone improvising in a dire situation,” he said. “He took a small knife and was able to amputate his arm in such a way that he did not bleed to death.”Slim and pale with short reddish-brown hair, Ralston believes that his story was not simply about an isolated individual who rose to a formidable challenge. For him there was a spiritual dimension to the experience. In his news conference he said, “I may never fully understand the spiritual aspects of what I experienced, but I will try. The source of the power I felt was the thoughts and prayers of many people, most of whom I will never know.” Vocabularydire /‘daɪə/ ADJ Dire is used to emphasize how serious or terrible a situation or event is. 严重的; 可怕的 ADJ If you describe something as dire, you are emphasizing that it is of very low quality. 质量低劣的 strait /streɪt/ N-COUNT/N-IN-NAMES You can refer to a narrow strip of sea which joins two large areas of sea as a strait or the straits. 海峡 N-PLURAL If someone is in dire or desperate straits, they are in a very difficult situation, usually because they do not have much money. (常指缺钱造成的) 困境 canyon /ˈkænjən/ N-COUNT/N-IN-NAMES A canyon is a long, narrow valley with very steep sides. 峡谷 smash /smæʃ/ V-T/V-I If you smash something or if it smashes, it breaks into many pieces, for example, when it is hit or dropped. 打碎; 破碎 V-T/V-I If you smash through a wall, gate, or door, you get through it by hitting and breaking it. 撞破 (墙或门) 而入 V-T/V-I If something smashes or is smashed against something solid, it moves very fast and with great force against it. 使猛撞; 撞击 V-T To smash a political group or system means to deliberately destroy it. 搞垮 (政治集团或体制) fall through PHRASAL VERB If an arrangement, plan, or deal falls through, it fails to happen. 落空 slight /slaɪt/ ADJ Something that is slight is very small in degree or quantity. 轻微的; 细微的 ADJ A slight person has a fairly thin and delicate looking body. 瘦小的; 纤细的 V-T If you are slighted, someone does or says something that insults you by treating you as if your views or feelings are not important. 轻蔑; 怠慢 N-COUNT Slight is also a noun. 轻视; 冷落 PHRASE You use in the slightest to emphasize a negative statement. (用于加强否定的陈述语气) 一点也 drastic /ˈdræstɪk/ ADJ If you have to take drastic action in order to solve a problem, you have to do something extreme to solve it. 极端的 ADJ A drastic change is a very great change. 剧烈的 horrific /hɒˈrɪfɪk, hə-/ ADJ If you describe a physical attack, accident, or injury as horrific, you mean that it is very bad, so that people are shocked when they see it or think about it. 极其可怕的 例: I have never seen such horrific injuries. 我从没见过这么严重的伤。 ADV 极其可怕地 例：He had been horrifically assaulted before he died. 他死之前曾遭人毒打。 ADJ If you describe something as horrific, you mean that it is so big that it is extremely unpleasant. 大得骇人的 例：…piling up horrific extra amounts of money on top of your original debt. …在你原有债务的基础上再加上数目骇人的几大笔钱。 ADV 大得骇人地 例：Opera productions are horrifically expensive. 歌剧的演出花销大得吓人。 amputate /ˈæmpjʊˌteɪt/1 V-T To amputate someone&apos;s arm or leg means to cut all or part of it off in an operation because it is diseased or badly damaged. 截 (肢) 2 N-VAR 截肢 amputation. strap /stræp/1 N-COUNT A strap is a narrow piece of leather, cloth, or other material. Straps are used to carry things, fasten things together, or to hold a piece of clothing in place. 带子 例：Nancy gripped the strap of her beach bag. 南希抓住自己海滩休闲包的带子。 例：She pulled the strap of her nightgown onto her shoulder. 她把睡衣的带子拉到她的肩上。 2 V-T If you strap something somewhere, you fasten it there with a strap. 用带子绑 例： She strapped the baby seat into the car. 她把婴儿座椅用带子绑在那辆汽车上。 参考资料1.2017 JUNE 3RD 阅读文章]]></content>
      <categories>
        <category>English Learning</category>
      </categories>
      <tags>
        <tag>test</tag>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基本数据类型转换原理以及数据溢出处理]]></title>
    <url>%2F2017%2F03%2F24%2Fjava%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[朋友发来的一个小问题，让我又把java底层数据原理深入学习了一遍。 Java中x=x+1 与x+=1 的一点区别许多程序员都会认为这里的表达式（s1 +=1）只是上面表达式（s1 = s1 + 1）的简写方式,至少以前我是这样认为的。但是这并不十分准确。 Java语言规范中讲到，复合赋值E1 op= E2等价于简单赋值E1 = (T)((E1)op(E2) 其中T是E1的数据类型,op为操作符。这种类型转换或者是一个恒等转换,或者是一个窄化转换. 1234567891011121314151617181920212223public class PlusEqualTest &#123; public static void main(String[] args) &#123; short s1 = 1; s1 = (short) (s1 + 1);//如果不加转换是不能通过编译的 System.out.println("s1 = " + s1); short s2 = 1; s2 += 1; System.out.println("s2 = " + s2); short s3 = Short.MAX_VALUE;//32767 short s4 = s3; s3 += 1; System.out.println("s3 += 1 结果为" + s3); s4 = (short) (s4 + 1); System.out.println("s3 = (short)(s3 + 1) 结果为" + s4); &#125;&#125;//输出结果如下：s1 = 2s2 = 2s3 += 1 结果为-32768s3 = (short)(s3 + 1) 结果为-32768 数据溢出咋办呢如上程序，为什么s3 = (short)(s3 + 1) 结果为-32768呢？很明显是数据超出其表示范围了，可是数据溢出后是怎么处理的呢？此事说来话长…… 数据类型基础知识储备 基本类型 位数 范围 默认值 byte(字节) 8 -128 至 127 0 shot(短整型) 16 -32768(-2^15 ) 至 32767(2^15-1 ) 0 int(整型) 32 -2147483648(-2^31 )至2147483647(2^31-1 ) 0 long(长整型) 64 -2^63 至2^63-1 0L或0l boolean 1 0,1或true，false false char 8 \u0000至\uffff \u0000\ float 32 1.4E-45至3.4028235E38 0.0f double 64 4.9E-324至1.7976931348623157E308 0.0d 类型转换的原理 真值：这个就看字面意思，int a = -1;a的真值就是-1. 原码：int类型的-1的二进制表示，由于-1是负数，又是int类型的，所以他需要32个二进制来表示，二进制的最高位是符号位，所以为1，1的二进制为1，所以-1的二进制表示为1000 0000 0000 0000 0000 0000 0000 0001； 反码：正数的反码就是原码，负数的反码是在原码的基础上，符号位不变，其余位取反。所以int类型的-1的反码是1111 1111 1111 1111 1111 1111 1111 1110。 补码：正数的补码就是原码，负数的补码是在其反码的基础上加1。所以，int类型的-1的补码是 1111 1111 1111 1111 1111 1111 1111 1111。 补位：补位是二进制中在扩充位数的时候，位数不够需要在左边补齐，补齐的方式为如果是正数的补位，左边全部补0，负数左边全部补1（也就是说补位的时候补足的是符号位）。 自动转换（从高位向低位转换）：将a位的变量A转换为b位的B：A–&gt;补码–&gt;截取（去掉前a-b位，只保留后b位）–&gt;补码–&gt;B。示例：1int a=-1；byte b=（byte）a；//b的结果是-1。 1000 0000 0000 0000 0000 0000 0000 0001补码1111 1111 1111 1111 1111 1111 1111 1111截取（保留后8位）1111 1111 1111 1111 1111 1111 1111 1111即1111 1111补码1000 0001即真值为-1 强制转换（从低位向高位转换时）：(byte-&gt;short-&gt;int-&gt;long-&gt;float-&gt;double)补码–&gt;补位–&gt;补码示例：1byte a =-1；int b=a；//b的结果是-1。 1000 0001补码1111 1111补位1111 1111 1111 1111 1111 1111 1111 1111补码1000 0000 0000 0000 0000 0000 0000 0001即真值为-1 数据溢出处理示例1：1int a=255;byte b=(byte)a; //b的值为什么是-1? 0000 0000 0000 0000 0000 0000 1111 1111补码0000 0000 0000 0000 0000 0000 1111 1111截取1111 1111补码1000 0001即真值为-1示例2：大家在自己算一下（byte）234的结果是什么，然后在IDE里在运行下看和自己手写算出来的一样吗。这里（byte）234的手写结果是150，但是150是超出了byte的表示范围的，所以这里还有一次转化？？？后面是怎么转化的呢？ 定点数与浮点数区别定点数在计算机系统的发展过程中，曾经提出过多种方法表达实数。典型的比如相对于浮点数的定点数（Fixed Point Number）。在这种表达方式中，小数点固定的位于实数所有数字中间的某个位置。货币的表达就可以使用这种方式，比如 99.00 或者 00.99 可以用于表达具有四位精度（Precision），小数点后有两位的货币值。由于小数点位置固定，所以可以直接用四位数值来表达相应的数值。SQL 中的 NUMBER 数据类型就是利用定点数来定义的。还有一种提议的表达方式为有理数表达方式，即用两个整数的比值来表达实数。定点数表达法的缺点在于其形式过于僵硬，固定的小数点位置决定了固定位数的整数部分和小数部分，不利于同时表达特别大的数或者特别小的数。 浮点数（float-point-number）最终，绝大多数现代的计算机系统采纳了所谓的浮点数表达方式。这种表达方式利用科学计数法来表达实数，即用一个尾数（Mantissa ），一个基数（Base），一个指数（Exponent）以及一个表示正负的符号来表达实数。比如 123.45 用十进制科学计数法可以表达为 1.2345 × 10^2 ，其中 1.2345 为尾数，10 为基数，2 为指数。浮点数利用指数达到了浮动小数点的效果，从而可以灵活地表达更大范围的实数。 NOTE: 尾数有时也称为有效数字（Significand）。尾数实际上是有效数字的非正式说法。 同样的数值可以有多种浮点数表达方式，比如上面例子中的 123.45 可以表达为 12.345 × 10^1 ，0.12345 × 10^3 或者 1.2345 × 10^2 。因为这种多样性，有必要对其加以规范化以达到统一表达的目标。规范的（Normalized）浮点数表达方式具有如下形式： $±d_0.d_1d_2…d_i × β^e , (0 ≤ d_i &lt; β)$ 其中 d.dd…d 即尾数，β 为基数，e 为指数。尾数中数字的个数称为精度，在本文中用 p 来表示。每个数字 d 介于 0 和基数之间，包括 0。小数点左侧的数字不为 0。 基于规范表达的浮点数对应的具体值可由下面的表达式计算而得：$±(d_0 + d1β^{e-1} + … + d{i-1}β ^ {e-(i-1)}+ d_iβ^{e-i} ), (0 ≤ d_i &lt; β)$对于十进制的浮点数，即基数 β 等于 10 的浮点数而言，上面的表达式非常容易理解，也很直白。计算机内部的数值表达是基于二进制的。从上面的表达式，我们可以知道，二进制数同样可以有小数点，也同样具有类似于十进制的表达方式。只是此时 β 等于 2，而每个数字$d_i$ 只能在 0 和 1 之间取值。比如二进制数 1001.101 相当于 1 × 2^3 + 0 × 2^2 + 0 × 2^1 + 1 × 2^0 + 1 × 2^-1 + 0 × 2^-2 + 1 × 2^-3 ，对应于十进制的 9.625。其规范浮点数表达为 1.001101 × 2^3。 IEEE 浮点数计算机中是用有限的连续字节保存浮点数的。保存这些浮点数当然必须有特定的格式，Java 平台上的浮点数类型 float 和 double 采纳了 IEEE 754 标准中所定义的单精度 32 位浮点数和双精度 64 位浮点数的格式。 float和double内存结构在 IEEE 标准中，浮点数是将特定长度的连续字节的所有二进制位分割为特定宽度的符号域，指数域和尾数域三个域，其中保存的值分别用于表示给定二进制浮点数中的符号，指数和尾数。float的指数位有8位，而double的指数位有11位，分布如下： float： 1bit（符号位） 8bits（指数位） 23bits（尾数位） double： 1bit（符号位） 11bits（指数位） 52bits（尾数位） 第二个域为指数域（图中标注为指数位），对应于我们之前介绍的二进制科学计数法中的指数部分。其中单精度数为 8 位，双精度数为 11 位。以单精度数为例，8 位的指数为可以表达 0 到 255 之间的 255 个指数值。但是，指数可以为正数，也可以为负数。为了处理负指数的情况，实际的指数值按要求需要加上一个偏差（Bias）值作为保存在指数域中的值，单精度数的偏差值为 127，而双精度数的偏差值为 1023。比如，单精度的实际指数值 0 在指数域中将保存为 127；而保存在指数域中的 64 则表示实际的指数值 -63。 偏差的引入使得对于单精度数，实际可以表达的指数值的范围就变成 -127 到 128 之间（包含两端）。我们不久还将看到，实际的指数值 -127（保存为 全 0）以及 +128（保存为全 1）保留用作特殊值的处理。这样，实际可以表达的有效指数范围就在 -127 和 127 之间。在本文中，最小指数和最大指数分别用 emin 和 emax 来表达。 于是，float的指数范围为-128~+127，而double的指数范围为-1024~+1023，并且指数位是按补码的形式来划分的。其中负指数决定了浮点数所能表达的绝对值最小的非零数；而正指数决定了浮点数所能表达的绝对值最大的数，也即决定了浮点数的取值范围。float的范围为???-2^128 ~ +2^127，也即-3.40E+38 ~ +3.40E+38；double的范围为???-2^1024 ~ +2^1023，也即-1.79E+308(这里与IDEA打印出的最小值不符，博主会慢慢查证再来修改) ~ +1.79E+308。 之所以不能用f1==f2来判断两个数相等，是因为虽然f1和f2在可能是两个不同的数字，但是受到浮点数表示精度的限制，有可能会错误的判断两个数相等 精度float和double的精度是由尾数的位数来决定的。浮点数在内存中是按科学计数法来存储的，其整数部分始终是一个隐含着的“1”，由于它是不变的，故不能对精度造成影响。 float：2^23 = 8388608，一共七位，由于最左为1的一位省略了，这意味着最多能表示8位数： 2*8388608 = 16777216 。有8位有效数字，但绝对能保证的为7位，也即==float的精度为7~8位有效数字==； double：2^52 = 4503599627370496，一共16位，同理，==double的精度为16~17位==。 图例中的第三个域为尾数域，其中单精度数为 23 位长，双精度数为 52 位长。除了我们将要讲到的某些特殊值外，IEEE 标准要求浮点数必须是规范的。这意味着尾数的小数点左侧必须为 1，因此我们在保存尾数的时候，可以省略小数点前面这个 1，从而腾出一个二进制位来保存更多的尾数。这样我们实际上用 23 位长的尾数域表达了 24 位的尾数。比如对于单精度数而言，二进制的 1001.101（对应于十进制的 9.625）可以表达为 1.001101 × 23，所以实际保存在尾数域中的值为 00110100000000000000000，即去掉小数点左侧的 1，并用 0 在右侧补齐。 值得注意的是，对于单精度数，由于我们只有 24 位的尾数（其中一位隐藏），所以可以表达的最大尾数为 224 - 1 = 16,777,215。特别的，16,777,216 是偶数，所以我们可以通过将它除以 2 并相应地调整指数来保存这个数，这样 16,777,216 同样可以被精确的保存。相反，数值 16,777,217 则无法被精确的保存。由此，我们可以看到单精度的浮点数可以表达的十进制数值中，真正有效的数字不高于 8 位。事实上，对相对误差的数值分析结果显示有效的精度大约为 7.22 位。参考下面的示例： true value|16,777,215|16,777,216|16,777,217|16,777,218|16,777,219|—|—|—|—|—|—stored value|16,777,215|1.6777215E7|1.6777216E7|1.6777216E7|1.6777218E7 根据标准要求，无法精确保存的值必须向最接近的可保存的值进行舍入。这有点像我们熟悉的十进制的四舍五入，即不足一半则舍，一半以上（包括一半）则进。不过对于二进制浮点数而言，还多一条规矩，就是当需要舍入的值刚好是一半时，不是简单地进，而是在前后两个等距接近的可保存的值中，取其中最后一位有效数字为零者。从上面的示例中可以看出，奇数都被舍入为偶数，且有舍有进。我们可以将这种舍入误差理解为”半位”的误差。所以，为了避免 7.22 对很多人造成的困惑，有些文章经常以 7.5 位来说明单精度浮点数的精度问题。 可以试试在IDE中运行一下代码12345float f1 = 16777215f; for (int i = 0; i &lt; 10; i++) &#123; System.out.println(f1); f1++; &#125; 输出结果除了第一个是1.6777215E7其余全都是1.6777216E7，不是for循环出了问题，而是因为16,777,216在计算机中即1.6777216E7，+1后依旧等于1.6777216E7。 参考文章 Java中x=x+1 与x+=1 的一点区别 java中数据溢出处理 java 类型转换的原理 深入浅出浮点数Floating Point Number In a Nutshell]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA中使用maven编译Spark源码]]></title>
    <url>%2F2017%2F03%2F06%2FsparkSourceCompile%2F</url>
    <content type="text"><![CDATA[Spark源码编译及测试。 spark官网下载源码包 修改pom文件修改pom.xml文件中的com.google.guava，将其从provided改为compile。或者编译源码之后，打开project structure，将guava包的scpoe改为compile。如果不改的话，之后运行example包里的示例程序时，会报classNotFoundException。 scope选项中provided和compile的区别对于scope=compile的情况（默认scope),也就是说这个项目在编译，测试，运行阶段都需要这个artifact对应的jar包在classpath中。 而对于scope=provided的情况，则可以认为这个provided是目标容器已经provide这个artifact。换句话说，它只影响到编译，测试阶段。在编译测试阶段，我们需要这个artifact对应的jar包在classpath中，而在运行阶段，假定目标的容器（比如我们这里的liferay容器）已经提供了这个jar包，所以无需我们这个artifact对应的jar包了。 做一个实验就可以很容易发现，当我们用maven install生成最终的构件包ProjectABC.war后，在其下的WEB-INF/lib中，会包含我们被标注为scope=compile的构件的jar包，而不会包含我们被标注为scope=provided的构件的jar包。这也避免了此类构件当部署到目标容器后产生包依赖冲突。 添加assembly包在example包的依赖（dependencies）中添加assembly包。spark-assembly-1.6.0-hadoop2.6.0.jar可以在部署包的lib中找到。 解决Flume Sink的依赖问题View-&gt;Tool Windows-&gt;Maven Projects找到Flume Sink，右键点击Generate Sources and Update Folders。右键项目，选择Open Module Settings，然后找到flume-sink, 先点击target（默认是黄色，也就是和上面的Exclude对应），点击Exclude将其改为正常的目录，再点击Sources将其设为源，再将scala-2.10也source。 编译Build-&gt;Build Project，编译时间比较长，耐心等待吧。 测试Run-&gt;Edit Configuration。新建Application，然后选择Main class，VM options设置为-Dspark.master=local。点击example包的SparkPi，运行并查看结果。 参考资料 maven dependency中scope=compile 和 provided区别]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Bigdata</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论迷思]]></title>
    <url>%2F2016%2F12%2F07%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[当你抛起一枚硬币，你不知道它会是正面还是反面，但你确切的知道正面与反面的概率都是50%。概率论的神奇之处在于，它居然能从不确定性中找到确定性。 本文不教科书，只是阐述我的观点和思考，如有谬误，欢迎讨论或指正。 一些有趣的观点：一个事情有N种发生的可能性，我们不能确信哪种会发生，是因为我们不能控制结果的发生，影响结果的许多因素不在我们的支配范围之内，这些因素影响结果的机理或者我们不知道，或者太复杂以至于超出了我们大脑或电脑的运算能力。比如：我们不确定掷硬币得到正面或反面，是因为我们的能力不足以用一些物理方程来求解这个结果。再比如：你不能断定你期末能考88分，因为出题、阅卷的不是你。 对于未发生之事，我们无法掌握其所有参数或无法计算。对于已经发生之事，事情都已经发生了，结果已定，也会因为掌握的信息不全而产生所谓概率。即过去发生的事情虽然事实上是确定的，但因为我们的无知，它成了随机的。 我们在某个地方挖出了一块瓷器的碎片，它可能是孔子的夜壶，可能是秦始皇的餐具，也可能是隔壁老王的破茶壶从他家到垃圾站又被埋在了这个地方。 因此：概率在实质上就是无知，而不是说事物本身是随机的。 这一点很重要，不要误以为概率应该是客观事实。如果你有上帝视角的话，那么一切都是注定，任何事的概率都是100%，也就没有所谓概率之说了。 所以概率论是建立在人们有限的认知中的，不是真正的客观事实。也就是说当孔子一看，这貌似是自己的夜壶啊，他认为这是夜壶的概率为70%，秦始皇一看那块碎片，朕心中只装的下江山，哪来的餐具，在他看来的餐具的概率是1/3，然而，老王的却早已看穿一切，那块碎片割过他的手所以他记得格外清楚，茶壶概率为100%。每个人所知道的信息决定了他所认知的概率。 就像狼人杀，这里假设游戏是7个人+上帝，1号和2号玩家是狼人，发完牌的时候就已经注定谁是狼人了。对于上帝和1、2号而言，没有概率可言，或者说1、2号是狼人的概率是100%。而对于平民而言，他除了自己，他无法找出理由认为谁是狼人，只好用古典概率的等可能假设，认为其他每个人是狼人的概率都是1/6，随着游戏的进行，预言家掌握更多的信息，他修正了自己的概率，而平民也根据自己掌握的信息修正自己概率，于是大家对于谁是狼人这件事都有了不同的概率。 注意到上面这个故事中，不难发现，假设碎片只有夜壶，餐具，茶壶这三种可能，即一开始概率应该是各1/3。从孔子到老王，他们都用各自掌握的信息修正了关于这个碎片是什么的概率。这就引出了先验概率和后验概率的概念。 先验概率（Prior probability）与后验概率(Posterior probability)事情还没有发生,要求这件事情发生的可能性的大小,是先验概率.事情已经发生,要求这件事情发生的原因是由某个因素引起的可能性的大小,是后验概率. 先看看来自wiki的定义： Similarly, the prior probability of a random event or an uncertain proposition is the unconditional probability that is assigned before any relevant evidence is taken into account. In Bayesian statistics, the posterior probability of a random event or an uncertain proposition is the conditional probability that is assigned after the relevant evidence or background is taken into account. Similarly, the posterior probability distribution is the probability distribution of an unknown quantity, treated as a random variable, conditional on the evidence obtained from an experiment or survey. “Posterior”, in this context, means after taking into account the relevant evidence related to the particular case being examined.要注意的是这是在贝叶斯统计中。不是公理化的概率定义。 再看看书上的解释在此墙裂推荐陈希孺院士的《概率论与数理统计》，这是豆瓣、知乎的书评和推荐。陈老这本书之所以受到如此簇拥，在于它授人以渔而非授人以鱼，你读一读就是知道。 举一个的简单的例子：一口袋里有3只红球、2只白球，采用不放回方式摸取，求：⑴ 第一次摸到红球（记作A）的概率；⑵ 第二次摸到红球（记作B）的概率；⑶ 已知第二次摸到了红球，求第一次摸到的是红球的概率。解：⑴ P(A)=3/5，还没还有摸球，就问概率，这就是验前概率；⑵ P(B)=P(A)P(B|A)+P(A逆)P(B|A逆)=3/5⑶ P(A|B)=P(A)P(B|A)/P(B)=1/2，这就是后验概率，第一次和第二次摸球这件事都已经发生了，但是我们不知道，比如第一次我们是闭着眼摸完又放回去了，便产生了概率之说。第一问事情未发生（或者说发生了，但是相对于未发生得情况，我们并没有掌握任何更多的信息）我们认为概率是3/5,第三问，我们知道了第二次摸到红球这件事，或者说证据，以此来修正这个概率，就像推理小说一样。关于先验概率和后验概率，推荐阅读：数理统计中的两个学派——频率学派和Bayes学派(1990年的期刊，能找到也是不容易） 一个笑话引发的血案：病人：我听说这个手术成功概率为1%，我是不是该放弃治疗？医生：你放心，我敢保证这次手术100%会成功。病人：真的？为什么？医生：因为我已经失败了99次了。 这是很多人都会犯的“常识”错误，也是经常让人迷惑的地方。可能在这个笑话里，大家没什么深刻感受，那换个例子，比如：A已经抛了100次硬币，每次都是正面，那么下一次反面的几率是不是更大？即使是统计学专业的学生也经常迷糊（比如统计学渣的我），我就一直纳闷，按照大数定律（知乎的解释），如果抛硬币的次数足够多，他就应该是正反各1/2的分布啊，A都抛了100次正面了，下一次就该是反面几率更大了啊。可是每次抛硬币应该是相互独立的，也就是说之前抛无数次也不该影响下一次的概率，即1/2。这个问题的争论，请参考先验概率与后验概率的区别（老迷惑了）。 我比较认可比较的解释是其评论中的一段话，当然，前提是你得清楚频率（ 千万别把频率直接等同于概率），概率的古典定义和统计定义以及公理化定义。 从网上的资料来看，概率本身有多种解释。每一种解释都有一定的漏洞。如果不是研究概率的根本问题（逻辑一致），我们在自己的研究领域不太可能会遇到这些漏洞。（PS:这里必须要说明的是，概率有多种定义（古典定义和统计定义以及公理化定义），但这不是三国演义，我柯早已一统天下，概率的公理化定义有且只有一个，没有漏洞。当然这里所说的逻辑不一致应该是指数理统计中的频率学派和贝叶斯学派之争） 频率学派和贝叶斯学派都有存在的理由。频率学派的“无穷次实验”，贝叶斯学派的“先验概率”，都是有争议的。楼主的第4问，前提是承认了频率学派的观点，但注意它们的观点有一个无穷次实验的假设。我揣测楼主的观点是，因为已经抛了100次正面了，那么，后面抛反面的次数应该多一点，不然总和就不是1/2了，再根据频率学派的观点，出现的频率多一点，于是概率也就大一点。从理论上来说，任何有限次的正面，都不会影响无穷次实验的结果比率。所以，“后面抛反面的次数应该多一点”这一点是不成立的，即使后面的反面的次数少一些，也不会影响这个比率。常数+无穷大=无穷大 关于概率论，一直有许多搞不懂的问题，迷迷糊糊混过四年。在学习HMM和CRF高楼大厦时，发现地基已碎，一边百度基础概念一边学，更是痛苦万分。在搜寻问题时产生更多的问题，终于在重新读了概率论前几章后，算是豁然开朗了很多，所以打算重读概率论，夯实基础，我应该会开个重读概率论的分类，有很多事要做，就并行处理吧，不知道会不会半途而废，也不知道半途而废的概率是多少（当然，我可以凭经验先给出一个主观先验概率，在以后的过程中再慢慢修正得到后验概率，直到概率为0或1），但是有些疑惑终究会推着我去探寻。 推荐阅读：数学之美番外篇：平凡而又神奇的贝叶斯方法 参考资料 先验概率与后验概率的区别（老迷惑了） 先验概率与后验概率及贝叶斯公式 《概率论与数理统计》–陈希孺 数理统计中的两个学派——频率学派和Bayes学派 《概率论与数理统计》–盛骤，谢式千，潘承毅编.-4版]]></content>
      <categories>
        <category>Algorithm</category>
        <category>重读概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>先验概率</tag>
        <tag>后验概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive--DDL和DML练习]]></title>
    <url>%2F2016%2F10%2F13%2FHive-DDl%2F</url>
    <content type="text"><![CDATA[hive的三种模式简介及其搭建测试。 语法参考HIve官网 DDL（Hive Data Definition Language）准备测试数据人员表:id、name、likes爱好、address地址123456781,xiaoming1,book-lol-lanqiu,beijing:xisanqi-shanghai:lujiazui2,xiaoming2,book-lol-lanqiu,beijing:xisanqi-shanghai:lujiazui-huoxing:xx3,xiaoming3,book-lol-lanqiu,beijing:xisanqi-huoxing:xx4,xiaoming4,book-lol-lanqiu,beijing:xisanqi-shanghai:lujiazui-huoxing:xx5,xiaoming5,booklanqiu,beijing:xisanqi-shanghai:lujiazui-huoxing:xx6,xiaoming6,book-lol-lanqiu,beijing:xisanqi-shanghai:lujiazui-huoxing:xx7,xiaoming7,book-lol-lanqiu,beijing:xisanqi-shanghai:lujiazui-huoxing:xx8,xiaoming8,book-lol,beijing:xisanqi-shanghai:lujiazui-huoxing:xx 默认为内部表/MANAGED删除内部表，数据就会从hdfs删除。123456789101112create table psn0 (id int,name string,likes ARRAY&lt;string&gt;,address MAP&lt;string,string&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY '-' MAP KEYS TERMINATED BY ':';-- 加载数据到表LOAD DATA LOCAL inpath '/root/data' INTO TABLE psn0； 外部表/EXTERNAL删除外部表只删除metastore的元数据，不删除hdfs中的表数据。因为外部表由hdfs管理，hive可以增删改查，但是无法将底层数据删除。1234567891011create external table psn1(id int,name string,likes ARRAY&lt;string&gt;,address MAP&lt;string,string&gt;)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;COLLECTION ITEMS TERMINATED BY &apos;-&apos;MAP KEYS TERMINATED BY &apos;:&apos;LOCATION &apos;/psn1&apos;; 分区表分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描创建分区表（顺便说，sql不区分大小写）分区就相当于分目录，后面还会有分桶，分桶相当于分文件。分区的顺序即创建时给定分区的顺序，也就是说下面的语句会创建一个sex目录，sex目录下创建age目录。删除sex就会把age也删除掉。1234567891011121314151617181920212223242526272829CREATE TABLE psn3( id int, name string, likes ARRAY&lt;string&gt;, address MAP&lt;string,string&gt;)PARTITIONED BY (sex string, age int)ROW format delimitedfields TERMINATED BY &apos;,&apos;COLLECTION items TERMINATED BY &apos;-&apos;map keys terminated BY &apos;:&apos;;-- 查询表的分区信息SHOW PARTITIONS psn3;-- 加载数据到分区表(静态分区)当数据被加载至表中时，不会对数据进行任何转换。Load操作只是将数据复制至Hive表对应的位置。数据加载时在表下自动创建一个目录-- 这里会在数据末尾再添加sex和age字段且全都是male和90LOAD DATA LOCAL inpath &apos;/root/data&apos; INTO TABLE psn3 PARTITION (sex=&apos;male&apos;, age=90);-- 分区必须都赋值，否则会报错hive&gt; load data local inpath &apos;/root/data&apos; into table psn3 partition (sex=&apos;girl&apos;);-- FAILED: SemanticException [Error 10006]: Line 1:63 Partition not found &apos;&apos;girl&apos;&apos;-- 修改分区测试-- 添加分区，即添加一个空目录ALTER TABLE psn3 add PARTITION (sex=&apos;male&apos;,age=1);-- 删除分区ALTER TABLE psn3 DROP PARTITION (sex = &apos;male&apos;, age = 1);-- 删除分区就相当于删除目录ALTER TABLE psn3 DROP PARTITION (sex = &apos;male&apos;);-- 当表中只剩sex=male，age=1的数据时，删除age会将male目录也删除。ALTER TABLE psn3 drop PARTITION (age=1); 在外部表中删除分区，数据是否会丢失?聪明的你应该想到了，答案是不会。1234567891011121314151617create external table psn4(id int,name string,likes ARRAY&lt;string&gt;,address MAP&lt;string,string&gt;)PARTITIONED BY (sex string, age int)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;COLLECTION ITEMS TERMINATED BY &apos;-&apos;MAP KEYS TERMINATED BY &apos;:&apos;LOCATION &apos;/psn4&apos;;-- 下面的sql会将psn1下的data文件移到psn4表中，导致psn1中无数据。LOAD DATA inpath &apos;/psn1/data&apos; INTO TABLE psn4 PARTITION (sex=&apos;male&apos;, age=40);LOAD DATA inpath &apos;/user/hive/warehouse/psn0/data&apos; INTO TABLE psn4 PARTITION (sex=&apos;female&apos;, age=28);-- 删除外部表分区，发现依旧只是删除对应的metastore的元数据，hdfs中的数据不会删除。ALTER TABLE psn4 DROP PARTITION (sex = &apos;male&apos;); 另外两种建表方式12345678CREATE TABLE psn5 like psn0;CREATE TABLE psn6 AS SELECT id, likes FROM psn4;CREATE TABLE res2( countLine int);FROM psn4 INSERT INTO TABLE res SELECT count(*); 由于删除外部表不会删除hdfs中的数据，如果再次执行建表语句，原来的数据自然就导入到表里了。也就是说如果你的hdfs中原本有一张表，那么可以用创建外部表的方式将其直接导入hive。上传相同文件或同名文件data3，hdfs中存储的文件会复制一份，不会覆盖。 DML]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Bigdata</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive--安装测试]]></title>
    <url>%2F2016%2F10%2F11%2FHive%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[hive的三种模式简介及其搭建测试。 Hive的三种模式local模式。此模式连接到一个In-memory 的数据库Derby，一般用于Unit Test。 Hive单用户模式通过网络连接到一个数据库中，是最经常使用到的模式。 Pre: 启动集群规划node01：MySQLnode01：Hive 客户端（其实本来该用node02，一时失误，但是不影响练习）123456789101112131415161.启动Zookeeper[root@node02 ~]# zkServer.sh start[root@node03 ~]# zkServer.sh start[root@node03 ~]# zkServer.sh statusJMX enabled by defaultUsing config: /opt/sxt/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: leader# zookeeper的过半机制[root@node04 ~]# zkServer.sh start2.启动Hdfs[root@node01 ~]# start-dfs.sh3.启动 Yarn[root@node01 ~]# start-yarn.sh单独启动Resourcemanager[root@node03 ~]# yarn-daemon.sh start resourcemanager[root@node04 ~]# yarn-daemon.sh start resourcemanager 1.安装MySQL server12345[root@node01 ~]# yum install mysql-server启动mysql[root@node01 ~]# service mysqld start开机启动[root@node01 ~]# chkconfig mysqld on 2.MySQL配置登录mysql修改权限12345678910111213141516171819202122232425262728293031321）进入到mysql的console：[root@node01 ~]# mysqlmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || test |+--------------------+3 rows in set (0.00 sec)2）选择mysql数据库mysql&gt; use mysqlmysql&gt; show tables; mysql&gt; select host, user from user;3）把所有库`*`下的所有表`.*`的权限都给了root用户，可以从任何地方访问`%`，密码是123。mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123&apos; WITH GRANT OPTION;4）定删除多余会对权限造成影响的数据mysql&gt; delete from mysql.user where host != &apos;%&apos;;mysql&gt; select user, host, password from mysql.user;+------+------+-------------------------------------------+| user | host | password |+------+------+-------------------------------------------+| root | % | *23AE809DDACAF96AF0FD78ED04B6A265E05AA257 |+------+------+-------------------------------------------+1 row in set (0.00 sec)5）刷新权限并退出mysql&gt; flush privileges;mysql&gt; exit6)使用密码重新登录[root@node01 ~]# mysql -u root -pEnter password:这里输入密码 3.hive部署上传hive部署包并解压123$ scp /Users/Chant/Documents/大数据0627/hive/apache-hive-1.2.1-bin.tar.gz root@node01:/root[root@node01 ~]# tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/[root@node01 opt]# mv apache-hive-1.2.1-bin/ hive-1.2.1 配置环境变量并生效1234[root@node01 hive-1.2.1]# vi /etc/profileexport HIVE_HOME=/opt/hive-1.2.1export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$HIVE_HOME/bin[root@node01 hive-1.2.1]# . /etc/profile 1234[root@node01 hive-1.2.1]# cd conf/[root@node01 conf]# mv hive-default.xml.template hive-site.xml[root@node01 conf]# vi hive-site.xml用:.,$-1d删除默认的配置 写入以下配置1234567891011121314151617181920212223242526272829&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_remote/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/hive_remote?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;/property&gt; 上传mysql驱动并放到hive的lib目录下12$ scp /Users/Chant/Documents/大数据0627/hive/mysql-connector-java-5.1.32-bin.jar root@node01:/root[root@node01 ~]# mv mysql-connector-java-5.1.32-bin.jar /opt/hive-1.2.1/lib/ 此时启动hive的话会报错，jline包不兼容，可以去看一下hadoop下的jline包与hive的jline包版本不一致，所以需要用高版本替换低版本。 1234[root@node01 lib]# cd /opt/sxt/hadoop-2.6.5/share/hadoop/yarn/lib[root@node01 lib]# rm -f jline-0.9.94.jar[root@node01 lib]# cp /opt/hive-1.2.1/lib/jline-2.12.jar ./[root@node01 lib]# ls jline-2.12.jar 接着启动hive成功。可以做一些练习测试。12345678hive&gt; show tables;hive&gt; create table tbl (id int, name string);hive&gt; show tables;大部分语句都会转成MapReduce来运行。hive&gt; insert into tbl values (1,&apos;xiaoming&apos;);hive&gt; select count(*) from tbl;select * from不会转成MapReduce，因为它只需要读取，没有计算。hive&gt; select * from tbl; 访问node01:50070可以看到之前配置文件中配置的&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_remote/warehouse&lt;/value&gt; 在hdfs中创建了这个目录，并用于存储我们创建的表数据。 单用户模式，元数据存储在mysql中,可以连接数据库查看一下。 Hive多用户模式远程服务器模式。用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库。 规划node01：MySQLnode03：MetaStore Servernode04：Hive 客户端 Pre上传hive部署包到node03和node04并解压1234$ scp /Users/Chant/Documents/大数据0627/hive/apache-hive-1.2.1-bin.tar.gz root@node03:/root$ scp /Users/Chant/Documents/大数据0627/hive/apache-hive-1.2.1-bin.tar.gz root@node04:/rootroot@node03 ~]# tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/[root@node04 ~]# tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/ 两台上都修改环境变量并生效1234[root@node03 hive-1.2.1]# vi /etc/profileexport HIVE_HOME=/opt/hive-1.2.1export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$ZOOKEEPER_PREFIX/bin:$HIVE_HOME/bin[root@node03 hive-1.2.1]# source /etc/profile 服务端配置修改Hive配置 123[root@node03 hive-1.2.1]# cd conf[root@node03 conf]# cp hive-default.xml.template hive-site.xml[root@node03 conf]# vi hive-site.xml 删除原有的configuration，修改为如下内容123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;/property&gt; 上传驱动到node03并放到hive的lib目录下12$ scp /Users/Chant/Documents/大数据0627/hive/mysql-connector-java-5.1.32-bin.jar root@node03:/root[root@node03 conf]# cp /root/mysql-connector-java-5.1.32-bin.jar ../lib/ 启动hive服务端1[root@node03 lib]# hive --service metastore 可以看到服务端的9083端口已经启动。 客户端配置12[root@node04 conf]# mv hive-default.xml.template hive-site.xml[root@node04 conf]# vi hive-site.xml .,$-1d删除原有的configuration，修改为如下内容 1234567891011121314&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node03:9083&lt;/value&gt; &lt;/property&gt; 同样的，此时如果启动客户端会报错java.lang.IncompatibleClassChangeError: Found class jline.Terminal，所以去替换hadoop的jline。 123[root@node04 lib]# cp jline-2.12.jar /opt/sxt/hadoop-2.6.5/share/hadoop/yarn/lib/[root@node04 lib]# cd /opt/sxt/hadoop-2.6.5/share/hadoop/yarn/lib/[root@node04 lib]# rm -f jline-0.9.94.jar 这时候就可以启动hive了，做点简单的练习测试。 123456[root@node04 lib]# hivehive&gt; show tables;hive&gt; create table tbl (id int, name string);hive&gt; desc tbl;hive&gt; insert into tbl values (1,&apos;zhangsan&apos;);hive&gt; select * from tbl; 连接数据库，刷新，因为我们配置的&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;所以能看到多了一个hive数据库，内容和单用户模式下一样，能找到TBLS和COLUMNS_V2。访问http://node01:50070/ 可以看到表数据存在hdfs中。]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Bigdata</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HADOOP--MapReduce深入源码-Mapper]]></title>
    <url>%2F2016%2F09%2F08%2FHadoop-mapper%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[MapReduce Mapper源码解析。 概览鼠标移到Mapper上，查看文档，先看个大概，有个大概的逻辑框架后再去看源码。 点击Mapper，Command+F12查看文件结构，找到run方法。 setup开启服务连接（比如连接数据库，打开某个文件）中间循环传一行执行一次map，最后cleanup关闭连接，释放资源。setup和cleanup中除了一行注释外，并无操作，所以是需要我们自己来实现。可以看出map的输入来自context.nextKeyValue()，输出通过context.write()。即：context.nextKeyValue()–&gt;Mapper.map()–&gt;context.write()随便找个地方输入MapTask，然后点击，进入到org.apache.hadoop.mapred下的MapTask，选中MapTask,Command+F12打开文件结构，键盘敲击run找到run方法并进入。从注释看出，运行任务时，打印出的map百分之几的那些进度信息是在这里定义的。往下翻找到runNewMapper并点击（因为我们用的是新的API）。看注释，找到// get an output object，里面有一段try代码块，可以看到先初始化，然后调用了mapper.run()，之前看过的这个mapper.run()就是通过循环一行行去读数据并执行map方法。 反射Mapper往上翻，回到runNewMapper的第二个注释// make a mapper（第一个注释是准备创建上下文，所以就略过了，主要看计算框架）。打开getMapperClass()的继承关系，找到JobContextImpl可以看到，它的默认取得是Mapper.class而Mapper中的map是默认直接输出，开发时的业务运算逻辑就没写进去，所以这个MAP_CLASS_ATTR肯定是要由用户配置的，Mapper类由开发人员自己实现（比如WordCount中的MyMapper）。所以最终// make a mapper阶段就是将用户所写的Mapper反射出来成一个对象。 输入input// make the input format阶段也是同理，上一篇客户端源码中已经讲过，默认取TextInputFormat.class，可通过job.setInputFormatClass()来设置INPUT_FORMAT_CLASS_ATTR。// rebuild the input split阶段，一个split对应一个map，客户端会生成split列表，这个split列表支撑了计算向数据移动，这里是其中的一个计算map任务，把split重新构造了一下。往下继续，点击NewTrackingRecordReader， 可见，input = new NewTrackingRecordReader，NewTrackingRecordReader底层有一个real，而这个real是由createRecordReader返回的一个LineRecordReader。 所以翻到try代码块中的Initialize初始化阶段，打开实现关系，input是NewTrackingRecordReader，所以这里应该选择NewTrackingRecordReader发现里面调用的是real的初始化打开实现关系，real是LineRecordReader，所以选择LineRecordReader。LineRecordReader中的initialize首先准备好了split的开始和结束位置，然后通过文件路径拿到文件。接着看注释// open the file and seek to the start of the split，其实注释说得很清楚了，先open开启文件的IO输入流，如果这里从头开始读，一定会读到别的block块或者别的split去了，所以一定要用seek方法，将你这个计算要读的split的起始位置的偏移量给这个流设置进去。流开启后，通过in将输入流fileIn绑定到SplitLineReader，注意看注释及那段代码，通过in将第一行抛弃（里面new了一个匿名对象Text，匿名对象用完就被销毁了，以此达到抛弃第一行的目的，同时将偏移量start移到了下一行），避免出现文件切割造成的乱码和单词被切开的问题。初始化的主要功能就是这个。计算向数据移动是由客户端来支撑的，本地化数据读取就是由初始化的这段代码来完成的。 初始化后调用run方法，run方法循环迭代context.nextKeyValue()同时调用map方法，这个context来自于mapper.run(mapperContext)调用时传入的mapperContext，mapperContext来自于mapContext，mapContext中传的参数有输入input和输出output。点击MapContextImpl，其输入reader就是之前的input它的nextKeyValue()调用的reader的nextKeyValue()，也就是input的nextKeyValue()，而input是NewTrackingRecorder。所以，打开继承关系，找到NewTrackingRecorder并进入。里面实际调用的是real的nextKeyValue，而real是LineReacordReader，所以打开实现关系，找到LineReacordReader进入。可以看到，nextKeyValue主要做了四件事： set方法设置pos 给Key和Value赋值 pos更新 返回布尔型 而getCurrentKey()和getCurrentValue()就只是直接返回Key和Value就好了。整理一下整个流程：用逻辑语言描述一遍： 输出ouput看完了输入，接下来看输出。回到runNewMapper()，找到注释// get an output object所以reduce是可以设置的，使用语句job.setNumReduceTasks(n)来设置recude数量，key多的话就多设置reduce，因为多个reduce是并行计算的。点击NewOutputCollector，可见分区数量等于reduce数量。而分区器partitioner是通过反射得到的。打开getPartitionerClass的实现关系，找到JobContextImpl进入。可以看到用户可以设置partitioner，不设置的话，其默认值是HashPartitioner通过语句job.setPartitionerClass(cls);来设置partitioner，自己实现这个partitioner类。点击HashPartitioner，可以看到其实它和简单，就是将Key的哈希值和Reduce数取模，返回余数，打标签，让Reducer知道这条数据该去哪个Reduce。这里是一个调优的重点。因为HashPartioner并不清楚数据具体情况，有可能发生数据倾斜（比如，大部分数据取模后都等于1，都到同一个Reduce里去了），这里可以根据具体场景自己实现Partitioner。举个栗子：比如数据是乱序的数字，需求是全排序。那么可以在自己实现的Partioner中写个判断，1~100返回0，100~200返回1，这样来尽量让他们均匀的分布到每个Reduce中去。回到NewOutputCollector，如果partions等于1，那就直接返回0，所有数据去到同一个Reduce。点击createSortingCollector 先找到return collector，再一步步往上看。collector反射于subClazz，subClazz来自于clazz，clazz是for循环传入的collectorClasses，collectorClasses来自用户配置或默认的MapOutputBuffer。所以这个collector默认是MapOutputBuffer（下图中按数字顺序查看）。而这个MapOutputBuffer就是环形缓冲区（后面再讲，这个环形是个挺机智的机制😂）。可以点进去看一下，它有1109行代码，如果你能写一个更好的环形缓冲区，那就可以上天了。关于环形缓冲区源码详细解析请戳MapReduce之mapOutputBuffer解析。 我们来看一下这个环形缓冲区的初始化，找到colletor.init()打开实现关系，找到MapOutputBuffer进入。MAP_SORT_SPILL_PERCENT就是环形缓冲区溢写的阈值，默认为80%。IO_SORT_MB就是环形缓冲区的大小，默认100M。这是两个调优点，如果数据一行就好几百兆，那么显然要调大IO_SORT_MB。如果map运算速度比较快，剩下的20%不足以满足map的输出（比如：80%被锁住正在溢写，而剩下20%由于map计算较快，也已经也已经写满，就阻塞了），那么应该调小MAP_SORT_SPILL_PERCENT，可以直接调成50%，看看有没有提升，再去微调，不要一点点的调。在默认情况下，block大小hadoop1.0是64M，hadoop2.0是128M，而split大小默认等于block大小，所以map会读取128M，输出的时候也就差不多100M左右，最多溢写两次，所以IO_SORT_MB默认情况下还是可以的。 hadoop在执行MapReduce任务时，在map阶段，map函数产生的输出，并不是直接写入磁盘的。为了提高效率，它将输出结果先写入到内存中（即环形内存缓冲区，默认大小100M），再从缓冲区（溢）写入磁盘。缓存为什么要设计成环形的？有什么好处？答：使输入输出并行工作，即“写缓冲”可以和“溢写”并行。“溢写”工作由单独的线程来做。 往下看，可以看到默认的排序器用的是快速排序QuickSorter，这里也是一个调优点，可以自行配置排序器。继续往下，K/V序列化这里，看到有一个比较器。点击getOutputComparator，经鼠标放到getMapOutputKeyClass上，可以看到这里默认取的是map输出的key的比较器，你也可以用job.setSortComparatorClass(cls);配置自己实现的比较器，这就允许你map输出时用的比较器和放到环形缓冲区后最终输出时的比较器可以不同。我们可以找个简单的类型比如IntWritable来看一下它的comparator继续看MapOutputBuffer的init()，往下翻，可以看到// combiner，这个combiner在一开始的概览里有描述 Users can optionally specify a combiner, via Job.setCombinerClass(Class), to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer. 就是可以调用combiner在map端合并数据，这样就可以减少要传输的数据量和reduce端的压力。但是要注意的是，不是什么数据都适合简单的合并，比如平均数，简单的合并后是不对的（比如：$(\frac{a+b}{2}+\frac{c+d}{2})/2\neq\frac{a+b+c+d}{4}$），这时候需要注意书写正确的合并方法。往下找到try代码块中的spillThread，shift+Command+B打开声明类型找到try代码块中sortAndSpill，看名字就知道这方法是负责排序和溢写。可以看到排序用的sorter，而之前已经看到过sorter的默认排序方法是快速排序。 参考资料以下是环形缓冲区的一些源码解读和排序算法的文章： MapReduce之mapOutputBuffer解析–详细 MapReduce源码解析–环形缓冲区–详细 map的环形内存缓冲区–概览 常用排序算法总结]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HADOOP--MapReduce深入源码-Client]]></title>
    <url>%2F2016%2F09%2F05%2FHadoop-client%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[MapReduce客户端提交过程的源码，以及手写wordCount测试。 一入源码深似海，源码深深深几许 点击submit 点击submitJobInternal 往下翻，看到客户端启动后先创建切片列表由于我使用的是新的API，所以点击writeNewSplites这里不是直接点，而是command+option+B打开继承关系，选择JobContextImpl。这里用户如果设置了InputFormat.class则取用户自己实现的，否则取默认值TextInputFormat.class也就是说可以在自己的代码中设置InputFormat.class，自己去实现ooxx.class这个类，这样就可以控制它是按行还是按一块数据来输入。setInputFormatClass里面设置的就是INPUT_FORMAT_CLASS_ATTR，而getInputFormatClass里get的也是这个INPUT_FORMAT_CLASS_ATTR。 这里顺带说一下,进入Job.java,点击JobContext–&gt;MRJobConfig可以看到很多常量属性，这与你的hadoop配置文件有关，搜索Default,找到DEFAULT_MAP_MEMORY_MB，可以看到他有默认配置且为1G。这里是一个调优点。当你的数据每一行都是好几百兆时，1G的内存会导致频繁的磁盘IO？？？，或者内存溢出，当你确定计算没有问题但是提交计算总是报内存溢出，可以试试将它调大。DEFAULT_REDUCE_MEMORY_MB只有1G，这导致你的buffer会很小，会频繁地触发磁盘IO（比如一次读3G数据到内存后写入磁盘一次，和读3次1G后写3次到磁盘。IO调用次数越多，开销越大）。所以DEFAULT_REDUCE_MEMORY_MB基本上是必然要调的。回到TextInputFormat，Ctrl+H打开继承层级，发现它继承自FileInputFormat，对比我们创建输入路径时的语句，可见TextInputFormat是通过继承来获取到父类中的Job配置和input路径。 接下来，看getSplits。打开实现关系，选择FileInputFormat。点击getFormatMinSplitSize，可见其固定返回1。 点击getMinSplitSize,可见其默认值为1，所以long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job)); 计算出的默认值也是1，且用户可以干预调节。通过FileInputFormat这个类来调节SplitSize。点击getMaxSplitSize，可以看到MaxSplitSize的默认值是long类型的最大值。返回getSplits接着往下翻。判断是否为本地文件，否则读取集群的blockLocations。标红的代码在之前的测试中使用过，用来取回所有的block位置信息。从computeSplitSize中可以看出，切片大小splitSize的默认值就是block块大小blockSize。如果想让切片的在block中，应该设置MaxSize，令其小于blockSize；如果想让切片大于block块，应该设置MinSize，令其大于blockSize。鼠标放到makeSplit上，可以看出split列表中的信息就是makeSplit的参数，文件，偏移量，split长度，节点主机名。再点击上面的getBloclIndex向上翻，找到// Create the splits for the job的地方，不知道你还记不记得，刚才看的那么多源码是writeSplits的实现，这里的maps就是writeSplits计算出的需要多少个map（一个block中的splits数量）。可以重新追溯着复盘一下，maps&lt;–writeSplits&lt;–writeNewSplits(返回array.length&lt;–splits.toarray)&lt;–getSplits(返回splits&lt;–for循环迭代files过程中splits.add())下面的就看着注释走马观花吧，到最后，终于submit了。谢天谢地，我快要疯了。从源码可以看出客户端主要做了以下事情，计算向数据移动，本地化数据读取： 配置完善：个性化 检查路径 计算Split:mapsblocklocation：位置信息file1.txt,0,1048576,node02,node03,node04file1.txt,1048576,1048576,node01,node02,node03 资源提交到HDFS 提交任务接下来由集群中AppMaster向RM申请资源（参照split）。 测试理解写一个WordCount程序，在Mapper中添加一行Thread.sleep(999999);如下： 12345while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); Thread.sleep(999999); context.write(word, one);&#125; 打jar包拷贝到你的HDFS主节点，运行。看到map 0% reduce 0%后，新开一个ssh窗口,查看hdfs的根目录，发现多了一个tmp目录，一步步深入其中。可以将这些文件get到本地，然后发现这里面的job.jar和一开始上传的testHadoop-1.0-SNAPSHOT.jar大小一样，其实内容也是一样的，集群中任一节点启动MapTask任务后，就会把这个jar包拿过去，然后反射出里面的MyMapper。然后任一节点启动ReduceTask任务后，也会从hdfs中把这个jar包拿过去，反射出里面的MyReducer,这样就可以分布式地计算了。12-rw-r--r-- 1 root root 13K Jul 7 03:44 testHadoop-1.0-SNAPSHOT.jar-rw-r--r-- 1 root root 13K Jul 7 03:50 job.jar 查看job.split,发现里面的乱码，这部分乱码之前源码中看到的makeSplit的参数，即split列表中的信息：文件，偏移量，split长度（大小 ），节点主机名。12[root@node01 tmp]# vi job.splitSPL^@^@^@^A/org.apache.hadoop.mapreduce.lib.input.FileSplit*hdfs://mycluster/sxt/mr/wc/input/hello.txt^@^@^@^@^@^@^@^@^@^@^@^@^@%ú? WordCount示例将Job，Mapper，Reducer的example里的代码拼起来就刚好是一个WordCount程序，自己稍作修改就行。 Client类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.chant.mr.wc;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.db.DBInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * Created by Chant on 2017/7/8. */public class MyJob &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(true); Job job = Job.getInstance(conf); // Create a new Job// Job job = Job.getInstance(); job.setJarByClass(MyJob.class); // Specify various job-specific parameters job.setJobName("myjob");// job.setInputPath(new Path("in"));// job.setOutputPath(new Path("out"));// 上面的是示例里的传统方法，现在用以下方法,因为还可以从数据库或者其他地方拿数据。 Path input = new Path("/sxt/mr/wc/input"); FileInputFormat.addInputPath(job,input);// DBInputFormat.setInput(); Path output = new Path("/sxt/mr/wc/output"); if (output.getFileSystem(conf).exists(output))//也可以用FileSystem fs = FileSystem.get(conf);然后fs.exist(output) &#123; output.getFileSystem(conf).delete(output); &#125; FileOutputFormat.setOutputPath(job,output); job.setMapperClass(MyMapper.class);// map的输出结果到buffer环里，会序列化。需要告诉reduce你这个反序列化后是什么类型，reduce会将反序列化后的类型强转为对应类型。 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setReducerClass(MyReducer.class); // Submit the job, then poll for progress until the job is complete job.waitForCompletion(true); &#125;&#125; Mapper类1234567891011121314151617181920212223242526package com.chant.mr.wc;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;import java.util.StringTokenizer;/** * Created by Chant on 2017/7/8. */public class MyMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;// 为什么要把word和one定义到map之外呢？ 因为map端可能读入很多行，有10万行就调用十万次map方法，如果word和one定义到map里面，就会创建10万个word和one对象，gc就会很忙，程序阻塞。 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); Thread.sleep(999999); context.write(word, one); &#125; &#125;&#125; Reduceer类123456789101112131415161718192021222324package com.chant.mr.wc;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/** * Created by Chant on 2017/7/8. */public class MyReducer extends Reducer&lt;Text,IntWritable, Text,IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125;&#125;]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java阻塞与非阻塞的同步异步，以及回调]]></title>
    <url>%2F2016%2F08%2F19%2Fjava%E5%9B%9E%E8%B0%83%2F</url>
    <content type="text"><![CDATA[怎样理解阻塞非阻塞与同步异步的区别，回调又是什么鬼？ 阻塞非阻塞与同步异步“阻塞”与”非阻塞”与”同步”与“异步”不能简单的从字面理解，提供一个从分布式系统角度的回答。 1.同步与异步 同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。典型的异步编程模型比如Node.js 举个通俗的例子： 你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。 而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 2.阻塞与非阻塞 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 还是上面的例子，你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 同步异步的经典例子老张爱喝茶，废话不说，煮开水。出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶)。 老张把水壶放到火上，立等水开。（同步阻塞） 老张觉得自己有点傻 老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞） 老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后,能大声发出嘀~~~~的噪音。 老张把响水壶放到火上，立等水开。（异步阻塞） 老张觉得这样傻等意义不大 老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞） 老张觉得自己聪明了。 所谓同步异步，只是对于水壶而言。普通水壶，同步；响水壶，异步。虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。 所谓阻塞非阻塞，仅仅对于老张而言。立等的老张，阻塞；看电视的老张，非阻塞。情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。 回调的经典例子回调：就是A类中调用B类中的某个方法C，然后B类中反过来调用A类中的方法D，D这个方法就叫回调方法，这样子说你是不是有点晕晕的，其实我刚开始也是这样不理解，看了人家说比较经典的回调方式： • Class A实现接口CallBack callback——背景1 • class A中包含一个class B的引用b ——背景2 • class B有一个参数为callback的方法f(CallBack callback) ——背景3 • A的对象a调用B的方法 f(CallBack callback) ——A类调用B类的某个方法 C • 然后b就可以在f(CallBack callback)方法中调用A的方法 ——B类调用A类的某个方法D 回调代码示例告诉他干活的结果。这个例子其实是一个回调+异步的例子，再举一个例子，A程序员写了一段程序a，其中预留了回调函数接口，并封装好了该程序，程序员B让a调用自己的程序b中的一个方法，于是，他通过a中的接口回调自己b中的方法，到这里你可能似懂非懂了。 首先创建一个回调接口，让老板得告知干完活如何找到他的方式：留下老板办公室地址： 123456789101112package net.easyway.test;/** * 此接口为联系的方式，不论是电话号码还是联系地址，作为 * 老板都必须要实现此接口 * @author Administrator * */public interface CallBackInterface &#123; public void execute();&#125; 创建回调对象，就是老板本人，因为员工干完活后要给他打电话，因此老板必须实现回调接口，不然员工去哪里找老板？ 12345678910111213141516package net.easyway.test;/** * 老板是作为上层应用身份出现的，下层应用（员工）是不知道 * 有哪些方法，因此他想被下层应用（员工）调用必须实现此接口 * @author Administrator * */public class Boss implements CallBackInterface &#123; @Override public void execute() &#123; System.out.println("收到了！！" + System.currentTimeMillis()); &#125;&#125; 创建控制类，也就是员工对象，他必须持有老板的地址（回调接口），即使老板换了一茬又一茬，办公室不变，总能找到对应的老板。 123456789101112131415161718192021222324252627package net.easyway.test;/** * 员工类，必须要记住，这是一个底层类，底层是不了解上层服务的 * @author Administrator * */public class Employee &#123; private CallBackInterface callBack = null; //告诉员工老板的联系方式，也就是注册 public void setCallBack(CallBackInterface callBack)&#123; this.callBack = callBack; &#125; //工人干活 public void doSome()&#123; //1.开始干活了 for(int i=0;i&lt;10;i++)&#123; System.out.println("第【" + i + "】事情干完了！"); &#125; //2.告诉老板干完了 callBack.execute(); &#125;&#125; 测试类代码： 123456789101112131415package net.easyway.test;public class Client &#123; public static void main(String[] args) &#123; Employee emp = new Employee(); //将回调对象（上层对象）传入，注册 emp.setCallBack(new Boss()); //开启控制器对象运行 emp.doSome(); &#125;&#125; 回调总结要明确的一点是，首先要搞清回调函数出现的原因，也就是适用场景，才能搞清楚回调机制，不然事倍功半。 最后，再举一例，为了使我们写的函数接近完美，就把一部分功能外包给别人，让别人个性化定制，至于别人怎么实现不管，我唯一要做的就是定义好相关接口，这一设计允许了底层代码调用高层定义的子程序，增强程序灵活性，和反射有着异曲同工之妙，这才是回调的真正原因！ 用一段话来总结下回调：上层模块封装时，很难预料下层模块会如何实现，因此，上层模块只需定义好自己需要但不能预料的接口（也就是回调接口），当下层模块调用上层模块时，根据当前需要的实现回调接口，并通过注册或参数方式传入上层模块即可，这样就实现下层调用上层，并且上层还能根据传入的引用来调用下层的具体实现，将程序的灵活性大大的增加了。 参考文章 Java回调机制解析–代码 经典的回调方式 怎样理解阻塞非阻塞与同步异步的区别？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA中文乱码问题]]></title>
    <url>%2F2016%2F08%2F17%2FIDEA%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[导入eclipse项目时，出现中文乱码的所有解决办法。 1.导入eclipse项目出现中文乱码这个情况，通常都是因为编码为GBK导致的。第一步是点击右下角，有一个显示你当前编码的地方，然后选择GBK（由于我的项目之前是GBK编码，所以在这里我要是选择的GBK）。另外提醒一点，如果你的idea右下角没有这个按钮，请在你的编码界面中随意右键，然后选择“File Encoding”，效果一样。 第二步会出现如下提示，这里很重要，不要选错，先选择“Reload”，这里请严格按照我说的来，文章后面会告诉你如果选错会有什么后果 接着你就会看到乱码已经变成中文了，但是这对我还没结束，由于我将页面改成了GBK编码，但这并不是我想要的，我想要的是utf8的编码格式。 第三步，就是再次点击设置编码地方，然后选择UTF-8格式，这次就是选择Convert，这就结束了。 相信有些朋友已经有点头绪了。这个“Reload”选择后不会改变文件和内容的编码格式，而是将IDE本身的解码格式由我原先的UTF-8换成了 GBK，由GBK的解码格式解GBK的文件就不会再看到乱码。而“Convert”是将GBK格式的文件内容转换成了UTF-8，同时将IDE的解码格式 也换成UTF-8。所以之前说的，如果你第一次选择了“Convert”那么就会由原来的乱码弄成另一种乱码，反正我是没弄回来过。 控制台输出是乱码比如:System.out.println(“中文”);执行这句话控制台输出乱码，这个问题在Run-&gt;Edit configurations中的VM options里加上-Dfile.encoding=UTF-8，就好了，这种问题是操作系统不是中文环境导致的。 所有配置都没问题但仍然输出乱码这个时候只有一种解释：IDEA把你的字体编码弄错了，但是在哪里弄错的呢。经过多次排查寻找，终于，在项目的目录下有个.idea的文件夹，这个文件夹里有个encodings.xml的文件,里面记录了你某些文件对应的特殊编码，为什么会有这种编码呢，因为之前我无意中点了右下角的编码，改了一下，就被idea记录到 encodings.xml中，当你再次访问的时候，它就会用那种编码。我说IDEA你那么智能你妈知道吗？只要把encodings.xml里面的除了UTF-8的都删了就好啦（我的所有字体都是UTF-8）！ 参考文章 idea中的汉语注释出现乱码的解决方案 小技巧！两分钟解决IntelliJ IDEA中文乱码问题]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper详解]]></title>
    <url>%2F2016%2F08%2F06%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[zookeeper学习笔记。 Zookeeper介绍在Zookeeper的官网上有这么一句话：ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. 即：1.配置管理，2.名字服务，3.提供分布式同步4.以及集群管理 Zookeeper是Google的Chubby一个开源的实现，是 Hadoop 的分布式协调服务。它包含一个简单的原语集，分布式应用程序可以基于它实现同步服务，配置维护和命名服务等。 Zookeeper特点大部分分布式应用需要一个主控、协调器或控制器来管理物理分布的子进程（如资源、任务分配等），目前，大部分应用需要开发私有的协调程序（JournalNode、Sentinel等），缺乏一个通用的机制，协调程序的反复编写浪费，且难以形成通用、伸缩性好的协调器，ZooKeeper提供通用的分布式锁服务，用以协调分布式应用，相比于Keepalived，后者采用优先级监控，监控节点不好管理，没有协同工作，功能单一，可扩展性差 Zookeeper应用Hadoop使用Zookeeper的事件处理确保整个集群只有一个NameNode，存储配置信息等。HBase使用Zookeeper的事件处理确保整个集群只有一个HMaster，察觉HRegionServer联机和宕机，存储访问控制列表等。 Zookeeper安装配置解压，配置环境变量 node02上解压zookeeper包12[root@node02 software]# tar xf zookeeper-3.4.6.tar.gz[root@node02 software]# mv zookeeper-3.4.6 /opt/sxt/ 2.修改/etc/profile文件，配置zookeeper路径1234567[root@node02 sxt]# vi + /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67export HADOOP_PREFIX=/opt/sxt/hadoop-2.6.5export ZOOKEEPER_PREFIX=/opt/sxt/zookeeper-3.4.6export PATH=$JAVA_HOME/bin:$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$ZOOKEEPER_PREFIX/bin[root@node02 sxt]# . /etc/profile 3. 配置zookeeper12345678910111213141516171819202122232425262728293031[root@node02 sxt]# cd zookeeper-3.4.6/conf[root@node02 conf]# cp zoo_sample.cfg zoo.cfg[root@node02 conf]# vi zoo.cfg# zookeeper数据存放目录dataDir=/var/sxt/zookeeper#发送心跳的间隔时间，单位：毫秒tickTime=2000#日志存放位置dataLogDir=/var/sxt/zookeeper/log#客户端连接Zookeeper服务器的端口，Zookeeper会监听这个端口，接受客户端的访问请求clientPort=2181#这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper服务器的客户端，而是 Zookeeper 服务器集群中连接到Leader的Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过5个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表 明这个客户端连接失败。总的时间长度就是 5*2000=10秒initLimit=5#这个配置项标识Leader与Follower之间发送消息，请求和应答时间长度，最长不能超过多少个tickTime的时间长度，总的时间长度就是2*2000=4秒syncLimit=2# 在文件末尾追加以下内容server.A=B:C:DA是一个数字，表示这个是第几号服务器；B是这个服务器的ip地址；C表示的是这个服务器与集群中的Leader服务器交换信息的端口；D表示的是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于B都是一样的，所以不同的Zookeeper实例通信端口号不能一样，所以要给它们分配不同的端口号选举leader有几个影响因素：1. 集群的投票2. 谁的缓存中数据最多3. 靠ID来区分server.1=node02:2888:3888server.2=node03:2888:3888server.3=node04:2888:3888[root@node02 conf]# mkdir /var/sxt/zookeeper[root@node02 conf]# cd /var/sxt/zookeeper/[root@node02 zookeeper]# echo 1 &gt; myid 4. 配置node03和node04的zookeeper1234567891011[root@node02 sxt]# cd /opt/sxt[root@node02 sxt]# scp -r zookeeper-3.4.6 node03:`pwd`[root@node02 sxt]# scp -r zookeeper-3.4.6 node04:`pwd`[root@node02 sxt]# scp /etc/profile node03:/etc[root@node02 sxt]# scp /etc/profile node04:/etc[root@node03 ~]# . /etc/profile[root@node03 ~]# mkdir /var/sxt/zookeeper[root@node03 ~]# echo 2 &gt; /var/sxt/zookeeper/myid[root@node04 ~]# . /etc/profile[root@node04 ~]# mkdir /var/sxt/zookeeper[root@node04 ~]# echo 3 &gt; /var/sxt/zookeeper/myid 5. 启动zookeeper集群12345678910[root@node02 ~]# zkServer.sh start[root@node03 ~]# zkServer.sh start[root@node04 ~]# zkServer.sh start# 当正常启动服务的集群过半（我们这里过半是两台）时，zookeeper就可以决策出主从关系，当启动的服务不过半（我们这里是只启动一台）时，zkServer的状态是报错状态，id最大的为leader，node04的zookeeper的id为3，所以node04为leader[root@node02 ~]# zkServer.sh statusMode: follower[root@node03 ~]# zkServer.sh statusMode: follower[root@node04 ~]# zkServer.sh statusMode: leader Zookeeper角色领导者（leader），负责进行投票的发起和决议，更新系统状态学习者（learner），包括跟随者（follower）和观察者（observer），follower用于接受客户端请求并想客户端返回结果，在选主过程中参与投票，Observer可以接受客户端连接，将写请求转发给leader，但observer不参加投票过程，只同步leader的状态，observer的目的是为了扩展系统，提高读取速度客户端（client），请求发起方 Zookeeper选举如何在zookeeper集群中选举出一个leader，zookeeper使用了三种算法：LeaderElectionAuthFastLeaderElectionFastLeaderElection具体使用哪种算法，在配置文件中是可以配置的，对应的配置项是”electionAlg”，其中1对应的是LeaderElection算法，2对应的是AuthFastLeaderElection算法，3对应的是FastLeaderElection算法。默认使用FastLeaderElection算法。我们分析它的选举机制。 选择机制中的概念服务器IDserver.1=node02:2888:3888server.2=node03:2888:3888server.3=node04:2888:3888这里的1、2、3就是服务器的ID，ID越大在选择算法中的权重越大。数据ID每个在zookeeper服务器先读取当前保存在磁盘的数据，zookeeper中的每份数据，都有一个对应的id值，这个值是依次递增的，换言之，越新的数据,对应的ID值就越大，在选举算法中数据越新权重越大。逻辑时钟或者叫投票的次数，同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加，然后与接收到的其它服务器返回的投票信息中的数值相比，根据不同的值做出不同的判断。选举状态LOOKING，竞选状态。FOLLOWING，随从状态，同步leader状态，参与投票。OBSERVING，观察状态，同步leader状态，不参与投票。LEADING，领导者状态。选举消息内容在读取数据完毕之后，每个zookeeper服务器发送自己选举的leader（首次选自己），这个协议中包含了以下几部分的数据： 所选举leader的id（就是配置文件中写好的每个服务器的id)），在初始阶段，每台服务器的这个值都是自己服务器的id，也就是它们都选举自己为leader 服务器最大数据的id，这个值大的服务器，说明存放了更新的数据。 逻辑时钟的值，这个值从0开始递增，每次选举对应一个值，也就是说：如果在同一次选举中,那么这个值应该是一致的。逻辑时钟值越大，说明这一次选举leader的进程更新。 本机在当前选举过程中的状态，有以下几种：LOOKING，FOLLOWING，OBSERVING，LEADING。 选举流程简述1.从磁盘读取数据，更新数据ID2.向其他节点发送投票值，包括服务器ID、数据ID、逻辑时钟、选举状态3.接受来自其他节点的数据 每台服务器将自己服务器的以上数据发送到集群中的其他服务器之后，同样的也需要接收来自其他服务器的数据，它将做以下的处理：（1）如果所接收数据中服务器的状态还是在选举阶段(LOOKING 状态)，那么首先判断逻辑时钟值，又分为以下三种情况： a) 如果发送过来的逻辑时钟大于目前的逻辑时钟，那么说明这是更新的一次选举，此时需要更新一下本机的逻辑时钟值，同时将之前收集到的来自其他服务器的选举清空，因为这些数据已经不再有效了，然后判断是否需要更新当前自己的选举情况，在这里是根据选举leader id，保存的最大数据id来进行判断，这两种数据之间对这个选举结果的影响的权重关系是：首先看数据id，数据id大者胜出；其次再判断leader id，leader id大者胜出。然后再将自身最新的选举结果(也就是上面提到的四种数据)广播给其他服务器。 b) 发送过来数据的逻辑时钟小于本机的逻辑时钟，说明对方在一个相对较早的选举进程中，这里只需要将本机的数据发送过去 c) 两边的逻辑时钟相同，此时也只是调用totalOrderPredicate函数判断是否需要更新本机的数据，如果更新了再将自己最新的选举结果广播出去（2）然后再处理两种情况： a) 服务器判断是不是已经收集到了所有服务器的选举状态，如果是，那么这台服务器选举的leader就定下来了，然后根据选举结果设置自己的角色(FOLLOWING还是LEADER)，然后退出选举过程 b) 即使没有收集到所有服务器的选举状态，也可以根据该节点上选择的最新的leader是不是得到了超过半数以上服务器的支持，如果是，那么当前线程将被阻塞等待一段时间(这个时间在finalizeWait定义)看看是不是还会收到当前leader的数据更优的leader，如果经过一段时间还没有这个新的leader提出来，那么这台服务器最终的leader就确定了，否则进行下一次选举（3) 如果所接收服务器不在选举状态，也就是在FOLLOWING或者LEADING状态做以下两个判断: a) 如果逻辑时钟相同，将该数据保存到recvset，如果所接收服务器宣称自己是leader，那么将判断是不是有半数以上的服务器选举它，如果是则设置选举状态，退出选举过程b) 否则这是一条与当前逻辑时钟不符合的消息，那么说明在另一个选举过程中已经有了选举结果，于是将该选举结果加入到outofelection集合中，再根据outofelection来判断是否可以结束选举，如果可以也是保存逻辑时钟，设置选举状态，退出选举过程 集群启动leader选举的过程假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的，假设这些服务器依序启动，来看看会发生什么.1) 服务器1启动，此时只有它一台服务器启动了，它发出去的信息没有任何响应，所以它的选举状态一直是LOOKING状态2) 服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1,2还是继续保持LOOKING状态3) 服务器3启动，根据前面的理论分析，服务器3成为服务器1,2,3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader4) 服务器4启动，根据前面的分析，理论上服务器4应该是服务器1,2,3,4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接受当小弟的命了5) 服务器5启动，同4一样，当小弟 Leader down掉之后的选举 Zookeeper的一些概念假死在分布式系统中，每个节点上都存在一个监控者来监控本机的状态，但是监控者很难判定其他的节点的状态，心跳是一种可靠的途径，Zookeeper就是使用心跳来判断客户端是否仍然活着。使用ZooKeeper来做master HA基本都是同样的方式，每个节点都尝试注册一个象征master的临时节点，没有注册成功的成为slave，并且通过watch机制监控着master所创建的临时节点，Zookeeper通过内部心跳机制来确定master的状态，一旦master出现意外，Zookeeper能很快获悉并且通知其他的slave，其他slaver在之后作出相关反应。这样就完成了一个切换。这种模式也是比较通用的模式，基本大部分都是这样实现的，但是，心跳出现超时可能是master挂了，但是也可能是master，zookeeper之间网络出现了问题，我们把因为master和zookeeper之间的网络问题造成的两者之间不能通信这种情况称作假死。 网络分区/脑裂lit-brain” mean? “Split brain” is a condition whereby two or more computers or groups of computers lose contact with one another but still act as if the cluster were intact. This is like having two governments trying to rule the same country. If multiple computers are allowed to write to the same file system without knowledge of what the other nodes are doing, it will quickly lead to data corruption and other serious problems. Split-brain is prevented by enforcing quorum rules (which say that no group of nodes may operate unless they are in contact with a majority of all nodes) and fencing (which makes sure nodes outside of the quorum are prevented from interfering with the cluster).在“双机热备”高可用（HA）系统中，当联系2个节点的“心跳线”断开时，本来为一整体、动作协调的HA系统，就分裂成为2个独立的个体。由于相互失去了联系，都以为是对方出了故障，2个节点上的HA软件像“裂脑人”一样，“本能”地争抢“共享资源”、争起“应用服务”，就会发生严重后果：或者共享资源被瓜分、2边“服务”都起不来了；或者2边“服务”都起来了，但同时读写“共享存储”，导致数据损坏（常见如数据库轮询着的联机日志出错）。即：过半机制解决了脑裂问题。 半数机制在zookeeper的选举在zookeeper的选举在zookeeper的选举在zookeeper的选举机制中，只要有过半的机器已经选好了leader，leader就被确认了，而不会询问每台服务器的意见，这就是半数机制。 顺序性客户端的更新顺序与它们被发送的顺序相一致。客户端发送给leader的请求，会被leader记录一个序列号，达到请求有序的效果，leader与所有flower之间会建立消息队列，请求的请求会放到follower队列中，达到有序的处理请求的效果。 可用性Zookeeper保证了可Zookeeper保证了可用性，数据总是可用的，没有锁。并且有一大半的节点所拥有的数据是最新的，实时的。如果想保证取得是数据一定是最新的，需要手工调用Sync() 原子性更新操作要么成功要么失败，没有第三种结果。 一致性一致性是指从系统外部读取系统内部的数据时，在一定约束条件下相同，即数据变动在系统内部各节点应该是同步的。根据一致性的强弱程度不同，可以将一致性级别分为如下几种： ①强一致性（strong consistency）。任何时刻，任何用户都能读取到最近一次成功更新的数据。 ②单调一致性（monotonic consistency）。任何时刻，任何用户一旦读到某个数据在某次更新后的值，那么就不会再读到比这个值更旧的值。也就是说，可获取的数据顺序必是单调递增的。③会话一致性（session consistency）。任何用户在某次会话中，一旦读到某个数据在某次更新后的值，那么在本次会话中就不会再读到比这值更旧的值，会话一致性是在单调一致性的基础上进一步放松约束，只保证单个用户单个会话内的单调性，在不同用户或同一用户不同会话间则没有保障。④ 最终一致性（eventual consistency）。用户只能读到某次更新后的值，但系统保证数据将最终达到完全一致的状态，只是所需时间不能保障。⑥弱一致性（weak consistency）。用户无法在确定时间内读到最新更新的值。 Zookeeper保证了最终一致性，原因： 1. 假设有2n+1个server，在同步流程中，leader向follower同步数据，当同步完成的follower数量大于 n+1时同步流程结束，系统可接受client的连接请求。如果client连接的并非同步完成的follower，那么得到的并非最新数据，但在一段时间后，所有数据都会同步。 2. follower接收写请求后，转发给leader处理；leader完成两阶段提交的机制。向所有server发起提案，当提案获得超过半数（n+1）的server认同后，将对整个集群进行同步，超过半数（n+1）的server同步完成后，该写请求完成。如果client连接的并非同步完成follower，那么得到的并非最新数据，但在一段时间后数据都会同步。 Session客户端与集群节点建立TCP连接后获得一个session，如果连接的Server出现问题，在没有超过Timeout时间时，可以连接其他节点，同一session期内的特性不变 Zookeeper数据模型ZnodeZookeeper的结构是目录型结构，便于管理逻辑关系，节点znode不是文件file，Znode中的信息包含最大1MB的数据信息和Zxid等元数据信息节点类型Znode有两种类型，短暂的（ephemeral）和持久的（persistent），短暂znode的客户端会话结束时，zookeeper会将该短暂znode删除，短暂znode不可以有子节点，持久znode不依赖于客户端会话，只有当客户端明确要删除该持久znode时才会被删除，Znode的类型在创建时确定并且之后不能再修改，Znode有四种形式的目录节点：PERSISTENTEPHEMERALPERSISTENT_SEQUENTIAL（有序）EPHEMERAL_SEQUENTIAL（有序） 事件监听WatcherWatcher是ZooKeeper 是一个核心功能，Watcher 可以监控目录节点的数据变化以及子目录的变化，一旦这些状态发生变化，服务器就会通知所有设置在这个目录节点上的Watcher，从而每个客户端都很快知道它所关注的目录节点的状态发生变化，而做出相应的反应。可以设置观察的操作：exists，getChildren，getData可以触发观察的操作：create，delete，setData 原子消息广播协议ZABZookeeper的核心是原子广播，这个机制保证了各个server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式和广播模式。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数server的完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和server具有相同的系统状态广播模式需要保证proposal被按顺序处理，因此zk采用了递增的事务id号(zxid)来保证。所有的提议(proposal)都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，低32位是个递增计数。 Zookeeper全解析—Paxos作为灵魂（摘抄）ZK Server最基础的东西是什么呢？我想应该是Paxos了。所以本文会介绍Paxos以及它在ZK Server中对应的实现。 先说Paxos，它是一个基于消息传递的一致性算法，Leslie Lamport在1990年提出，近几年被广泛应用于分布式计算中，Google的Chubby，Apache的Zookeeper都是基于它的理论来实现的，Paxos还被认为是到目前为止唯一的分布式一致性算法，其它的算法都是Paxos的改进或简化。有个问题要提一下，Paxos有一个前提：没有拜占庭将军问题。就是说Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。 关于Paxos的具体描述可以在Wiki中找到：http://zh.wikipedia.org/zh-cn/Paxos算法。网上关于Paxos分析的文章也很多。这里希望用最简单的方式加以描述并建立起Paxos和ZK Server的对应关系。 Paxos描述了这样一个场景，有一个叫做Paxos的小岛(Island)上面住了一批居民，岛上面所有的事情由一些特殊的人决定，他们叫做议员(Senator)。议员的总数(Senator Count)是确定的，不能更改。岛上每次环境事务的变更都需要通过一个提议(Proposal)，每个提议都有一个编号(PID)，这个编号是一直增长的，不能倒退。每个提议都需要超过半数((Senator Count)/2 +1)的议员同意才能生效。每个议员只会同意大于当前编号的提议，包括已生效的和未生效的。如果议员收到小于等于当前编号的提议，他会拒绝，并告知对方：你的提议已经有人提过了。这里的当前编号是每个议员在自己记事本上面记录的编号，他不断更新这个编号。整个议会不能保证所有议员记事本上的编号总是相同的。现在议会有一个目标：保证所有的议员对于提议都能达成一致的看法。 好，现在议会开始运作，所有议员一开始记事本上面记录的编号都是0。有一个议员发了一个提议：将电费设定为1元/度。他首先看了一下记事本，嗯，当前提议编号是0，那么我的这个提议的编号就是1，于是他给所有议员发消息：1号提议，设定电费1元/度。其他议员收到消息以后查了一下记事本，哦，当前提议编号是0，这个提议可接受，于是他记录下这个提议并回复：我接受你的1号提议，同时他在记事本上记录：当前提议编号为1。发起提议的议员收到了超过半数的回复，立即给所有人发通知：1号提议生效！收到的议员会修改他的记事本，将1好提议由记录改成正式的法令，当有人问他电费为多少时，他会查看法令并告诉对方：1元/度。 现在看冲突的解决：假设总共有三个议员S1-S3，S1和S2同时发起了一个提议:1号提议，设定电费。S1想设为1元/度, S2想设为2元/度。结果S3先收到了S1的提议，于是他做了和前面同样的操作。紧接着他又收到了S2的提议，结果他一查记事本，咦，这个提议的编号小于等于我的当前编号1，于是他拒绝了这个提议：对不起，这个提议先前提过了。于是S2的提议被拒绝，S1正式发布了提议: 1号提议生效。S2向S1或者S3打听并更新了1号法令的内容，然后他可以选择继续发起2号提议。 好，我觉得Paxos的精华就这么多内容。现在让我们来对号入座，看看在ZK Server里面Paxos是如何得以贯彻实施的。 小岛(Island)——ZK Server Cluster 议员(Senator)——ZK Server 提议(Proposal)——ZNode Change(Create/Delete/SetData…) 提议编号(PID)——Zxid(ZooKeeper Transaction Id) 正式法令——所有ZNode及其数据 貌似关键的概念都能一一对应上，但是等一下，Paxos岛上的议员应该是人人平等的吧，而ZK Server好像有一个Leader的概念。没错，其实Leader的概念也应该属于Paxos范畴的。如果议员人人平等，在某种情况下会由于提议的冲突而产生一个“活锁”（所谓活锁我的理解是大家都没有死，都在动，但是一直解决不了冲突问题）。Paxos的作者Lamport在他的文章”The Part-Time Parliament“中阐述了这个问题并给出了解决方案——在所有议员中设立一个总统，只有总统有权发出提议，如果议员有自己的提议，必须发给总统并由总统来提出。好，我们又多了一个角色：总统。 总统——ZK Server Leader 又一个问题产生了，总统怎么选出来的？oh, my god! It’s a long story. 在淘宝核心系统团队的Blog上面有一篇文章是介绍如何选出总统的，有兴趣的可以去看看：http://rdc.taobao.com/blog/cs/?p=162 现在我们假设总统已经选好了，下面看看ZK Server是怎么实施的。 情况一： 屁民甲(Client)到某个议员(ZK Server)那里询问(Get)某条法令的情况(ZNode的数据)，议员毫不犹豫的拿出他的记事本(local storage)，查阅法令并告诉他结果，同时声明：我的数据不一定是最新的。你想要最新的数据？没问题，等着，等我找总统Sync一下再告诉你。 情况二： 屁民乙(Client)到某个议员(ZK Server)那里要求政府归还欠他的一万元钱，议员让他在办公室等着，自己将问题反映给了总统，总统询问所有议员的意见，多数议员表示欠屁民的钱一定要还，于是总统发表声明，从国库中拿出一万元还债，国库总资产由100万变成99万。屁民乙拿到钱回去了(Client函数返回)。 情况三： 总统突然挂了，议员接二连三的发现联系不上总统，于是各自发表声明，推选新的总统，总统大选期间政府停业，拒绝屁民的请求。 呵呵，到此为止吧，当然还有很多其他的情况，但这些情况总是能在Paxos的算法中找到原型并加以解决。这也正是我们认为Paxos是Zookeeper的灵魂的原因。当然ZK Server还有很多属于自己特性的东西：Session, Watcher，Version等等等等，需要我们花更多的时间去研究和学习。 Zookeeper客户端1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#启动服务[root@node02 ~]# zkServer.sh start#进入客户端[root@node02 ~]# zkCli.sh#查看zookeeper有哪些命令[zk: localhost:2181(CONNECTED) 0] helpZooKeeper -server host:port cmd args connect host:port get path [watch] ls path [watch] set path data [version] rmr path delquota [-n|-b] path quit printwatches on|off create [-s] [-e] path data acl stat path [watch] close ls2 path [watch] history listquota path setAcl path acl getAcl path sync path redo cmdno addauth scheme auth delete path [version] setquota -n|-b val path[zk: localhost:2181(CONNECTED) 1] ls /[hbase, yarn-leader-election, hadoop-ha, zookeeper]#创建节点[zk: localhost:2181(CONNECTED) 2] create /sxt01 &quot;hello&quot;Created /sxt01[zk: localhost:2181(CONNECTED) 3] ls /[sxt01, hbase, yarn-leader-election, hadoop-ha, zookeeper][zk: localhost:2181(CONNECTED) 4] get /sxt01&quot;hello&quot;cZxid = 0x1800000003ctime = Wed Jul 26 05:15:08 CST 2017mZxid = 0x1800000003mtime = Wed Jul 26 05:15:08 CST 2017pZxid = 0x1800000003cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 0[zk: localhost:2181(CONNECTED) 5] set /sxt01 &quot;byebye&quot;cZxid = 0x1800000003ctime = Wed Jul 26 05:15:08 CST 2017mZxid = 0x1800000004mtime = Wed Jul 26 05:17:53 CST 2017pZxid = 0x1800000003cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 8numChildren = 0#监控节点[zk: localhost:2181(CONNECTED) 6] get /sxt01 true&quot;byebye&quot;……[zk: localhost:2181(CONNECTED) 7] set /sxt01 &quot;world&quot;WATCHER::#这里只是打印出监控的节点变化了，代码中可以自定义功能WatchedEvent state:SyncConnected type:NodeDataChanged path:/sxt01cZxid = 0x1800000003……#创建带序列号的节点（序列号单步递增）[zk: localhost:2181(CONNECTED) 8] create -s /sxt01 &quot;&quot;Created /sxt010000000007[zk: localhost:2181(CONNECTED) 9] create -s /sxt01 &quot;&quot;Created /sxt010000000008#创建临时节点[zk: localhost:2181(CONNECTED) 10] create -e /sxt02 &quot;sxt02&quot;Created /sxt02……#再开一个客户端查看这个节点，有临时节点标志[zk: localhost:2181(CONNECTED) 2] get /sxt02&quot;sxt02&quot;……ephemeralOwner = 0x15d7b9833480000 #写明了持有者dataLength = 7numChildren = 0#关闭创建临时节点的客户端后，别的客户端就不能访问这个临时节点了[zk: localhost:2181(CONNECTED) 2] get /sxt02Node does not exist: /sxt02 参考资料 Zookeeper可以干什么 ZooKeeper常见问题 Zookeeper编程]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Bigdata</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HADOOP--yarn详解及配置测试]]></title>
    <url>%2F2016%2F07%2F06%2FHadoop-yarnTest%2F</url>
    <content type="text"><![CDATA[YARN：Yet Another Resource Negotiator Client通过RS(ResourceManager)来提交作业，RS选择一个节点创建AppMaster进程，AppMaster启动后向RS申请资源（申请Container），然后RS去通知NodeManager来启动Container进程，然后AppMaster向Container分发任务（比如让它反射哪个类）。即任务调度（任务运行的先后顺序，重试机制）、任务监控和容错等由AppMaster来完成，资源（每个节点硬件资源，Container也是资源）管理全部交给RS管理。 图中的客户端提交了两个作业，每个作业都有自己的AppMaster，其中一个AppMaster死了不会影响另一个AppMaster，任务调度不是单点运行，一个任务故障不会影响另一个任务，其实，AppMaster也是一种Container，其容错机制为：如果AppMaster挂了，MR会另起一个AppMaster。解决了1.0里面的jobTracker负载过重，单点故障问题。下图是spark提交作业到yarn流程图，给以后的spark笔记做个铺垫吧。 搭建yarn1234567[root@node01 hadoop]# vi mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678910111213141516171819202122232425262728293031323334[root@node01 hadoop]# vi yarn-site.xml&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;cluster1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;node03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;node04&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node02:2181,node03:2181,node04:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 分发配置到集群。12root@node01 hadoop]# scp mapred-site.xml yarn-site.xml node02:`pwd`#node03和node04同理 node01上启动yarn。nodemanager会启动。1234567[root@node01 hadoop]# start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /opt/sxt/hadoop-2.6.5/logs/yarn-root-resourcemanager-node01.outnode04: starting nodemanager, logging to /opt/sxt/hadoop-2.6.5/logs/yarn-root-nodemanager-node04.outnode02: starting nodemanager, logging to /opt/sxt/hadoop-2.6.5/logs/yarn-root-nodemanager-node02.outnode03: starting nodemanager, logging to /opt/sxt/hadoop-2.6.5/logs/yarn-root-nodemanager-node03.out[root@node01 hadoop]# 但是yarnmanager不会，需要zai node03和node04上单独启动。记得jps验证一下。12345678910[root@node03 etc]# yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /opt/sxt/hadoop-2.6.5/logs/yarn-root-resourcemanager-node03.out[root@node03 etc]# jps1703 QuorumPeerMain3316 ResourceManager3540 Jps2748 JournalNode3178 NodeManager2668 DataNode# node04同理 访问http://node03:8088/cluster和http://node04:8088/cluster。讲道理，node04会发生跳转到node03。但是我的出错了。clusterID都不一致了。 12[root@node03 logs]# cd /opt/sxt/hadoop-2.6.5/logs[root@node03 logs]# vi yarn-root-resourcemanager-node03.log 发现是hosts文件配置错误 修改hosts文件后再启动resourcemanager，jps查看进程启动成功，访问node03:8088发生跳转。 WordCount测试/opt/sxt/hadoop-2.6.5/share/hadoop/mapreduce文件夹下执行example包中的wordcount程序，然后查看输出是否正确。在任务执行过程中，可以去其它节点jps查看下进程，node02会多了一个YarncChild，node03会多了个MRAppMaster，node04多了两个YarnChild。作业结束，这些进程也就消失了。1[root@node01 mapreduce]# hadoop jar hadoop-mapreduce-examples-2.6.5.jar wordcount /user/root /output ResourceManager和NodeManager是yarn框架的常服务，提交任务后，会在node03启动MRAppMaster并和ResourceManager去通信，然后申请Container, 也就是那三个YarncChild，这三个Container中运行map task 和reduce task。]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>yarn</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HADOOP--mac下eclipse连接hadoop集群开发API]]></title>
    <url>%2F2016%2F07%2F05%2FHadoop-eclipseConnectHadoop%2F</url>
    <content type="text"><![CDATA[mac下eclipse连接hadoop集群开发API 下载安装EclipseEclipse IDE for Java Developers 配置Eclipse下载Hadoop-Eclipse-Plugin可下载 Github 上的 hadoop2x-eclipse-plugin（选择下载hadoop-eclipse-plugin-2.6.0.jar） 安装Hadoop-Eclipse-Plugin在Applications中找个Eclise, 右键, Show Package Contents 将插件复制到plugins目录下然后重新打开Eclipse，添加插件。 连接Hadoop集群配置Hadoop安装目录1)mkdir /Users/Chant/opt将源码包和部署包都解压到这里。2)配置环境变量vim ~/.zshrc 12export HADOOP_PREFIX=/Users/Chant/opt/hadoop-2.6.5export PATH=&quot;$ANACONDA_HOME/bin:/usr/local/bin:$PATH:$HADOOP_HOME/bin&quot; 3)在Eclipse中指向该安装目录（Eclipse–&gt;偏好设置）需要改Location name, Location name 这里就是起个名字，随便起。Port填的是8020,这个由你的集群配置决定（去web页面可以查看端口）。 ( 勾选Use M/R的话，需要在hosts文件中添加192.168.14.11 Master # 添加Master的IP，Master会引用Mac中的hosts配置的IP ) Map/Reduce(V2) Master Host 这里就是虚拟机里hadoop master对应的IP地址，下面的端口对应 hdfs-site.xml里dfs.datanode.ipc.address属性所指定的端口，默认端口50020。 DFS Master Port： 这里的端口，对应core-site.xml里fs.defaultFS所指定的端口 最后的user name要跟虚拟机里运行hadoop的用户名一致，比如用zkpk身份安装运行hadoop 2.5.2的，所以这里填写zkpk，如果你是用root安装的，相应的改成root,但是改成root也没用,且看权限设置。 权限设置（Permisssion deny的三种解决办法）1.由于客户端与服务器的权限问题，对输入目录等需要赋予授权123456# 假设Mac的用户名为hadoopgroupadd supergroup # 添加supergroup组useradd -g supergroup hadoop # 添加hadoop用户到supergroup组# 修改hadoop集群中hdfs文件的组权限, 使属于supergroup组的所有用户都有读写权限hadoop fs -chmod 777 / 2.或者hdfs-site.xml里添加,并重启hdfs。1234&lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 3.或者在环境变量中设置HADOOP_USER_NAME值为root，就不用改系统用户名了（windows下是可以修改电脑用户名为root来解决）。ps:记得重启一下eclipse。 查看HDFS点击展开左上角的DFS Location, 查看是否可以直接访问HDFS。 添加jar包使用命令将部署包中（其实可以不要kms和httpfs下的jar包）的jar包都拷贝到同一目录下，方便添加。12[root@node01 share]# hadoop-2.6.5/share[root@node01 share]# \cp $(find hadoop -name *.jar) hadoop-2.6.5-lib.test 添加测试单元在项目下新建conf（HA,full分别用于不同的模式）文件夹，用于存储hdfs集群配置 配置Hadoop参数复制集群的配置文件。1$ scp root@node02:/opt/sxt/hadoop-2.6.5/etc/hadoop/core-site.xml root@node02:/opt/sxt/hadoop-2.6.5/etc/hadoop/hdfs-site.xml ./ 拷贝到HA文件夹下面，并将HA目录设置为源目录，这样就能通过相对路径引用它了。效果如下： 链接源码随便搜索一个类（command+shift+T，搜索NameNode），发现没有源码，点击Attach Source -&gt; External location -&gt; External Floder 测试创建测试类HDFS-API开发测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.chant.hadoop.hdfs.test;import java.io.BufferedInputStream;import java.io.BufferedOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.BlockLocation;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.junit.After;import org.junit.Before;import org.junit.Test;public class TestHDFS &#123; Configuration conf = null; FileSystem fs = null; @Before public void conn() throws IOException&#123; conf = new Configuration(false); //默认即为true，去bin目录下寻找配置文件并读取 conf.set("fs.defaultFS", "hdfs://node01:8020"); fs = FileSystem.get(conf); &#125; @After public void close() throws IOException&#123; fs.close(); &#125; @Test public void testConn()&#123; System.out.print(conf.get("fs.defaultFS")); &#125; @Test public void mkdir() throws IOException&#123; Path tmp = new Path("/temp"); if(!fs.exists(tmp))&#123; fs.mkdirs(tmp); &#125; &#125; //上传文件 @Test public void uploadFile() throws IOException&#123; Path file = new Path("/temp/sxt.txt"); FSDataOutputStream output = fs.create(file); InputStream input = new BufferedInputStream(new FileInputStream(new File("/Users/Chant/core-site.xml"))); IOUtils.copyBytes(input, output, conf, true); &#125; //下载文件 @Test public void download() throws IOException&#123; Path file = new Path("/temp/sxt.txt"); FSDataInputStream input = fs.open(file); OutputStream output = new BufferedOutputStream(new FileOutputStream(new File("/Users/Chant/sxtTest"))); IOUtils.copyBytes(input, output, conf, true); &#125;// 获取blockLocations，偏移量，计算向数据移动就靠这个来支撑 @Test public void bl() throws IOException&#123; Path f = new Path("/user/root/hello.txt"); FileStatus file = fs.getFileStatus(f); BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(file, 0, file.getLen()); for (BlockLocation blockLocation : fileBlockLocations) &#123; System.out.println(blockLocation); &#125; //读取指定block中的数据 FSDataInputStream input = fs.open(f); input.seek(1068577);//修改读取的偏移量，计算向数据移动 System.out.println((char)input.readByte()); &#125;&#125; 运行，取到了配置信息。 mkdir测试 获取blockLocations获取blockLocations（偏移量，位置信息），计算向数据移动就靠这个来支撑。访问http://node01:50070/找到文件的block信息，然后去对应节点查看文件，再测试代码，看读取是否正确。 1234567[root@node03 logs]# cd /var/sxt/hadoop/ha/dfs/data/current/BP-570581829-192.168.14.11-1499209010319/current/finalized/subdir0/subdir0/[root@node03 subdir0]# ls *1073741826*blk_1073741826 blk_1073741826_1002.meta[root@node03 subdir0]# head blk_1073741826hant hadoop 42388hello Chant hadoop 42389hello Chant hadoop 42390 参考资料 Mac下Eclipse提交任务到Hadoop集群]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HADOOP--mac下IDEA连接hadoop集群开发API]]></title>
    <url>%2F2016%2F07%2F05%2FHadoop-IDEAConnectHadoop%2F</url>
    <content type="text"><![CDATA[mac下IDEA连接hadoop集群开发API 创建项目 pom.xml文件如下 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.Chant.hadoop&lt;/groupId&gt; &lt;artifactId&gt;testHadoop&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.5&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 添加依赖的Libary引用项目和是哪个右击open module stting或者command+向下键。使用命令将部署包中share/hadoop下（其实可以不要kms和httpfs下的jar包）的jar包都拷贝到同一目录下，方便添加。12[root@node01 share]# hadoop-2.6.5/share[root@node01 share]# \cp $(find hadoop -name *.jar) hadoop-2.6.5-lib.test 将所有jar包添加进来，导入的libary可以起个名称，比如hadoop2.6.5。 设置运行参数hdfs://node01:9000/chant/words/input/test.txthdfs://node01:9000/chant/words/output大家参考这个改一下(主要是把IP换成自己虚拟机里的IP)，注意的是，如果input/test.txt文件没有，请先手动上传，然后/output/ 必须是不存在的，否则程序运行到最后，发现目标目录存在，也会报错， 这里IP和端口要看集群配置，比如我的core-site.xml里dfs.DefaultFS配的是hdfs://mycluster,而hdfs-site.xml里rpc配的node01:8020，所以写node01:8020。12345678910# core-site.xml&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt;&lt;/property&gt;# hdfs-site.xml&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;node01:8020&lt;/value&gt;&lt;/property&gt; working directory：指向你本地的hadoop安装目录。 WordCount测试代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133package com.chant.mr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import java.io.IOException;import java.io.InputStream;/** * Created by Chant on 2017/7/6. *//** * 这是统计单词个数的例子 * &lt;p&gt; * Created by zhangws on 16/7/31. */public class WordsCount &#123; public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] strings = value.toString().split(&quot; &quot;); for (String s : strings) &#123; //将文本行放入key context.write(new Text(s), new IntWritable(1)); &#125; &#125; &#125; public static class MyReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable v : values) &#123; count += v.get(); &#125; //输出key context.write(key, new IntWritable(count)); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length &lt; 2) &#123; System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;); System.exit(2); &#125; //先删除output目录 rmr(conf, otherArgs[otherArgs.length - 1]); Job job = Job.getInstance(conf, &quot;WordsCount&quot;); job.setJarByClass(WordsCount.class); job.setMapperClass(MyMapper.class); job.setCombinerClass(MyReducer.class); job.setReducerClass(MyReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); if (job.waitForCompletion(true)) &#123; cat(conf, otherArgs[1] + &quot;/part-r-00000&quot;); System.out.println(&quot;success&quot;); &#125; else &#123; System.out.println(&quot;fail&quot;); &#125; &#125; /** * 删除指定目录 * * @param conf * @param dirPath * @throws IOException */ private static void rmr(Configuration conf, String dirPath) throws IOException &#123; boolean delResult = false;// FileSystem fs = FileSystem.get(conf); Path targetPath = new Path(dirPath); FileSystem fs = targetPath.getFileSystem(conf); if (fs.exists(targetPath)) &#123; delResult = fs.delete(targetPath, true); if (delResult) &#123; System.out.println(targetPath + &quot; has been deleted sucessfullly.&quot;); &#125; else &#123; System.out.println(targetPath + &quot; deletion failed.&quot;); &#125; &#125;// return delResult; &#125; /** * 输出指定文件内容 * * @param conf HDFS配置 * @param filePath 文件路径 * @return 文件内容 * @throws IOException */ public static void cat(Configuration conf, String filePath) throws IOException &#123;// FileSystem fileSystem = FileSystem.get(conf); InputStream in = null; Path file = new Path(filePath); FileSystem fileSystem = file.getFileSystem(conf); try &#123; in = fileSystem.open(file); IOUtils.copyBytes(in, System.out, 4096, true); &#125; finally &#123; if (in != null) &#123; IOUtils.closeStream(in); &#125; &#125; &#125;&#125; 参考文章 Macbook Intellij idea与Eclipse远程调试Hadoop应用程序 eclipse/intellij idea 远程调试hadoop 2.6.0]]></content>
      <categories>
        <category>Bigdata</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS+Keepalived+Nginx实战]]></title>
    <url>%2F2016%2F06%2F16%2FLVS%2BKeepalived%2BNginx%20%2F</url>
    <content type="text"><![CDATA[LVS+Keepalived+Nginx实战,从背景介绍到搭建测试。 大型网站架构演进历程初始阶段应用程序、数据库、文件等所有资源在一台服务器上。典型架构Linux+Apache+Mysql+PHP，简称LAMP。 应用服务和数据服务分离随着网站业务的发展，一台服务器逐渐不能满足需求：越来越多的用户访问导致性能越来越差，越来越多的数据导致存储空间不足。这时就需要将应用和数据分离。这三台服务器对硬件资源的要求各不相同，应用服务器要处理大量业务逻辑，因此需要更快更强大的CPU；数据库服务器需要磁盘检索和数据缓存，因此需要更大的硬盘和内存，文件服务器需要存储大量用户上传的文件，因此需要更大的硬盘。 使用缓存改善网站性能网站访问特点和现实世界的财富分配一样遵循二八定律：80%的业务访问集中在20%的数据上。例如淘宝买家浏览的商品集中在少部分成交数多、评价良好的商品上；百度搜索关键词集中在少部分热门词汇上。网站使用的缓存分为两种：缓存在应用服务器上的本地缓存和缓存在专门的分布式缓存服务器的远程缓存。本地缓存的速度更快一些，但是受应用服务器内存限制，其缓存数量有限，而且会出现与应用程序争用内存的情况。远程分布式缓存可以使用集群的方式，部署大内存的服务器作为专门的缓存服务器，可以在理论上做到不受内存容器限制的缓存服务。 使用应用服务器集群改善网站并发处理处理能力使用集群是网站解决高并发、海量数据问题的常用手段。通过使用应用服务器集群，改善负载压力，实现系统的可伸缩性。通过负载均衡调度服务器，可将来自用户浏览器的访问请求分发到应用服务器集群中的任何一台服务器。当有更多的用户时，在集群中加入更多的应用服务器即可。当一台服务器的处理能力、存储空间不足时，不要企图去更换更强大的服务器，对答应网站而言，不管多么强大的服务器，都满足不了网站持续增长的业务需求，这种情况下，跟恰当的做法是增加一台服务器分担缘由服务器的访问及存储压力。 数据库读写分离网站在使用缓存后，是绝大部分数据读操作访问都可以不通过数据库就能完成，但是仍有一部分读操作（缓存访问不命中、缓存过期）和全部的写操作需要访问数据库，在网站的规模达到一定规模后，数据库因为负载压力过高而成为网站的瓶颈。目前大部分的主流数据库都提供主从热备的功能，通过配置两台数据库主从关系，可以将一台数据库服务器的数据更新同步到另一台服务器上。网站利用数据库的这一功能，实现数据库读写分离，从而改善数据库负载压力。 使用反向代理和CDN加速网站响应使用网站业务不断发展，用户规模越来越大，由于中国复杂的网络环境，不同地区的用户访问网站时，速度差别也极大。使用CDN和反向代理的目的都是早返回数据给用户。一方面加快用户访问速度，另一方面也减轻后端服务器的负载压力。CDN和反向代理的基本原理都是缓存，区别在于CDN部署在网络提供商的机房，使用户在请求网站服务时，可以从距离自己最近的网络提供商机房提取数据；而反向代理则部署在网站的中心机房，当用户请求到达中心机房后，首先访问的服务器是反向代理服务器，如果反向代理服务器缓存着用户请求的资源，就将其直接返回给用户。 使用分布式文件系统和分布式数据库系统分布式数据库是网站数据库拆分的最后手段，只有在单表数据规模非常庞大时才使用。网站最长使用的数据库拆分手段是业务分库，将不同业务的数据库部署在不同的物理服务器上。 业务拆分大型网站为了应付日益复杂的业务场景，通过使用分而治之的手段将整个网站业务分为不同的产品线，如大型电商网站会将首页、商铺、订单、买家、卖家等拆分成不同的产品线，分归不同的业务团队负责。将一个网站拆分成不同的应用，每个应用独立部署维护。应用之间可以通过超链接建立联系，也可以通过消息队列进行数据转发，也可通过同一个数据库系统构建一个关联的完整系统。 使用NoSQL和搜索引擎随着网站业务越来越复杂，对数据存储和检索的需求也越来越复杂，网站需要采用一些非数据库技术如NoSQL和非数据库查询技术如搜索引擎。源自互联网的技术手段，对可伸缩的分布式特性具有更好的支持。应用服务器则通过一个统一的数据模块访问各种数据，减轻应用程序管理诸多数据的麻烦。 分布式服务随着业务拆分越来越小，存储系统越来越庞大，应用系统的整体复杂度呈指数级增加，部署维护越来越难。由于所有应用要和所有数据库系统连接，在数万台服务器规模的网站中，这些连接的数目是服务器规模的平方，导致数据库连接资源不足，拒绝服务。各个应用系统需要执行很多相同的业务操作，比如用户管理、商品管理等。可以将这些共用业务提取出来，独立部署。应用系统只需要通过调用共用业务服务完成具体业务操作。 大型网站架构核心技术分布式缓存系统分布式缓存概述分布式缓存系统是进化的产物。本地缓存 -&gt; 集群缓存 -&gt; 分布式缓存（数据网格）。性能：访问缓存中的一个对象比直接访问远端数据存储引擎(例如数据库)要快很多。直接访问一个已经存在的对象比从数据创建一个对象要快。数据网格支持一些性能调优特性可能不被集群缓存所支持，例如，应用程序可以根据数据之间关联关系的紧密程度来确保相互关联的对象被保存在相同的缓存节点。一致性：本地缓存只有在应用程序被部署到单一的应用服务器上的时候才有意义，如果它被部署到了多台应用服务器上的话, 那么本地缓存一点意义都没有，问题出在过期数据。集群缓存是通过复制和让缓存数据失效来解决这个问题的。除了支持JTA事物之外，分布式缓存还支持XA（分布式）和两阶段提交事物。可伸缩性：集群缓存和数据网格的区别就在于可伸缩性。数据网格是可伸缩的。缓存数据是通过动态的分区被分发的。结果就是，增加一个缓存节点既提高了吞吐量也提高了容量。独立性：允许分布式缓存能够独立于应用服务器而被独立的扩展. 也让其服务器能够被指派与应用服务器不同的资源。这样的架构也让数据网格的基础架构能够独立于应用服务器的惯例和调整。能够独立于应用而被升级，应用的重新部署也不会对数据网格本身产生任何影响。基础架构：使用在基础架构中作为顶级系统的独立数据网格服务。 使用分布式缓存的好处是它的可扩展性和独立性，并且，作为顶级基础设施组件，它能够同时提供本地缓存和集群缓存所能够提供的性能和一致性。 典型应用场景页面缓存： 用来缓存Web页面的内容片段，包括HTML、CSS和图片等，多应用于社交网站等。应用对象缓存： 缓存系统作为ORM框架的二级缓存对外提供服务，目的是减轻数据库的负载压力，加速应用访问。状态缓存： 缓存包括Session会话状态及应用横向扩展时的状态数据等，这类数据一般是难以恢复的，对可用性要求较高，多应用于高可用集群。并行处理： 通常涉及大量中间计算结果需要共享。事件处理： 分布式缓存提供了针对事件流的连续查询（continuous query）处理技术，满足实时性需求。 Memcached分布式缓存系统Memcached 是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提高动态、数据库驱动网站的速度。Memcached是基于一个存储键/值对的HashMap。其守护进程是用C写的，但是客户端可以用任何语言来编写，并通过memcached协议与守护进程通信。 Memcached的存储方式： 为了提高性能，Memcached中保存的数据都存储在Memcached内置的内存存储空间中。由于数据仅存在于内存中，因此重启Memcached、重启操作系统会导致全部数据消失。另外，内容容量达到指定值之后，就基于LRU（Least Recently Used，近期最少使用）算法自动删除不使用的缓存。Memcached本身是为缓存而设计的服务器，因此并没有过多考虑数据的永久性问题。 Redis分布式缓存系统Redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string（字符串）、list（链表）、set（集合）、zset（sorted set –有序集合）和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，Redis支持各种不同方式的排序。与Memcached一样，为了保证效率，数据都是缓存在内存中。区别的是Redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。Redis支持语言：Redis的存储方式：Redis的存储分为：内存存储、磁盘存储和log文件三部分，配置文件中有三个参数对其进行配置。save seconds updates，save配置，指出在多长时间内，有多少次更新操作，就将数据同步到数据文件。这个可以多个条件配合，比如默认配置文件中的设置，就设置了三个条件。appendonly yes/no ，appendonly配置，指出是否在每次更新操作后进行日志记录，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为redis本身同步数据文件是按上面的save条件来同步的，所以有的数据会在一段时间内只存在于内存中。appendfsync no/always/everysec ，appendfsync配置，no表示等操作系统进行数据缓存同步到磁盘，always表示每次更新操作后手动调用fsync()将数据写到磁盘，everysec表示每秒同步一次。 两种分布式缓存系统比较存储数据位置：Redis中，并不是所有的数据都一直存储在内存中的，这是和Memcached相比一个最大的区别。数据类型支持：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储。对数据备份的支持： Redis支持数据的备份，即master-slave模式的数据备份。数据持久化：Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 负载均衡负载均衡概述负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。负载均衡，英文名称为Load Balance，其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。服务器负载均衡从字面意义来讲可以理解为，单台服务器不能称之为均衡，只有多个服务器才能称之为均衡，也就是说：多个服务器组成的这样一个系统，我们称之为服务器均衡系统。负载均衡组成的方式：负载均衡的服务器（管理器）被均衡的服务器集群（客户机）负载均衡管理器，是整个负载均衡的控制服务器（DR），所有用户的请求都要先经过这台服务器，然后由此服务器根据各个实际处理服务器状态将请求具体分配到某个实际处理服务器中，用户是感觉不到后端的服务器的，他们只看到当前这台DR服务器。DR服务器只负责转发和安装相应的管理软件，所以一般企业负载均衡服务器非常重要，但是资源使用非常少，所以不必用非常高的配置来担当负载均衡管理器。 负载均衡分类：软/硬件软件负载均衡解决方案是指在一台或多台服务器相应的操作系统上安装一个或多个附加软件来实现负载均衡，它的优点是基于特定环境，配置简单，使用灵活，成本低廉，可以满足一般的负载均衡需求。软件解决方案缺点也较多，因为每台服务器上安装额外的软件运行会消耗系统不定量的资源，越是功能强大的模块，消耗得越多，所以当连接请求特别大的时候，软件本身会成为服务器工作成败的一个关键；软件可扩展性并不是很好，受到操作系统的限制；由于操作系统本身的Bug，往往会引起安全问题。 硬件负载均衡解决方案是直接在服务器和外部网络间安装负载均衡设备，这种设备通常称之为负载均衡器，由于专门的设备完成专门的任务，独立于操作系统，整体性能得到大量提高，加上多样化的负载均衡策略，智能化的流量管理，可达到最佳的负载均衡需求。负载均衡器有多种多样的形式，除了作为独立意义上的负载均衡器外，有些负载均衡器集成在交换设备中，置于服务器与Internet链接之间，有些则以两块网络适配器将这一功能集成到PC中，一块连接到Internet上，一块连接到后端服务器群的内部网络上。一般而言，硬件负载均衡在功能、性能上优于软件方式，不过成本昂贵。 部署方式：路由模式负载均衡有三种部署方式：路由模式、桥接模式、服务直接返回模式。路由模式部署灵活，约60%的用户采用这种方式部署；桥接模式不改变现有的网络架构；服务直接返回（DSR）比较适合吞吐量大特别是内容分发的网络应用。约30%的用户采用这种模式。 服务器的网关必须设置成负载均衡机的LAN口地址，且与WAN口分属不同的逻辑网络。因此所有返回的流量也都经过负载均衡。这种方式对网络的改动小，能均衡任何下行流量。 应用场景在当业务系统服务器无法支撑当前的业务量时，用户可以选择更高性能的服务器。但更为合理的做法是通过在既有业务服务器基础上，增量的方式增加相同功能的服务器，将计算任务分摊到后台多台较低配置的服务器处理，每台服务器都可以响应服务请求。实现合理安排客户请求并加快了请求相应速度，来提高用户体验。而用户仅感受到是一台高性能服务器在提供服务。 负载均衡硬件 负载均衡软件 消息队列消息队列概述介绍：消息队列中间件是分布式系统中重要的组件，主要解决应用耦合，异步消息，流量削锋等问题。实现高性能，高可用，可伸缩和最终一致性架构。是大型分布式系统不可缺少的中间件。常用消息队列系统：目前在生产环境，使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ等。 应用场景异步处理：流量削峰：应用解耦：日志处理：消息通讯—点到点方式：消息通讯—发布订阅方式： 开源消息队列软件RabbitMQRabbitMQ是流行的开源消息队列系统，用erlang语言开发。RabbitMQ是一个在AMQP基础上完整的，可复用的企业消息系统。他遵循Mozilla Public License开源协议。AMQP当中有四个概念非常重要: 虚拟主机（virtual host），交换机（exchange），队列（queue）和绑定（binding）。一个虚拟主机持有一组交换机、队列和绑定。 页面静态化技术页面静态化概述静态页面：最早的时候，网站内容是通过在主机空间中放置大量的静态网页实现的。为了方便对这些分散在不同目录的静态网页的管理，（一般是通过FTP)，象frontpage/dreamweaver这样软件甚至直接提供了向主页空间以FTP方式直接访问文件的功能。以静态网页为主的网站最大的困难在于对网页的管理，在这种框架里，网页框架和网页中的内容混杂在一起，很大程度地加大了内容管理的难度。为了减轻这种管理的成本，发展出了一系列的技术，甚至连css本身，原本也是针对这种乱七八糟的网页维护而设计的，目的就是把网页表达的框架和内容本身抽象分离出来。A．静态网页的内容稳定，页面加载速度快。B．静态网页的没有数据库支持，在网站制作和维护方面的工作量较大。C．静态网页的交互性差，有很大的局限性。 动态页面：通过执行asp、php、jsp和.net等程序生成客户端网页代码的网页。通常可以通过网站后台管理系统对网站的内容进行更新管理。发布新闻，发布公司产品，交流互动，博客，网上调查等，这都是动态网站的一些功能。也是我们常见的。 常见的扩展名有：.asp、php、jsp、cgi和aspx 等。 注意：动态页面的“动态”是网站与客户端用户互动的意思，而非网页上有动画的就是动态页面。A．交互性好。 B．动态网页的信息都需要从数据库中读取，每打开一个一面就需要去获取一次数据库，如果访问人数很多，也就会对服务器增加很大的荷载，从而影响这个网站的运行速度。其实大家都知道，效率最高、消耗最小的就是纯静态化的html页面，所以我们尽可能使我们的网站上的页面采用静态页面来实现，这个最简单的方法其实也是最有效的方法。为什么需要动态页面静态化：1) 搜索引擎的优化尽管搜索机器人有点讨厌，各个网站不但不会再象从前一样把它封起来，反而热情无比地搞SEO，所谓的面向搜索引擎的优化，其中就包括访问地址的改写，令动态网页看上去是静态网页，以便更多更大量地被搜索引擎收录，从而最大限度地提高自已的内容被目标接收的机会。但是，在完全以动态技术开发的网站，转眼中要求变换成静态网页提供，同时，无论如何，动态网页的内容管理功能也是必须保留的；就如同一辆飞驶的奔驰忽然要求180度转弯，要付出的成本代价是非常大的，是否真的值得，也确实让人怀疑。2) 提高程序性能很多大型网站，进去的时候看它很复杂的页面，但是加载也没有耗费多长时间，除了其它必要原因以外，静态化也是其中必需考虑的技术之一。先于用户获取资源或数据库数据进而通过静态化处理，生成静态页面，所有人都访问这一个静态页面，而静态化处理的页面本身的访问速度要较动态页面快很多倍，因此程序性能会有大大的提升。静态化在页面上的体现为：访问速度加快，用户体验性明显提升；在后台体现为：访问脱离数据库，减轻了数据库访问压力。 FreeMarker实现页面静态化FreeMarker是什么：FreeMarker是一个基于Java的开发包和类库的一种将模板和数据进行整合并输出文本的通用工具，FreeMarker实现页面静态化的原理是：将页面中所需要的样式写入到FreeMarker模板文件中，然后将页面所需要的数据进行动态绑定并放入到Map中，然后通过FreeMarker的模板解析类process()方法完成静态页面的生成。模板引擎：一种基于模板的、用来生成输出文本的通用工具；基于Java的开发包和类库。FreeMarker能做什么：MVC框架中的View层组件、html页面静态化、代码生成工具和CMS模板引擎。为什么要用FreeMarker：程序逻辑（Java 程序）和页面设计（FreeMarker模板）分离； 分层清晰，利于分工合作； 主流Web框架良好的集成（struts2，springmvc）； 简单易学、功能强大；免费开源。 Velocity实现页面静态化Velocity是什么： Velocity是一个基于Java的模板引擎（template engine）。它允许任何人仅仅使用简单的模板语言（template language）来引用由Java代码定义的对象。Velocity的应用： 当Velocity应用于Web开发时，界面设计人员可以和Java程序开发人员同步开发一个遵循MVC架构的web站点，也就是说，页面设计人员可以只关注页面的显示效果，而由Java程序开发人员关注业务逻辑编码。Velocity将Java代码从web页面中分离出来，这样为web站点的长期维护提供了便利，同时也为我们在JSP和PHP之外又提供了一种可选的方案。 分布式数据库中间件分布式数据库中间件概述虽然云计算时代，传统数据库存在着先天性的弊端，但是NoSQL数据库又无法将其替代。如果传统数据易于扩展，可切分，就可以避免单机（单库）的性能缺陷。传统数据库系统随着规模的增大，遇到了主要问题:单个表数据量太大；单个库数据量太大；单台数据量服务器压力很大；读写速度遇到瓶颈。当面临以上问题时，我们会想到的第一种解决方式就是 向上扩展(scale up) 简单来说就是不断增加硬件性能。此时我们不得不依赖于第二种方式： 水平扩展 。 直接增加机器，把数据库放到不同服务器上，在应用到数据库之间加一个proxy进行路由，这样就可以解决上面的问题了。 分布式数据库中间件比较 分布式数据库中间件MyCatMyCat是一个开源的分布式数据库系统，是一个实现了MySQL协议的服务器，前端用户可以把它看作是一个数据库代理，用MySQL客户端工具和命令行访问，而其后端可以用MySQL原生协议与多个MySQL服务器通信，也可以用JDBC协议与大多数主流数据库服务器通信。其核心功能是分表分库，即将一个大表水平切分为N个小表，存储在后端MySQL服务器里或者其他数据库里。 MyCat使用手册：http://mycat.org.cn/document/Mycat_V1.6.0.pdfMyCat的技术原理：MyCat技术原理中最重要的一个动词是“拦截”，它拦截了用户发送过来的SQL语句，首先对SQL语句做了一些特定的分析：如分片分析、路由分析、读写分离分析、缓存分析等，然后将此SQL发往后端的真实数据库，并将返回的结果做适当的处理，最终再返回给用户。MyCat的核心功能：1． 支持水平切分2． 支持垂直切分 3． 支持读写分离4． 支持全局表5． 支持独创的ER关系分片，解决E-R分片难处理问题存在关联关系的父子表在数据插入的过程中，子表会被MyCat路由到其相关父表记录的节点上，从而父子表的Join查询可以下推到各个数据库节点上完成，这是最高效的跨节点Join处理技术 LVSLVS使用的三种模式：NAT、TUN、DR NAT模式-网络地址转换Virtual Server via Network Address Translation（VS/NAT） 通过网路地址转换，调度器重写请求报文的目标地址，根据预设的调度算法，将请求分派给后端的真实服务器，真实服务器的响应报文通过调度器时，报文的源地址被重写，再返回给客户，完成整个负载调度过程。NAT模式特点：1． NAT技术请求和响应的报文都必须经过调度器地址重写然后再转发，返回时再改写2． 只需要在LB（调度器）上配置WAN IP即可，也要有LAN IP和内部通信，内部RS只需要配置LAN IP3． 每台内部节点RS的网关要配成LB的LAN物理网卡地址，这样才能确保数据返回时仍能经过LB4． 由于请求与回传数据都必须经过负载均衡器，因此访问量大时LB有瓶颈5． 支持对IP及端口进行转换 TUN模式-隧道模式Virtual Server via IP Tunneling（VS/TUN） 为了解决NAT模式的瓶颈问题，调度器把请求报文通过IP隧道转发至真实服务器，而真实服务器将响应直接返回给客户，这样调度器只处理请求报文，由于一般网络服务应答数据比请求数据大很多，采用TUN模式后，集群系统的吞吐量可以提高10倍。 TUN的连接调度和管理与NAT一样，只是转发报文的方法不同，调度器根据各个服务器的负载情况，动态地选择一台服务器，将请求报文封装在另一个IP报文中，再将封装后的报文转发给选出的服务器，服务器收到报文后，先将报文解封获得来自目标地址为VIP的报文，服务器发现VIP地址被配置在本地的IP隧道设备上，所以就处理这个请求，然后根据路由表将响应报文直接返回给客户。TUN模式的特点：1． 负载均衡器把请求的报文通过IP隧道的方式（不改写请求报文的地址，而是直接封装成另外的IP报文）转发给真实服务器，真实服务器处理请求后把响应直接返回给客户端。2． 由于真实服务器把响应后的报文直接返回给客户端，因此虽然理论上只要能出网，无需外网IP地址，但RS最好有一个外网IP地址，这样效率高。3． 由于调度器只处理入站报文，因此集群系统的效率会提高10倍以上，但是隧道模式会带来一定的开销，它适合LAN/WAN。4． LAN环境下不如DR模式效率高，有的系统还需要考虑IP隧道的支持问题，还需要绑定VIP，配置复杂。5． LAN下多采用DR，WAN环境下可以用TUN模式，但是在WAN下更多的被DNS+haproxy/nginx等代理取代，因此TUN模式在国内公司已经使用的很少。 DR模式-直接路由Virtual Server via Direct Routing（VS/DR）VS/DR模式是通过改写请求报文的目标MAC地址，将请求发给真实服务器的，而真实服务器将响应后的处理结果直接返回给客户端用户。同VS/TUN技术一样，VS/DR技术可极大地提高集群系统的伸缩性。而且，这种方法没有IP隧道的开销，对集群中的真实服务器也没有必须支持IP隧道协议的要求，但是要求调度器与真实服务器都有一块网卡连在同一物理网段上。在LVS-DR配置中，Dierctor将所有入站请求转发给集群内部节点，但集群内部节点直接将它们的回复发送给客户端计算机（没有通过Director回来）。如图所示：VS/DR模式是互联网使用的最多的一种模式。VS/DR模式的工作流程如下图7所示。它的连接调度和管理与VS/NAT和VS/TUN中的一样，它的报文转发方法和前两种又有不同，将报文直接路由给目标服务器。在VS/DR中，调度器根据各个服务器的负载情况、连接数等因素，动态的选择一台服务器，不修改目的IP地址和目的端口，也不封装IP报文，而是将请求的数据帧的MAC地址改为选出服务器的MAC地址，再将修改后的数据帧在服务器组的局域网上发送。因为请求数据帧的MAC地址是选出的真实服务器，所以服务器肯定可以收到这个数据帧，从中可以获得该IP报文。当真实服务器发现报文的目标地址VIP是在本地的网络设备上，服务器就会处理这个报文，然后根据路由表将响应报文直接返回给用户。DR模式的特点：1． 通过在调度器LB上修改数据包的目标MAC地址实现转发。注意，源地址仍然是CIP地址，目标地址仍然是VIP。2． 由于只有请求的报文经过负载均衡器，而回传的报文无需经过负载均衡器，因此，访问量大的时候效率很高（和NAT相比）。3． 因DR模式是通过MAC地址改写机制实现的转发，因此，所有RS节点和调度器LB只能在一个LAN中。（小缺点）4． 需要注意RS节点的VIP的绑定（lo:VIP）和ARP抑制问题。强调，RS节点的默认网关不需要是LB的IP，而是出口路由器的IP。由于仅进行了MAC地址的改写，因此，LB无法改变请求的目标端口（和NAT主要区别）。LB几乎支持所有的UNIX，Linux系统，目前无Windows版，但是RS节点可以是windows。5． 总体来说DR模式效率很高，但是配置也比较麻烦，访问量不是特别大的企业可以用haproxy/nginx取代它。直接对外的访问业务，例如：web服务做RS节点，最好用公网IP地址，不直接对外的业务，例如MySQL，存储系统RS节点，最好用内部IP地址。 3种模式的对比 在LINUX环境下搭建DR模式原理图 一些概念VIP: 虚拟服务器地址DIP: 转发的网络地址，有两个作用：和RIP通信：ARP协议，获取Real Server的RIP：MAC地址转发Client的数据包到RIP上（隐藏的VIP）RIP: 后端真实主机（后端服务器）CIP: 客户端IP地址VIP: 虚拟主机IP 步骤1.准备3台Linux机器并配置它们的eth0网卡的IP（在一个网段）node001：LVS，IP：192.168.9.101node002：Real Server，IP：192.168.9.102node003：Real Server，IP：192.168.9.103 2.在LVS的eth0:0接口上配置VIP12343. [root@node001 ~]# ifconfig eth0:0 192.168.9.100/24[root@node001 ~]# ifconfigeth0 inet addr:192.168.9.101 Bcast:192.168.9.255 Mask:255.255.255.0 eth0:0 inet addr:192.168.9.100 Bcast:192.168.9.255 Mask:255.255.255.0 24代表该IP的子网掩码为255.255.255.0注意，要想让配置永久生效，应修改配置文件vi /etc/sysconfig/network-scripts/ifcfg-eth0:0DEVICE=eth0:0IPADDR=192.168.9.100NETMASK=255.255.255.0 3. 修改node002和node003的内核ARP通告和响应级别（隐藏VIP）arp_ignore: 定义接收到ARP请求时的响应级别0：只要本地配置的有相应地址，就给予响应1：仅在请求的目标(MAC)地址配置请求到达的接口上的时候，才给予响应；arp_announce：定义将自己地址向外通告时的通告级别0：将本地任何接口上的任何地址向外通告1：试图仅向目标网络通告与其网络匹配的地址2：仅向与本地接口上地址匹配的网络进行通告 123456789[root@node002 ~]# echo 1 &gt; /proc/sys/net/ipv4/conf/eth0/arp_ignore [root@node002 ~]# echo 2 &gt; /proc/sys/net/ipv4/conf/eth0/arp_announce#后两步可选，为保险起见，防止再配置的网卡进行通告，可以这么设置[root@node002 ~]# echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore [root@node002 ~]# echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce[root@node003 ~]# echo 1 &gt; /proc/sys/net/ipv4/conf/eth0/arp_ignore [root@node003 ~]# echo 2 &gt; /proc/sys/net/ipv4/conf/eth0/arp_announce[root@node003 ~]# echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore [root@node003 ~]# echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce 4. 在node002和node003上配置VIP12[root@node002 ~]# ifconfig lo:0 192.168.9.100 netmask 255.255.255.255[root@node003 ~]# ifconfig lo:0 192.168.9.100 netmask 255.255.255.255 注意，要想让配置永久生效，应修改配置文件vi /etc/sysconfig/network-scripts/ifcfg-lo:0DEVICE=lo:0IPADDR=192.168.9.100NETMASK=255.255.255.255 5. 在node002和node003上安装httpd（apache静态web server，默认端口号80）服务并启动 12345678910111213[root@node002 ~]# yum install httpd –y[root@node002 ~]# cd /var/www/html[root@node002 html]# vi index.html&lt;h1&gt;From 192.168.9.102&lt;/h1&gt;[root@node002 html]# service httpd start[root@node002 html]# chkconfig httpd on 设置httpd服务开机启动[root@node003 ~]# yum install httpd –y[root@node003 ~]# cd /var/www/html[root@node003 html]# vi index.html&lt;h1&gt;From 192.168.9.103&lt;/h1&gt;[root@node002 html]# service httpd start[root@node002 html]# chkconfig httpd on 浏览器访问node002和node003，验证一下：但此时还不能通过LVS访问RS 6. 在LVS上安装ipvsadm客户端1234567891011121314151617181920[root@node001 network-scripts]# yum install ipvsadm –y添加监听的IP[root@node001 network-scripts]# ipvsadm -A -t 192.168.9.100:80 -s rr查看是否设置成功[root@node001 network-scripts]# ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.9.100:80 rr添加后台主机[root@node001 network-scripts]# ipvsadm -a -t 192.168.9.100:80 -r 192.168.9.102 -g[root@node001 network-scripts]# ipvsadm -a -t 192.168.9.100:80 -r 192.168.9.103 -g验证[root@node001 network-scripts]# ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.9.100:80 rr -&gt; 192.168.9.102:80 Route 1 0 0 -&gt; 192.168.9.103:80 Route 1 0 0 去页面验证：在浏览器输入192.168.9.100并反复刷新，页面交替显示node002和node003的主页，说明LVS集群配置成功。 也可以通过ipvsadm –lnc命令查看LVS调度状态。说明ipvsadm -A -t 192.168.9.100:80 -s rr命令中，rr指定了LVS调度RS的算法，rr代表轮询，即依次循环的调用所有的RS节点，其他算法还有wrr、dh、sh等。-t: TCP协议的集群-u: UDP协议的集群-f: FWM: 防火墙标记添加：-A修改：-E删除：-D -t|u|f service-address LVS+Keepalived搭建高可用集群Keepalived介绍LVS的弊端： 后端：需要镜像服务器 后端：没有健康检查机制 自身：单点故障（LVS出现故障后，整个集群都不能正常使用）keepalived是集群管理中保证集群高可用（High Available）的服务软件，对LVS进行了改进： 使用心跳机制探测后端RS是否提供服务。a) 探测网卡接口是否down，如果是，从LVS中删除该RSb) 检测到down的机器又up，则从LVS中再次添加该RS LVS DR，使用主从（HA）模式，即LVS有备用的机器Keepalived结构图： LINUX中搭建LINUX中搭建LVS+Keepalived集群准备工作需要修改集群中RS的内核ARP通告和响应级别，配置好VIP，并启动httpd服务，LVS不需要做任何配置 1. 在node001和node004上安装keepalived12[root@node001 ~]# yum install keepalived –y[root@node004 ~]# yum install keepalived –y 2. 修改keepalived的配置文件123[root@node001 ~]# cd /etc/keepalived/[root@node001 keepalived]# cp keepalived.conf keepalived.conf.bak[root@node001 keepalived]# vi keepalived.conf 对keepalived.conf配置文件做说明： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667! Configuration File for keepalived&lt;!-- global_defs start--&gt;global_defs &#123; notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL&#125;&lt;!-- global_defs end--&gt;&lt;!-- vrrp_instance VI_1 start--&gt;vrrp_instance VI_1 &#123; state MASTER //指定本机是主机还是从机，node004改为BACKUP interface eth0 virtual_router_id 51 priority 100 //从机的改为其一半 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.9.100/24 dev eth0 label eth0:0 //指定监听的接口 &#125;&#125;//可以添加多台LVSvirtual_server 192.168.9.100 80 &#123; delay_loop 6 lb_algo rr //LVS调度RS的算法 lb_kind DR //LVS的模式 nat_mask 255.255.255.0 //LVS的子网掩码 persistence_timeout 0 protocol TCP real_server 192.168.9.102 80 &#123; //指定RS的IP和端口 weight 1 HTTP_GET &#123; //监听的协议 url &#123; path / status_code 200 //根据状态码检测RS是否运行正常 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; //另一台RS，可以添加多台 real_server 192.168.9.103 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;&lt;!-- vrrp_instance VI_1 end-&gt; 修改配置文件用到一些VI命令：dG：从当前行到最后一行全部删除:.,$-1y：从当前行到倒数第二行全部复制 3. 修改node004的keepalived配置文件123456789101112[root@node004 ~]# cd /etc/keepalived/[root@node004 keepalived]# mv keepalived.conf keepalived.conf.bak[root@node001 keepalived]# scp keepalived.conf root@node004:`pwd`[root@node004 keepalived]# vi keepalived.confvrrp_instance VI_1 &#123;state BACKUPinterface eth0virtual_router_id 51priority 50 //是主机的一半……&#125; 4. 启动keepalived服务12[root@node001 keepalived]# service keepalived start[root@node004 keepalived]# service keepalived start 5. 配置完成，进行测试LVS功能正常破坏掉主机的eth0网卡[root@node001 keepalived]# ifconfig eth0 down发现VIP已经漂移到node00412345[root@node004 keepalived]# ifconfigeth0 inet addr:192.168.9.104 Bcast:192.168.9.255 Mask:255.255.255.0 eth0:0 Link encap:Ethernet HWaddr 00:0C:29:54:A1:77 inet addr:192.168.9.100 Bcast:0.0.0.0 Mask:255.255.255.0UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 浏览器也不受影响再恢复node001的eth0网卡，VIP再次漂移回node001123456789[root@node001 keepalived]# ifconfig eth0 down[root@node001 keepalived]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:67:1D:65 inet addr:192.168.9.101 Bcast:192.168.9.255 Mask:255.255.255.0 eth0:0 Link encap:Ethernet HWaddr 00:0C:29:67:1D:65 inet addr:192.168.9.100 Bcast:0.0.0.0 Mask:255.255.255.0[root@node004 keepalived]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:54:A1:77 inet addr:192.168.9.104 Bcast:192.168.9.255 Mask:255.255.255.0 杀掉keepalived进程1234567891011[root@node001 ~]# ps –efroot 1136 1 0 21:26 ? 00:00:00 /usr/sbin/keepalived -Droot 1138 1136 0 21:26 ? 00:00:00 /usr/sbin/keepalived -Droot 1139 1136 0 21:26 ? 00:00:00 /usr/sbin/keepalived –D[root@node001 ~]# kill -9 1136[root@node004 keepalived]# ps –efroot 1145 1 0 21:27 ? 00:00:00 /usr/sbin/keepalived -Droot 1146 1145 0 21:27 ? 00:00:00 /usr/sbin/keepalived -Droot 1147 1145 0 21:27 ? 00:00:00 /usr/sbin/keepalived –D[root@node004 keepalived]# kill -9 1145多次查看进程，确保杀死所有keepalived进程 此时主机和从机都拥有了VIP12345678910[root@node001 ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:67:1D:65 inet addr:192.168.9.101 Bcast:192.168.9.255 Mask:255.255.255.0eth0:0 Link encap:Ethernet HWaddr 00:0C:29:67:1D:65 inet addr:192.168.9.100 Bcast:0.0.0.0 Mask:255.255.255.0[root@node004 keepalived]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:54:A1:77 inet addr:192.168.9.104 Bcast:192.168.9.255 Mask:255.255.255.0eth0:0 Link encap:Ethernet HWaddr 00:0C:29:54:A1:77 inet addr:192.168.9.100 Bcast:0.0.0.0 Mask:255.255.255.0 这种情况说明keepalived会出现问题，在互联网中出现这种情况后，会导致发送的数据包同时发送到主机和从机，需要使用zookeeper来解决这个问题。 NginxNginx简介Nginx（”engine x”）是一个高性能的HTTP和反向代理服务器，也是一个 IMAP/POP3/SMTP 代理服务器。第一个公开版本0.1.0发布于2004年10月4日。其将源代码以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。官方测试nginx能够支撑5万并发链接，并且cpu、内存等资源消耗却非常低，运行非常稳定。2011年6月1日，nginx 1.0.4发布。Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，并在一个BSD-like 协议下发行。由俄罗斯的程序设计师Igor Sysoev所开发，其特点是占有内存少，并发能力强，事实上nginx的并发能力确实在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：新浪、网易、腾讯等。功能：web服务器、web reverse proxy、smtp reverse proxy Nginx和apache的比较 nginx相对于apache的优点：轻量级，同样起web服务，比apache占用更少的内存及资源 ；抗并发，nginx处理请求是异步非阻塞的，而apache则是阻塞型的，在高并发下nginx 能保持低资源低消耗高性能 ；高度模块化的设计，编写模块相对简单 ；社区活跃，各种高性能模块出品迅速。 apache 相对于nginx 的优点：rewrite ，比nginx的rewrite强大 ；模块超多，基本想到的都可以找到 ；少bug，nginx的bug相对较多。 Nginx配置简洁，Apache复杂 。 最核心的区别在于apache是同步多进程模型，一个连接对应一个进程；nginx是异步的，多个连接（万级别）可以对应一个进程。单个tomcat支持的最高并发解决高并发和单个服务器过载问题Tengine安装Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。 1. 解压安装包[root@node001 local]# tar xf tengine-2.1.0.tar.gz 2. 进入tengine目录[root@node001 local]# cd tengine-2.1.0/ 3. 查看README文件，找到安装方法To install Tengine, just follow these three steps: $ ./configure $ make # make install 4. 执行configure文件，指定安装目录12345678910[root@node001 tengine-2.1.0]# ./configure --prefix=/usr/local/nginx使用如下命令查看更多安装选项[root@node01 tengine-2.1.0]# ./configure --help --help print this message --prefix=PATH set installation prefix --sbin-path=PATH set nginx binary pathname --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname …… 5.报错：./configure: error: C compiler cc is not found 6.安装gcc，再执行configure文件 [root@node001 tengine-2.1.0]# yum install gcc 7. 报错：./configure: error: the HTTP rewrite module requires the PCRE library. 8. 查看PCRE有哪些版本123456[root@node001 tengine-2.1.0]# yum search pcrepcre-devel.i686 : Development files for pcrepcre-devel.x86_64 : Development files for pcrepcre-static.x86_64 : Static library for pcrepcre.x86_64 : Perl-compatible regular expression librarypcre.i686 : Perl-compatible regular expression library 9. 选择安装开发版，系统自动识别安装什么位数的软件[root@node001 tengine-2.1.0]# yum install pcre-devel 10. 再执行configure文件[root@node001 tengine-2.1.0]# ./configure --prefix=/usr/local/nginx 11. 报错：./configure: error: SSL modules require the OpenSSL library. 12. 根据pcre的经验，安装OpenSSL开发版[root@node001 tengine-2.1.0]# yum install openssl-devel 13. 再执行configure文件[root@node001 tengine-2.1.0]# ./configure --prefix=/usr/local/nginx看到如下信息说明configure文件执行成功： 14. 执行make命令[root@node001 tengine-2.1.0]# make 15. 执行make install命令[root@node001 tengine-2.1.0]# make install 16. 将nginx文件放到/etc/init.d目录下，并做修改123[root@node001 tengine-2.1.0]# vi /etc/init.d/nginxnginx=&quot;/usr/local/nginx/sbin/nginx&quot;NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot; 17. 给nginx文件赋予执行权限[root@node001 tengine-2.1.0]# chmod +x nginx 18. 启动服务[root@node001 sbin]# service nginx start 19. 验证是否启动12[root@node001 sbin]# service nginx statusnginx (pid 6510 6508) is running... 20.去网页验证，看到如下页面说明nginx安装成功！ Nginx配置解析nginx.conf配置文件的结构……events{……}http {…… server{……}server{……}} 全局的配置user nobody; #定义Nginx运行的用户和用户组说明：1234567[root@node01 tengine-2.1.0]# ps -fe | grep nginxroot 1367 1335 0 13:18 pts/1 00:00:00 vi nginx.confroot 1608 1 0 14:32 ? 00:00:00 nginx: master process /opt/sxt/nginx/sbin/nginx –c /opt/sxt/nginx/conf/nginx.confnobody 1610 1608 0 14:32 ? 00:00:00 nginx: worker process root 1626 1097 0 14:45 pts/0 00:00:00 grep nginx[root@node01 tengine-2.1.0]# service nginx statusnginx (pid 1610 1608) 正在运行... master process不负责处理客户端连接请求，负责对worker process的监管，而worker process负责处理客户端请求。Nginx支持热加载和热升级，比如更新了配置文件后执行reload命令，master会开出一个新进程去读取更新过的配置文件，而worker进程继续保持从旧请求的连接，直到旧进程死亡，新进程会与新请求连接。master process由root启动，worker process由nobody启动，权限较小。123456worker_processes 1; #nginx进程数，建议设置为等于虚拟机CPU总核心数error_log logs/error.log;error_log logs/error.log notice;error_log logs/error.log info;#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]pid logs/nginx.pid; #进程文件 event下的一些配置及其意义12345678910111213141516use epoll;#参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]；#epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型#如果跑在FreeBSD上面，就用kqueue模型。worker_connections 1024; #单个进程最大连接数（最大连接数=连接数*进程数） #假设worker_processes为8#系统可以打开的最大文件数和内存大小成正比#查看自己的系统可以打开的最大文件数 cat /proc/sys/fs/file-max ：97318#并发连接总数小于系统可以打开的文件总数，这样就在操作系统可以承受的范围之内#选择最大连接数为80000#在设置了反向代理的情况下，根据经验，最大连接数应该再除以4，就是20000#所以单个进程最大连接数为20000/8 = 2500#同时应该调整系统的单个进程可以打开的文件数#使用ulimit –a查看到open file =1024#应该调整系统的单个进程最大打开文件数（该数值x进程数&lt;=97318）#ulimit -SHn 10000 http下的一些配置及其意义12345678910111213include mime.types; #文件扩展名与文件类型映射表default_type application/octet-stream; #默认文件类型log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #日志格式access_log logs/access.log main; #日志文件位置sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off.原理，比如Nginx接受用户对某文件的请求，nginx不能直接读取磁盘的内容，需要经过内核的调用，nginx告诉内核需要读取x文件，内核会读取x文件到内核的内存中，在把这个文件copy到nginx的内存堆中，nginx得知数据已经准备好，会再把这个文件发给内核，内核切片之后发送给用户。当并发数过大时太耗费资源，所以这个选项的存在是为了减少文件在两个内存之间的copy，提高效率。keepalive_timeout 0; #长连接超时时间，单位是秒（与keeplived软件无关）#gzip on; #开启gzip压缩输出 server下的一些配置及其意义12345678listen 80; #监听的IP和地址server_name www.bbb.com; #主机名 location / &#123; root /opt; #网页文件存放的目录index index.html index.htm;#默认首页文件，顺序从小到右，如果找不到index.html，则index.htm为首页 autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。 &#125; 虚拟主机虚拟主机是一种特殊的软硬件技术，它可以将网络上的每一台计算机分成多个虚拟主机，每个虚拟主机可以独立对外提供www服务，这样就可以实现一台主机对外提供多个web服务，每个虚拟主机之间是独立的，互不影响的。 基于IP的虚拟主机1. 查看服务器的IP地址12345[root@node01 ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:A6:C6:18 inet addr:192.168.9.11 Bcast:192.168.9.255 Mask:255.255.255.0lo Link encap:Local Loopback ine###### inet addr:127.0.0.1 Mask:255.0.0.0 在eth0网卡设备上添加两个IP别名192.168.9.98和192.168.9.991234[root@node01 ~]# ifconfig eth0:1 192.168.9.98 broadcast 192.168.9.255 netmask 255.255.255.0 up[root@node01 ~]# route add -host 192.168.9.98 dev eth0:1[root@node01 ~]# ifconfig eth0:2 192.168.9.99 broadcast 192.168.9.255 netmask 255.255.255.0 up[root@node01 ~]# route add -host 192.168.9.99 dev eth0:2 注意：以上配置只是临时生效，想让它们永久生效，可以将这两条ifconfig和route命令添加到/etc/rc.local文件中，让系统开机时自动运行。在文件末尾增加以下内容，保存退出即可。123456789101112#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff.touch /var/lock/subsys/localifconfig eth0:1 192.168.9.98 broadcast 192.168.9.255 netmask 255.255.255.0 uproute add -host 192.168.9.98 dev eth0:1ifconfig eth0:2 192.168.9.99 broadcast 192.168.9.255 netmask 255.255.255.0 uproute add -host 192.168.9.99 dev eth0:2 3. 验证是否配置成功123456789[root@node01 ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:0C:29:A6:C6:18 inet addr:192.168.9.11 Bcast:192.168.9.255 Mask:255.255.255.0eth0:1 Link encap:Ethernet HWaddr 00:0C:29:A6:C6:18 inet addr:192.168.9.98 Bcast:192.168.9.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1eth0:2 Link encap:Ethernet HWaddr 00:0C:29:A6:C6:18 inet addr:192.168.9.99 Bcast:192.168.9.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 4. 配置基于IP的虚拟主机1234567891011121314151617181920212223242526272829303132[root@node01 ~]# vi /opt/sxt/nginx/conf/nginx.confhttp &#123;server &#123; listen 192.168.9.11:80; server_name 192.168.9.11; access_log logs/host.access.log main; location / &#123; root /var/www/server1; index index.html index.htm; &#125;&#125;server &#123; listen 192.168.9.98:80; listen 192.168.9.98; access_log logs/host.access.log main; location / &#123; root /var/www/server2; index index.html index.htm; &#125;&#125;server &#123; listen 192.168.9.99:80; server_name 192.168.9.99; access_log logs/host.access.log main; location / &#123; root /var/www/server3; index index.html index.htm; &#125;&#125;&#125; 5.在/var/www/下创建3个文件夹server1、server2、server3，在其中分别定义三个虚拟主机的首页index.html1234567[root@node01 ~]# mkdir -p /var/www/server&#123;1,2,3&#125;[root@node01 ~]# vi /var/www/server1/index.html&lt;h1&gt;From Virtual Host 1:192.168.9.11&lt;/h1&gt;[root@node01 ~]# vi /var/www/server2/index.html&lt;h1&gt;From Virtual Host 2:192.168.9.98&lt;/h1&gt;[root@node01 ~]# vi /var/www/server3/index.html&lt;h1&gt;From Virtual Host 3:192.168.9.99&lt;/h1&gt; 6. 开启nginx服务（如果已经启动则重启nginx服务）12[root@node01 ~]# service nginx start[root@node01 ~]# service nginx reload 7. 在网页查看效果 基于域名的虚拟主机1.在nginx.conf文件中配置123456789101112131415161718http &#123;server &#123; listen 80; server_name www.aaa.com; location / &#123; root /var; autoindex on; &#125; &#125; server &#123; listen 80; server_name www.bbb.com; location / &#123; root /opt; autoindex on; &#125;&#125;&#125; 2. 修改Windows的hosts文件192.168.9.11 node01 www.aaa.com www.bbb.com 3.重启服务[root@node01 ###### [root@node01 ~]# service nginx reload 4.去网页验证123456789101112131415161718[root@node01 ~]# ll /var总用量 60drwxr-xr-x. 6 root root 4096 6月 29 11:50 cachedrwxr-xr-x. 3 root root 4096 5月 22 12:05 dbdrwxr-xr-x. 3 root root 4096 5月 22 12:05 emptydrwxr-xr-x. 2 root root 4096 9月 23 2011 gamesdrwxr-xr-x. 17 root root 4096 6月 29 11:50 libdrwxr-xr-x. 2 root root 4096 9月 23 2011 localdrwxrwxr-x. 5 root lock 4096 7月 3 07:50 lockdrwxr-xr-x. 4 root root 4096 7月 3 12:39 loglrwxrwxrwx. 1 root root 10 5月 22 12:04 mail -&gt; spool/maildrwxr-xr-x. 2 root root 4096 9月 23 2011 nisdrwxr-xr-x. 2 root root 4096 9月 23 2011 optdrwxr-xr-x. 2 root root 4096 9月 23 2011 preservedrwxr-xr-x. 13 root root 4096 7月 3 12:39 rundrwxr-xr-x. 8 root root 4096 5月 22 12:05 spooldrwxrwxrwt. 2 root root 4096 6月 30 09:41 tmpdrwxr-xr-x. 2 root root 4096 9月 23 2011 yp 123[root@node01 ~]# ll /opt总用量 4drwxr-xr-x 3 root root 4096 6月 29 09:46 sxt 注意：确认配置没有问题的情况下如果报错，可尝试service nginx restart（当reload不能解决问题时） location映射解析location [ = | ~ | ~ | ^~ ] uri { … } location URI {}： 对当前路径及子路径下的所有对象都生效； location = URI {}：注意URL最好为具体路径。精确匹配指定的路径，不包括子路径，因此，只对当前资源生效； location ~ URI {}与location ~ URI {}： 模式匹配URI，此处的URI可使用正则表达式，~区分字符大小写，~不区分字符大小写； location ^~ URI {}: 不使用正则表达式 优先级：= &gt; ^~ &gt; ~|~ &gt; /|/dir/ 精确匹配优先级最高，如果找到，停止搜索12345678910111213server &#123; listen 192.168.9.11:80; server_name 192.168.9.11; location = /images/test.png &#123; return 601; &#125; location /images/test &#123; return 602; &#125; location /images &#123; return 603; &#125;&#125; 所有剩下的常规字符串，最长的匹配 如果这个匹配使用^〜前缀，搜索停止123456location /images/test &#123; return 602;&#125;location ^~ /images &#123; return 604;&#125; 正则表达式按照顺序匹配123456location ~ login &#123; return 605;&#125;location ~ (.*)\.html$ &#123; return 606;&#125; 123456location ~ (.*)\.html$ &#123; return 606;&#125;location ~ login &#123; return 605;&#125; 说明 “普通 location” 的匹配规则是“最大前缀”，因此“普通 location ”的确与 location 编辑顺序无关； “正则 location” 的匹配规则是“顺序匹配，且只要匹配到第一个就停止后面的匹配”； “普通location ”与“正则 location ”之间的匹配顺序是先匹配普通 location ，再“考虑”匹配正则 location ；也就是说匹配完“普通 location ”后，有的时候需要继续匹配“正则 location ”，有的时候则不需要继续匹配“正则 location ”。两种情况下，不需要继续匹配正则 location ：1) 当普通 location 前面指定了“ ^~ ”，特别告诉 Nginx 本条普通 location 一旦匹配上，则不需要继续正则匹配；2) 当普通location 恰好严格匹配上，不是最大前缀匹配，则不再继续匹配正则总结location匹配顺序：先普通顺序无关最大前缀被打断^~完全匹配再正则不完全匹配有顺序先匹配，先应用，即时退出匹配反向代理12345678server &#123; listen 192.168.9.11:80; server_name 192.168.9.11; location / &#123; root html; index index.html index.htm; &#125;&#125; 12location /mp3 &#123; proxy_pass http://192 前提：node03安装了httpd123location /baidu &#123; proxy_pass http://www.baidu.com/;&#125; 客户端发生了跳转！http://www.baidu.com会被百度转换为https://www.baidu.com再返回123location /baidu &#123; proxy_pass https://www.baidu.com/;&#125; Nginx负载均衡1234567891011upstream httpd &#123; server 192.168.9.12:80; server 192.168.9.13:80;&#125; server &#123; listen 192.168.9.11:80; server_name 192.168.9.11; location /mp3 &#123; proxy_pass http://httpd/; &#125;&#125; Nginx的session一致性问题http协议是无状态的，即你连续访问某个网页100次和访问1次对服务器来说是没有区别对待的，因为它记不住你。那么，在一些场合，确实需要服务器记住当前用户怎么办？比如用户登录邮箱后，接下来要收邮件、写邮件，总不能每次操作都让用户输入用户名和密码吧，为了解决这个问题，session的方案就被提了出来，事实上它并不是什么新技术，而且也不能脱离http协议以及任何现有的web技术。session的常见实现形式是会话cookie（session cookie），即未设置过期时间的cookie，这个cookie的默认生命周期为浏览器会话期间，只要关闭浏览器窗口，cookie就消失了。实现机制是当用户发起一个请求的时候，服务器会检查该请求中是否包含sessionid，如果未包含，则系统会创造一个名为JSESSIONID的输出 cookie返回给浏览器(只放入内存，并不存在硬盘中)，并将其以HashTable的形式写到服务器的内存里面；当已经包含sessionid是，服务端会检查找到与该session相匹配的信息，如果存在则直接使用该sessionid，若不存在则重新生成新的 session。这里需要注意的是session始终是有服务端创建的，并非浏览器自己生成的。但是浏览器的cookie被禁止后session就需要用get方法的URL重写的机制或使用POST方法提交隐藏表单的形式来实现。session共享：首先我们应该明白，为什么要实现共享，如果你的网站是存放在一个机器上，那么是不存在这个问题的，因为会话数据就在这台机器，但是如果你使用了负载均衡把请求分发到不同的机器呢？这个时候会话id在客户端是没有问题的，但是如果用户的两次请求到了两台不同的机器，而它的session数据可能存在其中一台机器，这个时候就会出现取不到session数据的情况，于是session的共享就成了一个问题。1234567891011121314151617[root@node02 software]# rpm -i jdk-7u67-linux-x64.rpm[root@node02 software]# tar xf apache-tomcat-7.0.61.tar.gz[root@node02 software]# vi /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_67export PATH=$PATH:$JAVA_HOME/bin[root@node02 software]# . /etc/profile[root@node02 software]# java -versionjava version &quot;1.7.0_67&quot;Java(TM) SE Runtime Environment (build 1.7.0_67-b01)Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)[root@node02 software]# jps1309 Jps[root@node02 software]# cd apache-tomcat-7.0.61/webapps/ROOT/[root@node02 ROOT]# cp index.jsp index.jsp.bak[root@node02 ROOT]# vi index.jspFrom 192.168.9.12&lt;br&gt;session=&lt;%= session.getId() %&gt; 对node03做相同的配置，index.jsp改为From 192.168.9.13启动node02和node03的tomcat12345678[root@node02 ~]# cd /usr/local/software/apache-tomcat-7.0.61/bin/[root@node02 bin]# ./startup.sh Using CATALINA_BASE: /usr/local/software/apache-tomcat-7.0.61Using CATALINA_HOME: /usr/local/software/apache-tomcat-7.0.61Using CATALINA_TMPDIR: /usr/local/software/apache-tomcat-7.0.61/tempUsing JRE_HOME: /usr/java/jdk1.7.0_67Using CLASSPATH: /usr/local/software/apache-tomcat-7.0.61/bin/bootstrap.jar:/usr/local/software/apache-tomcat-7.0.61/bin/tomcat-juli.jarTomcat started. 在两个页面交替访问192.168.9.12:8080和192.168.9.13:8080，sessionID不变 在nginx上配置，让请求转到node02和node03123456789101112upstream tom &#123; server 192.168.9.12:8080; server 192.168.9.13:8080;&#125;server &#123; listen 192.168.9.11:80; server_name 192.168.9.11; location /cat &#123; proxy_pass http://tom/; &#125;&#125;[root@node01 software]# service nginx reload 随着页面刷新，sessionId会一直改变！解决这个问题：memcached（一个内存数据库） 1.安装memcached[root@node01 software]# yum install memcached -y 2. 启动memcached[root@node01 software]# memcached -d -m 128m -p 11211 -l 192.168.9.11 -u root -P /tmp/ 3. 让node02和node03的tomcat使用memcached1234567891011[root@node02 bin]# cd /usr/local/software/apache-tomcat-7.0.61/conf/[root@node02 conf]# vi context.xml加入如下配置：&lt;Manager className=&quot;de.javakaffee.web.msm.MemcachedBackupSessionManager&quot; memcachedNodes=&quot;n1:192.168.9.11:11211&quot; sticky=&quot;false&quot; lockingMode=&quot;auto&quot; sessionBackupAsync=&quot;false&quot; requestUriIgnorePattern=&quot;.*\.(ico|png|gif|jpg|css|js)$&quot; sessionBackupTimeout=&quot;1000&quot; transcoderFactoryClass=&quot;de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory&quot; /&gt; 4. 把需要的jar包放到两台机器的tomcat的lib目录下 5.重启node02和node03的tomcat服务12[root@node03 bin]# ./shutdown.sh[root@node03 bin]# ./startup.sh 此时刷新页面，sessionId不会再改变，session在集群中的一致性问题得到解决，但还存在一种比memcached更优秀的内存数据库——redis.]]></content>
      <categories>
        <category>Bigdata</category>
        <category>loadBalance</category>
      </categories>
      <tags>
        <tag>Bigdata</tag>
        <tag>LVS+Keepalived+Nginx</tag>
        <tag>loadBalance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java远程调用shell脚本]]></title>
    <url>%2F2016%2F04%2F04%2Fjava%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8shell%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[目的：java程序调用远程服务器的shell脚本来实现服务的起、停、重启。 前言（废话）：由于知道我经验不足，领导给新同事分配任务时说：“这个java远程调用shell脚本重启服务，他两可能搞不出来，你研究下吧。”被人鄙视了一把，于是自己搭虚拟机，查资料，试一把，突然发现，好多事情其实没那么难嘛。 正文：Step1.准备脚本重启的关键在于关闭，要关闭程序，那就要先找到程序pid，然后kill。找到核心命令（获取pid）： 1ID=`ps -ef | grep "$NAME" | grep -v "grep" | grep -v kill | awk '&#123;print $2&#125;'` 于是初期博主调试后，写出了这样的shell 如果你的虚机是root用户，kill命令会很强大，所以一定判断参数是否为空或数字，否则，分分钟把你的所有进程都kill，虚机瞬间爆炸😂，重头再来。 12345678910111213141516171819202122232425#!/bin/shNAME=$1##检查参数，不能为空或纯数字,否则会kill几乎所有进程，直接死机。非root用户的话，应该不会死机，会报permission denied.a=`echo "$NAME" | grep [^0-9] &gt;/dev/null &amp;&amp; echo 0 || echo 1` #判断参数是否为数字,是数字则返回1，不是则返回0if [ "$NAME" == "" ]; then #$NAME一定要加引号，不然$NAME为空的时候就成了if[ == "" ],会报错：unary operatorecho "未输入要kill的进程名"elif [ $a = 1 ]; thenecho "进程名不能为纯数字"else##重启进程echo "-----------------------"echo -e "pNmae\t=\t$NAME"ID=`ps -ef | grep "$NAME" | grep -v "grep" | grep -v "restart" | awk '&#123;print $2&#125;'`echo -e "pid\t=\t$ID"echo "-----------------------"for id in $IDdokill -9 $idecho "killed $id"doneecho "-----------------------"echo "restarted" $NAME./$NAME 1&gt;/dev/null 2&gt;&amp;1 &amp;#./$NAMEfi 然后领导交于博主一份专业的shell脚本，瞬间把博主的三脚猫脚本秒成渣了。于是博主认真研读，添加注释，并稍作修改得到以下脚本。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#!/bin/sh# -*- coding: utf-8 -*-#Filename: server.sh.Chant#Author: Chant#Email: statuschuan@gmail.com#Date: 2017-06-03#Desc:#用途：该脚本用于停止、启动服务#使用说明：#启动该脚本时需要两个参数，参数均不能为空#第一个参数为：程序入口名称#第二个参数为：参数名称#注意事项：#远程调用时，请将脚本环境变量配置到/etc/bashrc或者用户目录下的.bashrcprint_usage()&#123; echo "Usage: $0 COMMAND" echo "where COMMAND is one of:" echo " help Help print this usage message" echo " start &lt;server_name&gt; &lt;server_param&gt; Start" echo " stop &lt;server_name&gt; &lt;server_param&gt; Stop" echo " restart &lt;server_name&gt; &lt;server_param&gt; Restart"&#125;start()&#123; #echo "start not suport now." cmd=$@ $cmd 1&gt;/dev/null 2&gt;&amp;1 &amp; #这里一定要写1&gt;/dev/null 2&gt;&amp;1 不能只写&amp;,否则远程调用时，会等待cmd的返回结果(stdOut)，就么法愉快地玩耍了。&#125;stop()&#123; #get arguments SERVER_NAME=$1 SERVER_PARAM=$2 PROCESS_NAME="$1 $2" #echo $PROCESS_NAME if [ "$1" = "" ]; then echo "第二个参数不能为空" exit 0; fi if [ -z "$2" ]; then echo "第三个参数不能为空" exit 0; fi #get process's pids pids=`ps -ef|grep "$PROCESS_NAME"|grep -v "grep"|awk '&#123;print $2&#125;'` #Chant:使用以下命令可以过滤掉脚本本身的pid，就不用写后面的判断语句了。 #但是其实用$$获取当前脚本pid在逻辑上更严密，否则，万一你的脚本名和要操作的程序名有相同部分就会出问题， #eg: 脚本名为：ser.sh 而程序名为：poser.sh,那么由于grep -v "$0" 就取不到其pid了 #pids=`ps -ef|grep "$PROCESS_NAME"|grep -v "grep"| grep -v "$0" |awk '&#123;print $2&#125;'` # 为basename指定一个路径，basename命令会删掉所有的前缀包括最后一个slash（‘/’）字符，然后将字符串显示出来。 #pids=`ps -ef|grep "$PROCESS_NAME"|grep -v "grep"| grep -v "$(basename $0)" |awk '&#123;print $2&#125;'` current_pid=$$ echo process pids is $pids. #kill process if [ -n "$pids" ]; #判断pids是否为空，引号必须加。"$pids" == ""等效 then for pid in $pids do #current shell pid shoud not kill. if [ $pid -ne $current_pid ]; then #check $pid is exist or not check=`ps -p $pid` if [ $? -eq 0 ]; then echo kill $pid start. kill -9 $pid #judge result if [ $? -eq 0 ]; then echo kill $pid success. else echo kill $pid fail. fi fi fi done else echo "$PROCESS_NAME does not exist." fi&#125;# get command argumentsCOMMAND=$1shift# support help commandscase $COMMAND in --help|-help|-h|help) print_usage exit 0 ;; "") print_usage exit 0 ;; "start") start $@ echo "$@ started" exit 0 ;; "stop") stop $@ exit 0 ;; "restart") start and stop的参数需要一致才可以，如果不一致则需要调整参数传入方式 stop $@ eep 3 tart $@ echo "$@ restarted" exit 0 ;;esac Step2.java程序远程调用shell 1.导入需要依赖的jar包。Java远程调用Shell脚本这个程序需要ganymed-ssh2-build210.jar包。里面还有example包，方便学习。为了调试方便，可以将\ganymed-ssh2-build210\src下的代码直接拷贝到我们的工程里，此源码的好处就是没有依赖很多其他的包，拷贝过来干干净净。2.导入commons-io包，里面的IOUtils会经常使用。 123456789101112131415&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/commons-io/commons-io --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/ch.ethz.ganymed/ganymed-ssh2 --&gt; &lt;dependency&gt; &lt;groupId&gt;ch.ethz.ganymed&lt;/groupId&gt; &lt;artifactId&gt;ganymed-ssh2&lt;/artifactId&gt; &lt;version&gt;build210&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3、编写RemoteShellExecutor工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package RemoteShell;import java.io.*;import java.nio.charset.Charset;import org.apache.commons.io.IOUtils;import ch.ethz.ssh2.ChannelCondition;import ch.ethz.ssh2.Connection;import ch.ethz.ssh2.Session;import ch.ethz.ssh2.StreamGobbler;/** * Created by Chant on 2017/5/27. * 远程调用脚本重启服务 */public class RemoteShellExecutor &#123; private Connection conn; /** 远程机器IP */ private String ip; /** 用户名 */ private String osUsername; /** 密码 */ private String password; private String charset = Charset.defaultCharset().toString(); private static final int TIME_OUT = 1000 * 5 * 60; /** * 构造函数 * @param ip * @param usr * @param pasword */ public RemoteShellExecutor(String ip, String usr, String pasword) &#123; this.ip = ip; this.osUsername = usr; this.password = pasword;// System.out.println(charset); &#125; /** * 登录 * @return * @throws IOException */ private boolean login() throws IOException &#123; conn = new Connection(ip); conn.connect(); return conn.authenticateWithPassword(osUsername, password); &#125; /** * 执行脚本 * * @param cmds * @return * @throws Exception */ public int exec(String cmds) throws Exception &#123; InputStream stdOut = null; InputStream stdErr = null; String outStr = ""; String outErr = ""; int ret = -1; try &#123; if (login()) &#123; // Open a new &#123;@link Session&#125; on this connection Session session = conn.openSession(); // Execute a command on the remote machine. session.execCommand(cmds); stdOut = new StreamGobbler(session.getStdout()); outStr = processStream(stdOut, charset); stdErr = new StreamGobbler(session.getStderr()); outErr = processStream(stdErr, charset); session.waitForCondition(ChannelCondition.EXIT_STATUS, TIME_OUT); System.out.println("outStr=" +"\n"+ outStr); System.out.println("outErr=" +"\n"+ outErr); ret = session.getExitStatus(); &#125; else &#123; throw new Exception("登录远程机器失败" + ip); // 自定义异常类 实现略 &#125; &#125; finally &#123; if (conn != null) &#123; conn.close(); &#125; IOUtils.closeQuietly(stdOut); IOUtils.closeQuietly(stdErr); &#125; return ret; &#125; /** * @param in * @param charset * @return * @throws IOException * @throws UnsupportedEncodingException */// private String processStream(InputStream in, String charset) throws Exception &#123;// byte[] buf = new byte[1024];// StringBuilder sb = new StringBuilder();// while (in.read(buf) != -1) &#123;// sb.append(new String(buf, charset));// &#125;// return sb.toString();// &#125; private String processStream(InputStream in, String charset)throws Exception &#123; StringBuilder sb = new StringBuilder(); BufferedReader bufr = new BufferedReader(new InputStreamReader(in,charset)); String line = null; while((line = bufr.readLine()) != null)&#123; sb.append(line); sb.append("\n");//？？换行符是依赖平台的 &#125; return sb.toString(); &#125;&#125; 4、Java程序调用远程Shell 1234567public static void main(String args[]) throws Exception &#123; RemoteShellExecutor("10.10.10.100", "root", "123123"); String proName = "entranceMain.sh"; String para = "1"; System.out.println(executor.exec("server.sh.Chant start " + proName +" "+ para));&#125; 将start换为restart，stop，测试结果如下，注意pid发生变化，说明重启成功。 参考资料：Java程序调用远程Shell脚本]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIVE和HBASE区别]]></title>
    <url>%2F2016%2F03%2F06%2FHIVE%E5%92%8CHBASE%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[从架构图中不难看出，Hive是运行在MapReduce之上的，而Hbase则是在更底层的HDFS之上，他们的读写和查询特点也因此而迥然不同。 如果英语不错的话，请直接跳到英文原版 1. 两者分别是什么？ Apache Hive 是一个构建在Hadoop基础设施之上的数据仓库。通过Hive可以使用HQL语言查询存放在HDFS上的数据。HQL是一种类SQL语言，这种语言最终被转化为Map/Reduce. 虽然Hive提供了SQL查询功能，但是Hive不能够进行交互查询–因为它只能够在Haoop上批量的执行Hadoop。 Apache HBase 是一种Key/Value系统，一种运行于HDFS顶层的NoSQL(=Not Only SQL，泛指非关系型的数据库)数据库系统。区别于Hive，HBase具备随即读写功能，是一种面向列的数据库。和Hive不一样，Hbase的能够在它的数据库上实时运行，而不是运行MapReduce任务。 Hbase被分区为表格，表格又被进一步分割为列簇。列簇必须使用schema定义，列簇将某一类型列集合起来（列不要求schema定义）。例如，“message”列簇可能包含：“to”, ”from” “date”, “subject”, 和”body”. 每一个 key/value对在Hbase中被定义为一个cell，每一个key由row-key，列簇、列和时间戳组成。 在Hbase中，行是key/value映射的集合，这个映射通过row-key来唯一标识。Hbase利用Hadoop的基础设施，可以利用通用的设备进行水平的扩展。 2. 两者的特点Hive帮助熟悉SQL的人运行MapReduce任务。因为它是JDBC兼容的，同时，它也能够和现存的SQL工具整合在一起。**运行Hive查询会花费很长时间，因为它会默认遍历表中所有的数据。** 虽然有这样的缺点，一次遍历的数据量可以通过Hive的分区机制来控制。分区允许在数据集上运行过滤查询，这些数据集存储在不同的文件夹内，查询的时候只遍历指定文件夹（分区）中的数据。这种机制可以用来，例如，只处理在某一个时间范围内的文件，只要这些文件名中包括了时间格式。 HBase通过存储key/value来工作。它支持四种主要的操作: • 添加或更新数据行 • 扫描获取某范围内的cells • 为某一具体数据行返回对应的cells • 从数据表中删除数据行/列，或列的描述信息 • 列信息可用于获取数据变动前的取值（透过HBase压缩策略可以删除列信息历史记录来释放存储空间）。 虽然HBase包括表格，但是schema仅仅被表格和列簇所要求，列不需要schema。Hbase的表格包括增加/计数功能。 3. 限制Hive不支持常规的SQL更新语句，如：数据插入，更新，删除。因为其对数据的操作是针对整个数据表的。另外，由于hive在hadoop上运行批量操作，它需要花费很长的时间，通常是几分钟到几个小时才可以获取到查询的结果。Hive必须提供预先定义好的schema将文件和目录映射到列，并且Hive与ACID不兼容。HBase查询是通过特定的语言来编写的，这种语言需要重新学习。类SQL的功能可以通过Apache Phonenix实现，但这是以必须提供schema为代价的。另外，Hbase也并不是兼容所有的ACID特性，虽然它支持某些特性。最后但是最重要的–为了运行Hbase，Zookeeper是必须的，zookeeper是一个用来进行分布式协调的服务，这些服务包括配置服务，维护元信息和命名空间服务。 4. 应用场景Hive适合用来 对一段时间内的数据进行分析查询，适用于网络日志等数据量大、静态的数据查询 。例如，用来计算趋势或者网站的日志。Hive不应该用来进行实时的查询。因为它需要很长时间才可以返回结果。 Hbase非常适合用来 进行大数据的实时查询。Facebook用Hbase进行消息和实时的分析。它也可以用来统计Facebook的连接数。 5. 总结Hive和Hbase是两种基于Hadoop的不同技术–Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。当然，这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到Hbase，设置再从Hbase写回Hive。例如：利用Hive处理静态离线数据，利用HBase进行联机实时查询，而后对两者间的结果集进行整合归并，从而使得数据完整且永葆青春，为进一步的商业分析提供良好支持。 6.英文原版万一你英语不错却翻不了墙，那多尴尬啊。😂给你准备了英文原版，请享用。 Hive vs. HBase By Saggi Neumann Big Data May 26, 2014 Comparing Hive with HBase is like comparing Google with Facebook - although they compete over the same turf (our private information), they don’t provide the same functionality. But things can get confusing for the Big Data beginner when trying to understand what Hive and HBase do and when to use each one of them. Let’s try and clear it up. What They DoApache Hive is a data warehouse infrastructure built on top of Hadoop. It allows for querying data stored on HDFS for analysis via HQL, an SQL-like language that gets translated to MapReduce jobs. Despite providing SQL functionality, Hive does not provide interactive querying yet - it only runs batch processes on Hadoop.Apache HBase is a NoSQL key/value store which runs on top of HDFS. Unlike Hive, HBase operations run in real-time on its database rather than MapReduce jobs. HBase is partitioned to tables, and tables are further split into column families. Column families, which must be declared in the schema, group together a certain set of columns (columns don’t require schema definition). For example, the “message” column family may include the columns: “to”, “from”, “date”, “subject”, and “body”. Each key/value pair in HBase is defined as a cell, and each key consists of row-key, column family, column, and time-stamp. A row in HBase is a grouping of key/value mappings identified by the row-key. HBase enjoys Hadoop’s infrastructure and scales horizontally using off the shelf servers. FeaturesHive can help the SQL savvy to run MapReduce jobs. Since it’s JDBC compliant, it also integrates with existing SQL based tools. Running Hive queries could take a while since they go over all of the data in the table by default. Nonetheless, the amount of data can be limited via Hive’s partitioning feature. Partitioning allows running a filter query over data that is stored in separate folders, and only read the data which matches the query. It could be used, for example, to only process files created between certain dates, if the files include the date format as part of their name.HBase works by storing data as key/value. It supports four primary operations: put to add or update rows, scan to retrieve a range of cells, get to return cells for a specified row, and delete to remove rows, columns or column versions from the table. Versioning is available so that previous values of the data can be fetched (the history can be deleted every now and then to clear space via HBase compactions). Although HBase includes tables, a schema is only required for tables and column families, but not for columns, and it includes increment/counter functionality. LimitationsHive does not currently support update statements. Additionally, since it runs batch processing on Hadoop, it can take minutes or even hours to get back results for queries. Hive must also be provided with a predefined schema to map files and directories into columns and it is not ACID compliant.HBase queries are written in a custom language that needs to be learned. SQL-like functionality can be achieved via Apache Phoenix, though it comes at the price of maintaining a schema. Furthermore, HBase isn’t fully ACID compliant, although it does support certain properties. Last but not least - in order to run HBase, ZooKeeper is required - a server for distributed coordination such as configuration, maintenance, and naming. Use CasesHive should be used for analytical querying of data collected over a period of time - for instance, to calculate trends or website logs. Hive should not be used for real-time querying since it could take a while before any results are returned.HBase is perfect for real-time querying of Big Data. Facebook use it for messaging and real-time analytics. They may even be using it to count Facebook likes. SummaryHive and HBase are two different Hadoop based technologies - Hive is an SQL-like engine that runs MapReduce jobs, and HBase is a NoSQL key/value database on Hadoop. But hey, why not use them both? Just like Google can be used for search and Facebook for social networking, Hive can be used for analytical queries while HBase for real-time querying. Data can even be read and written from Hive to HBase and back again. 参考资料：主要源于：HIVE和HBASE区别 汉化于 原文出处部分源于：浅谈Hive vs. HBase 区别在哪里]]></content>
      <categories>
        <category>Bigdata</category>
      </categories>
      <tags>
        <tag>Bigdata</tag>
        <tag>Hive</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程]]></title>
    <url>%2F2015%2F11%2F21%2Fjava%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程大总结 并行与并发 并行：多个cpu实例或者多台机器同时执行一段处理逻辑，是真正的同时。 并发：通过cpu调度算法，让用户看上去同时执行，实际上从cpu操作层面不是真正的同时。并发往往在场景中有公用的资源，那么针对这个公用的资源往往产生瓶颈，我们会用TPS或者QPS来反应这个系统的处理能力。 Java线程具有五种基本状态 网上有很多图，但是感觉做好的还是这张图（虽然里面interrupt写错成interupt，但是总体架构却是最清晰完整的）新建状态（New）：当线程对象对创建后，即进入了新建状态，如：Thread t = new MyThread();就绪状态（Runnable）：当调用线程对象的start()方法（t.start();），线程即进入就绪状态。处于就绪状态的线程，只是说明此线程已经做好了准备，随时等待CPU调度执行，并不是说执行了t.start()此线程立即就会执行；运行状态（Running）：当CPU开始调度处于就绪状态的线程时，此时线程才得以真正执行，即进入到运行状态。注：就绪状态是进入到运行状态的唯一入口，也就是说，线程要想进入运行状态执行，首先必须处于就绪状态中；阻塞状态（Blocked）：处于运行状态中的线程由于某种原因，暂时放弃对CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才有机会再次被CPU调用以进入到运行状态。根据阻塞产生的原因不同，阻塞状态又可以分为三种： 等待阻塞：运行状态中的线程执行wait()方法，使本线程进入到等待阻塞状态； 同步阻塞 – 线程在获取synchronized同步锁失败(因为锁被其它线程所占用)，它会进入同步阻塞状态； 其他阻塞 – 通过调用线程的sleep()或join()或发出了I/O请求时，线程会进入到阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。 死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。 Object的方法synchronized, wait, notify 是属于Object的方法。 它们是应用于同步问题的人工线程调度工具。讲其本质，首先就要明确monitor的概念，Java中的每个对象都有一个监视器，来监测并发代码的重入（也就是很多人说的锁）。在非多线程编码时该监视器不发挥作用，反之如果在synchronized 范围内，监视器发挥作用。 wait/notify必须存在于synchronized块中。并且，这三个关键字针对的是同一个监视器（某对象的监视器）。这意味着wait之后，其他线程可以进入同步块执行（也就是说wait会释放锁，而sleep是属于Thread的方法，不会释放锁）。 当某代码并不持有监视器的使用权时（即已经脱离同步块）去使用wait或notify，会抛出java.lang.IllegalMonitorStateException。也包括在synchronized块中去调用另一个对象的wait/notify，因为不同对象的监视器不同，同样会抛出此异常。 synchronized单独使用： 同步代码块：如下，在多线程环境下，synchronized块中的方法获取了lock实例的monitor，如果实例相同，那么只有一个线程能执行该块内容public 12345678public class Thread1 implements Runnable &#123; Object lock; public void run() &#123; synchronized(lock)&#123; ..do something &#125; &#125;&#125; 直接用于方法： 相当于上面代码中用lock来锁定的效果，实际获取的是Thread1类的monitor。更进一步，如果修饰的是static方法，则锁定该类所有实例。 12345public class Thread1 implements Runnable &#123; public synchronized void run() &#123; ..do something &#125;&#125; 生产者消费者问题(Producer-consumer problem)synchronized, wait, notify结合：典型场景生产者消费者问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class TestProduce &#123; public static void main(String[] args) &#123; SyncStack sStack = new SyncStack(); Shengchan sc = new Shengchan(sStack); Xiaofei xf = new Xiaofei(sStack); sc.start(); xf.start(); &#125;&#125;class Mantou &#123; int id; Mantou(int id)&#123; this.id=id; &#125;&#125;class SyncStack&#123; int index=0; Mantou[] ms = new Mantou[10]; public synchronized void push(Mantou m)&#123; while(index==ms.length)&#123; try &#123; this.wait(); //wait后，线程会将持有的锁释放。sleep是即使睡着也持有互斥锁。 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; this.notify(); //唤醒在当前对象等待池中等待的第一个线程。notifyAll叫醒所有在当前对象等待池中等待的所有线程。 //如果不唤醒的话。以后这两个线程都会进入等待线程，没有人唤醒。 ms[index]=m; index++; &#125; public synchronized Mantou pop()&#123; while(index==0)&#123; try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; this.notify(); index--; return ms[index]; &#125;&#125;class produce extends Thread&#123; SyncStack ss = null; public Shengchan(SyncStack ss) &#123; this.ss=ss; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 20; i++) &#123; System.out.println("造馒头："+i); Mantou m = new Mantou(i); ss.push(m); &#125; &#125;&#125;class consume extends Thread&#123; SyncStack ss = null; public Xiaofei(SyncStack ss) &#123; this.ss=ss; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 20; i++) &#123; Mantou m = ss.pop(); System.out.println("吃馒头："+i); &#125; &#125;&#125; volatile多线程的内存模型：main memory（主存）、working memory（线程栈），在处理数据时，线程会把值从主存load到本地栈，完成操作后再save回去(volatile关键词的作用：每次针对该变量的操作都激发一次load and save)。 针对多线程使用的变量如果不是volatile或者final修饰的，很有可能产生不可预知的结果（另一个线程修改了这个值，但是之后在某线程看到的是修改之前的值）。其实道理上讲同一实例的同一属性本身只有一个副本。但是多线程是会缓存值的，本质上，volatile就是不去缓存，直接取值。在线程安全的情况下加volatile会牺牲性能。 基本线程类基本线程类指的是Thread类，Runnable接口，Callable接口。 ####Thread类，Runnable接口继承Thread类（Thread类本身就是继承了Runnable接口的），实现Runnable接口都可以创建线程。如果Thread和Runnable都复写了run()方法，最终会运行哪一个run()方法呢？12345678910111213141516171819202122232425262728293031323334353637383940public class ThreadTest &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getName() + " " + i); if (i == 30) &#123; Runnable myRunnable = new MyRunnable(); Thread thread = new MyThread(myRunnable); thread.start(); &#125; &#125; &#125;&#125;class MyRunnable implements Runnable &#123; private int i = 0; @Override public void run() &#123; System.out.println("in MyRunnable run"); for (i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getName() + " " + i); &#125; &#125;&#125;class MyThread extends Thread &#123; private int i = 0; public MyThread(Runnable runnable)&#123;super(runnable);&#125; @Override public void run() &#123; System.out.println("in MyThread run"); for (i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getName() + " " + i); &#125; &#125;&#125; 同样的，与实现Runnable接口创建线程方式相似，不同的地方在于 1Thread thread = new MyThread(myRunnable); 通过输出我们知道线程执行体是MyThread类中的run()方法。其实原因很简单，因为Thread类本身也是实现了Runnable接口，而run()方法最先是在Runnable接口中定义的方法。我们看一下Thread类中对Runnable接口中run()方法的实现：123456@Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; 也就是说，当执行到Thread类中的run()方法时，会首先判断target是否存在，存在则执行target中的run()方法，也就是实现了Runnable接口并重写了run()方法的类中的run()方法。但是上述给到的列子中，由于多态的存在，根本就没有执行到Thread类中的run()方法，而是直接先执行了运行时类型即MyThread类中的run()方法。 Callablefuture模式：并发模式的一种，可以有两种形式，即无阻塞和阻塞，分别是isDone和get。其中Future对象用来存放该线程的返回值以及状态。这是Thread和Runnable做不到的 12345ExecutorService e = Executors.newFixedThreadPool(3); //submit方法有多重参数版本，及支持callable也能够支持runnable接口类型.Future future = e.submit(new myCallable());future.isDone() //return true,false 无阻塞future.get() // return 返回值，阻塞直到该线程运行结束 后台线程（Daemon Thread）概念/目的：后台线程主要是为其他线程（相对可以称之为前台线程）提供服务，或“守护线程”。如JVM中的垃圾回收线程。生命周期：后台线程的生命周期与前台线程生命周期有一定关联。主要体现在：当所有的前台线程都进入死亡状态时，后台线程会自动死亡(其实这个也很好理解，因为后台线程存在的目的在于为前台线程服务的，既然所有的前台线程都死亡了，那它自己还留着有什么用…伟大啊 ! !)。设置后台线程：调用Thread对象的setDaemon(true)方法可以将指定的线程设置为后台线程。 Thread类相关方法：12345678//当前线程可转让cpu控制权，让别的就绪状态线程运行（切换）public static Thread.yield() //暂停一段时间public static Thread.sleep() //在一个线程中调用other.join(),将等待other执行完后才继续本线程。 public join()//后两个函数皆可以被打断public interrupt() 关于中断它并不像stop方法那样会中断一个正在运行的线程。线程会不时地检测中断标识位，以判断线程是否应该被中断（中断标识值是否为true）。中断只会影响到wait状态、sleep状态和join状态。被打断的线程会抛出InterruptedException。Thread.interrupted()检查当前线程是否发生中断，返回boolean类型值。synchronized在获锁的过程中是不能被中断的。 中断是一个状态！interrupt()方法只是将这个状态置为true而已。所以说正常运行的程序不去检测状态，就不会终止，而wait等阻塞方法会去检查并抛出异常。如果在正常运行的程序中添加while(!Thread.interrupted())，则同样可以在中断后离开代码体。 Thread类最佳实践写的时候最好要设置线程名称 Thread.name，并设置线程组 ThreadGroup，目的是方便管理。在出现问题的时候，打印线程栈 (jstack -pid) 一眼就可以看出是哪个线程出的问题，这个线程是干什么的。 Java多线程的就绪、运行和死亡状态以及停止线程 就绪状态转换为运行状态：当此线程得到处理器资源； 运行状态转换为就绪状态：当此线程主动调用yield()方法或在运行过程中失去处理器资源。 运行状态转换为死亡状态：当此线程线程执行体执行完毕或发生了异常。 此处需要特别注意的是：当调用线程的yield()方法时，线程从运行状态转换为就绪状态，但接下来CPU调度就绪状态中的哪个线程具有一定的随机性，因此，可能会出现A线程调用了yield()方法后，接下来CPU仍然调度了A线程的情况。 由于实际的业务需要，常常会遇到需要在特定时机终止某一线程的运行，使其进入到死亡状态。目前最通用的做法是设置一boolean型的变量，当条件满足时，使线程执行体快速执行完毕。如：12345678910111213141516171819202122232425262728293031323334public class ThreadTest &#123; public static void main(String[] args) &#123; MyRunnable myRunnable = new MyRunnable(); Thread thread = new Thread(myRunnable); for (int i = 0; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getName() + " " + i); if (i == 30) &#123; thread.start(); &#125; if(i == 40)&#123; myRunnable.stopThread(); &#125; &#125; &#125;&#125;class MyRunnable implements Runnable &#123; private boolean stop; @Override public void run() &#123; for (int i = 0; i &lt; 100 &amp;&amp; !stop; i++) &#123; System.out.println(Thread.currentThread().getName() + " " + i); &#125; &#125; public void stopThread() &#123; this.stop = true; &#125;&#125; 高级多线程控制类以上都属于内功心法，接下来是实际项目中常用到的工具了，Java1.5提供了一个非常高效实用的多线程包:java.util.concurrent, 提供了大量高级工具,可以帮助开发者编写高效、易维护、结构清晰的Java多线程程序。 Lock类 lock: 在java.util.concurrent包内。共有三个实现： • ReentrantLock • ReentrantReadWriteLock.ReadLock • ReentrantReadWriteLock.WriteLock主要目的是和synchronized一样， 两者都是为了解决同步问题，处理资源争端而产生的技术。功能类似但有一些区别。区别如下： 1. lock更灵活，可以自由定义多把锁的枷锁解锁顺序（synchronized要按照先加的后解顺序） 2. 提供多种加锁方案，lock 阻塞式, trylock 无阻塞式, lockInterruptily 可打断式， 还有trylock的带超时时间版本。 3. 本质上和监视器锁（即synchronized是一样的） 4. 能力越大，责任越大，必须控制好加锁和解锁，否则会导致灾难。 5. 和Condition类的结合。 6. 性能更高，对比如下图： ReentrantLock可重入的意义在于持有锁的线程可以继续持有，并且要释放对等的次数后才真正释放该锁。使用方法是：1.先new一个实例1static ReentrantLock r=new ReentrantLock(); 2.加锁1r.lock()或r.lockInterruptibly(); 此处也是个不同，后者可被打断。当a线程lock后，b线程阻塞，此时如果是lockInterruptibly，那么在调用b.interrupt()之后，b线程退出阻塞，并放弃对资源的争抢，进入catch块。（如果使用后者，必须throw interruptable exception 或catch）3.释放锁1r.unlock() 必须做！何为必须做呢，要放在finally里面。以防止异常跳出了正常流程，导致灾难。这里补充一个小知识点，finally是可以信任的：经过测试，哪怕是发生了OutofMemoryError，finally块中的语句执行也能够得到保证。 ReentrantReadWriteLock可重入读写锁（读写锁的一个实现123ReentrantReadWriteLock lock = new ReentrantReadWriteLock()ReadLock r = lock.readLock();WriteLock w = lock.writeLock(); 两者都有lock,unlock方法。写写，写读互斥；读读不互斥。可以实现并发读的高效线程安全代码 管理类管理类的概念比较泛，用于管理线程，本身不是多线程的，但提供了一些机制来利用上述的工具做一些封装。了解到的值得一提的管理类：ThreadPoolExecutor和 JMX框架下的系统级管理类 ThreadMXBeanThreadPoolExecutor如果不了解这个类，应该了解前面提到的ExecutorService，开一个自己的线程池非常方便：12345678ExecutorService e = Executors.newCachedThreadPool();ExecutorService e = Executors.newSingleThreadExecutor();ExecutorService e = Executors.newFixedThreadPool(3);// 第一种是可变大小线程池，按照任务数来分配线程，// 第二种是单线程池，相当于FixedThreadPool(1)// 第三种是固定大小线程池。// 然后运行e.execute(new MyRunnableImpl()); 该类内部是通过ThreadPoolExecutor实现的，掌握该类有助于理解线程池的管理，本质上，他们都是ThreadPoolExecutor类的各种实现版本。请参见javadoc： corePoolSize:池内线程初始值与最小值，就算是空闲状态，也会保持该数量线程。maximumPoolSize:线程最大值，线程的增长始终不会超过该值。keepAliveTime：当池内线程数高于corePoolSize时，经过多少时间多余的空闲线程才会被回收。回收前处于wait状态unit：时间单位，可以使用TimeUnit的实例，如TimeUnit.MILLISECONDSworkQueue:待入任务（Runnable）的等待场所，该参数主要影响调度策略，如公平与否，是否产生饿死(starving)threadFactory:线程工厂类，有默认实现，如果有自定义的需要则需要自己实现ThreadFactory接口并作为参数传入。 参考文章 Java中的多线程你只要看这一篇就够了 Java总结篇系列：Java多线程（一） Java总结篇系列：Java多线程（二） 尚学堂java300集]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在hexo博客中插入图片，音乐，视频，公式]]></title>
    <url>%2F2015%2F11%2F04%2F%E5%9C%A8hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%EF%BC%8C%E9%9F%B3%E4%B9%90%EF%BC%8C%E8%A7%86%E5%B1%8F%EF%BC%8C%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[摘要：如何简洁优雅地在hexo博客中插入图片，音乐，视频，公式.这篇博客本来是拿来测试图片音乐的插入的，但是测着测着，忽然灵光一现，为什么不直接写成一篇博客呢？ 页内跳转添加一则小技巧，实现markdown页内跳转。html标签实现 1. 定义一个锚(id)：&lt;span id=&quot;jump&quot;&gt;跳转到的地方&lt;/span&gt; 2. 使用markdown语法：[点击跳转](#jump) Example：我在本文参考文献前加了一行&lt;span id=&quot;jump&quot;&gt; &lt;/span&gt;，然后在此处编写[点击跳转](#jump)点击跳转 图片关于插入图片，网上的图床（注册麻烦，使用麻烦，需要上传，网速不行咋办？类似Lightshot Screenshot，上传基本30秒，如果你要写一份需要详细截图的安装文档，那估计一天的时间都花在上传图片上了），hexo官方的方法（将图片放在source/image/下，这样在编辑器中并不能实时预览，而且你还需要记住图片的名字。）都令人觉得插入图片简直是个噩梦。知道我遇到了MWeb，一个可以直接把图片拖入即可完成图片插入的Markdown编辑器，或者使用微信截图Ctrl+command+a保存(记得是点下载按钮，不是点那个小红勾)后，直接Command+v,图片就插入了。而且其低调简洁的界面，简单实用的快捷键，完美的诠释了什么叫优雅的写作方式（一些bug和程序崩溃除外）。当然如此美腻而又强大，必然不便宜的，官方价格￥98。不要桑心，他还有免费的lite版，两个版本的区别主要是以下几点： 文档库文档限制在 10 个以内；（然鹅，写hexo一直都是用外部模式，压根不需要文档库）外部模式可引入的文件夹限制在 1 个；（然鹅，一个文件下可以有无数个文件夹）支持的发布服务只可增加 1 个；（然鹅，写hexo博客，不需要编辑器来帮我发布） 当然，MWeb不只是拿来写hexo博客的，更多信息请戳MWeb官网以及帮助文档。 1![](/media/14967219636093.jpg) 当然，markdown是支持GIF动图的，使用方法和图片一致，只要你放的链接是一张动图的链接就行。 Eaxmple: 1![](http://upload-images.jianshu.io/upload_images/291600-3b00271942fef854.gif?imageMogr2/auto-orient/strip) 音乐1.使用网易云音乐的外链播放器比如你右键某首歌，复制链接的到链接为http://music.163.com/#/m/song?id=2919622只需要其中的id=2919622就够了。 那么在文章中使用ifname标签如下，记得将其中的id改为你想要的音乐id。 1&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=430 height=86 src="//music.163.com/outchain/player?type=2&amp;id=2919622&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt; 2.使用 Hexo 插件插入音乐/音频hexo-tag-aplayer：https://github.com/grzhan/hexo-tag-aplayer官方简介：Embed APlayer(https://github.com/DIYgod/APlayer) in Hexo posts/pages.很明显，除了posts，还可以在pages页面使用。 Installation: 1npm install --save hexo-tag-aplayer Usage:1&#123;% aplayer title author url [picture_url, narrow, autoplay, width:xxx, lrc:xxx] %&#125; 参数说明及详细信息，请戳github，还有加入歌词和播放例表等强大功能等你探索。Example:1&#123;% aplayer "她的睫毛" "周杰伦" "http://home.ustc.edu.cn/~mmmwhy/%d6%dc%bd%dc%c2%d7%20-%20%cb%fd%b5%c4%bd%de%c3%ab.mp3" "http://home.ustc.edu.cn/~mmmwhy/jay.jpg" "autoplay=false" %&#125; new APlayer({ element: document.getElementById("aplayer0"), narrow: false, autoplay: false, showlrc: 0, music: { title: "她的睫毛", author: "周杰伦", url: "http://home.ustc.edu.cn/~mmmwhy/%d6%dc%bd%dc%c2%d7%20-%20%cb%fd%b5%c4%bd%de%c3%ab.mp3", pic: "http://home.ustc.edu.cn/~mmmwhy/jay.jpg", } }); 使用唱吧录制并上传音频，用浏览器打开分享链接,右键显示网页源代码 搜索mp3，找到你的音频链接。 1&#123;% aplayer "Job or Education" "Chant" "http://lzscuw.changba.com/899486104.mp3" "http://aliimg.changba.com/cache/photo/735572112_640_640.jpg" "autoplay=false" %&#125; new APlayer({ element: document.getElementById("aplayer1"), narrow: false, autoplay: false, showlrc: 0, music: { title: "Job or Education", author: "Chant", url: "http://lzscuw.changba.com/899486104.mp3", pic: "http://aliimg.changba.com/cache/photo/735572112_640_640.jpg", } }); 视频1使用标签插入视频1.1使用iframe标签插入视频一般的国内网站，获取嵌入代码的方法如下图： Youtube，右键视频，复制嵌入代码，直接将嵌入代码粘贴进你的markdown文章就OK啦。 此方法在嵌入Youtube视频时，其成败与hexo主题有关，在本主题下加载嵌入失败，在别的主题下，比如maupassant成功。1&lt;iframe width=&quot;854&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/xqf2DJgucsU&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt; 但腾讯视频的引用嵌入是没有问题的。1&lt;iframe frameborder=&quot;0&quot; width=&quot;640&quot; height=&quot;498&quot; src=&quot;https://v.qq.com/iframe/player.html?vid=g0512cgb51w&amp;tiny=0&amp;auto=0&quot; allowfullscreen&gt;&lt;/iframe&gt; 1.2使用embed标签插入视屏在优酷，腾讯等等网站都是一样的方法，如图：然后将代码直接粘贴到你的markdown就OK了。1&lt;embed src=&quot;https://imgcache.qq.com/tencentvideo_v1/playerv3/TPout.swf?max_age=86400&amp;v=20161117&amp;vid=g0512cgb51w&amp;auto=0&quot; allowFullScreen=&quot;true&quot; quality=&quot;high&quot; width=&quot;480&quot; height=&quot;400&quot; align=&quot;middle&quot; allowScriptAccess=&quot;always&quot; type=&quot;application/x-shockwave-flash&quot;&gt;&lt;/embed&gt; 2.使用hexo插件插入视频hexo-tag-dplayer：https://github.com/NextMoe/hexo-tag-dplayer与aplayer类似，不过它是用来插入视频。 Installation: 1npm install hexo-tag-dplayer --save Usage: 1&#123;% dplayer key=value ... %&#125; 参数说明及详细信息，请戳githubExample:首先，也是小白最难的一步，找到视频源 以下代码中的url即为你刚才复制的视频源链接12&#123;% dplayer &quot;url=http://ugcydzd.qq.com/flv/92/216/g0512cgb51w.p712.1.mp4?sdtfrom=v1010&amp;guid=72ffbc53bc13455246dcec4efd2c2b02&amp;vkey=0CE5FC72FB6FA6D97FD1077E0449F0AB0ADDF71FFE82014D6FA31F80237EA5C3A40C048E207507FA283E0EB1C1C3E10188B0D7CAD66E072FF8AB6BBC2D2E8E9E34631C122081535D7168D0D2723548E25E94E04EC20FD1A10848CDB66FBE45E35E6F7D1D0C3C0520AFA5331498386C8D&quot; &quot;pic=/media/14972328758364.jpg&quot; &quot;loop=yes&quot; &quot;theme=#FADFA3&quot; &quot;autoplay=false&quot; &quot;token=tokendemo&quot; %&#125; 效果如下：var dplayer0 = new DPlayer({"element":document.getElementById("dplayer0"),"autoplay":false,"theme":"#FADFA3","loop":true,"video":{"url":"http://ugcydzd.qq.com/flv/92/216/g0512cgb51w.p712.1.mp4?sdtfrom=v1010&guid=72ffbc53bc13455246dcec4efd2c2b02&vkey=0CE5FC72FB6FA6D97FD1077E0449F0AB0ADDF71FFE82014D6FA31F80237EA5C3A40C048E207507FA283E0EB1C1C3E10188B0D7CAD66E072FF8AB6BBC2D2E8E9E34631C122081535D7168D0D2723548E25E94E04EC20FD1A10848CDB66FBE45E35E6F7D1D0C3C0520AFA5331498386C8D","pic":"/media/14972328758364.jpg"}}); 此方法youtube视频的引用还在探索中。 var dplayer1 = new DPlayer({"element":document.getElementById("dplayer1"),"video":{"url":"https://www.youtube.com/embed/xqf2DJgucsU"}}); 以下是官方示例，点击设置按钮还有弹幕、速度、洗脑循环等选项。 1&#123;% dplayer "url=http://devtest.qiniudn.com/若能绽放光芒.mp4" "addition=https://dplayer.daoapp.io/bilibili?aid=4157142" "api=http://dplayer.daoapp.io" "pic=http://devtest.qiniudn.com/若能绽放光芒.png" "id=9E2E3368B56CDBB4" "loop=yes" "theme=#FADFA3" "autoplay=false" "token=tokendemo" %&#125; var dplayer2 = new DPlayer({"element":document.getElementById("dplayer2"),"autoplay":false,"theme":"#FADFA3","loop":true,"video":{"url":"http://devtest.qiniudn.com/若能绽放光芒.mp4","pic":"http://devtest.qiniudn.com/若能绽放光芒.png"},"danmaku":{"api":"http://dplayer.daoapp.io","id":"9E2E3368B56CDBB4","token":"tokendemo","addition":["https://dplayer.daoapp.io/bilibili?aid=4157142"]}}); 参考资料：1.Hexo博客中插入音乐/视频2.markdown语法及编辑器推荐，都是踩过的坑3.页内跳转]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java设计模式--六大原则]]></title>
    <url>%2F2015%2F09%2F19%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[传说Java有六大心法，23种武功招式。分别就是Java设计模式六大原则和常用的23种设计模式了。传说掌握心法再配合招式，便能独步天下，称霸武林。这里先来学习下这6大心法。 1.单一职责原则(Single Responsibility Principle)一个类只负责一项职责，不要存在多余一个职责导致类的变更。 比如：类A负责两个不同的职责，b,c职责。由于b职责需求发生变化而需要改变A类，原本运行正常的c职责出现故障。这就违背了单一职责原则。 2.里氏替换原则(Liskov Substitution Principle) 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 子类中可以增加自己特有的方法。 当子类的方法重载父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松。 当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格。 总之尽量不要重写父类已经实现的方法，可以用接口其它方法绕过去。关于第三点的宽松和严格，一开始我也不理解，还是用代码解释一下吧。父类能够存在的地方，子类就能存在，并且不会对运行结果有变动。反之则不行。父类say()里面的参数是HashMap类型，是Map类型的子类型。（因为子类的范围应该比父类大）123456789101112131415161718192021222324252627282930313233343536373839404142package LSP;import java.util.Collection;import java.util.HashMap;import java.util.Map;/** * 里氏置换原则（Liskov Substitution Principle），简称LSP。 * Created by Chant on 2017/6/19. *///场景类public class LspTest &#123; public static void main(String args[]) &#123; invoke(); &#125; public static void invoke() &#123; //父类存在的地方，子类就应该能够存在 Father f = new Father(); Son s = new Son(); HashMap map = new HashMap(); f.say(map); s.say(map); &#125;&#125;//父类class Father &#123; public Collection say(HashMap map) &#123; System.out.println("父类被执行..."); return map.values(); &#125;&#125;//子类class Son extends Father &#123; //方法输入参数类型 public Collection say(Map map) &#123; System.out.println("子类被执行..."); return map.values(); &#125;&#125; 无论是用父类还是子类调用say方法，得到的结果都是 父类被执行… 但是，如果将上面Father里的say参数改为Map，子类Son里的say参数改为HashMap，得到的结果就变成了 f.say(map)结果：父类被执行… s.say(map)结果： 子类被执行… 这样会造成逻辑混乱。所以子类中方法的前置条件必须与父类中被覆写的前置条件相同或者更宽。 3.依赖倒置原则（Dependence Inversion Principle） 高层模块不应该依赖底层模块，都应该依赖抽象；抽象不应该依赖细节，细节应该依赖抽象。 总之：多用抽象的接口来描述要做的动作，降低实现这个动作的事务之间的耦合度。（各自拥有各自的接口，不要放在一起使用，降低耦合性） 4.接口隔离原则(Interface Segregation Principle)客户端不应该依赖它不需要的接口；一个类对另一个类的依赖建立在最小的接口上。总之就是一个接口尽量完功能的单一，不要让一个接口承担过多的责任。 5.迪米特法则(law of Demeter)通俗的来讲，就是一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类来说，无论逻辑多么复杂，都尽量地的将逻辑封装在类的内部，对外除了提供的public方法，不对外泄漏任何信息。只与自己的成员变量和参数打交道，不与其它打交道。 6.开闭原则(Open Close Principle)对扩展开放，修改关闭。尽量通过扩展软件实体的行为来实现变化，而不是通过修改已有的代码来实现变化。 最后，是丑得不行的思维导图，怕辣眼睛就别看了😂。 参考文章 Java设计模式六大原则–详细解说版 Java六大原则–精简版 Java六大原则–代码版]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
</search>